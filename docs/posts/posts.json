[
  {
    "path": "posts/2022-03-21-github-sop/",
    "title": "GitHub Standard Operating Procedure",
    "description": "How we use GitHub in the CDU data science team.",
    "author": [
      {
        "name": "Chris Beeley",
        "url": {
          "https://twitter.com/chrisbeeley": {}
        }
      }
    ],
    "date": "2022-03-21",
    "categories": [
      "Open source",
      "GitHub",
      "Teamwork"
    ],
    "contents": "\nIntroduction\nWe’ve been using GitHub in the CDU data science team since 8th May 2020 (I think- that’s the oldest commit I could find). Team members past and present have used git and GitHub to varying degrees, often solo. Data science is a team sport, without question, but like any team sport it requires proper coordination between participants to be effective. Git and GitHub are very powerful tools, but they are hard to use well even when you’re collaborating just with yourself, and working well with others is another set of skills in itself. We’ve all made a lot of mistakes and learned a lot in the last two years, and this post is designed to codify some of this learning into a Standard Operating Procedure (SOP) for the benefit of team members past and present. And of course, we share it for the general benefit of others who might learn something from our experiences, and also in the hope that those with more/ different skills and knowledge might be able to help us refine it further (get in touch through the usual channels ☺️).\nStandard operating procedure\nBranches\nWe adopted gitflow fairly early on as the team expanded and it worked very well for our purposes. I now see that there is a health warning on the linked post and gitflow is deprecated elsewhere in favour of something like GitHub flow. I’m not going to go into the differences here because it is not the focus of this post but there are some important differences and I can feel another blog post coming on about different models of using git and GitHub once I’ve discussed the matter with team members.\nThe essential feature of gitflow as we’re using it is that there are three types of branches- main branch, which is the working code, development, which is a kind of “staging” branch where code can be worked on and tested and feature branches which either fix bugs or add new features. The main idea is that feature code is merged into the development branch and the development branch is periodically merged into the main branch. Each time you merge to main you do a point release with semantic versioning.\nWe’ve all found that it works pretty well but it is overkill for simpler repos. The development branch is not necessary, and you can merge straight to main from a feature branch for simpler things, especially where it is mainly one person writing, maintaining, and using the code. For important stuff that we all use though, particularly {nottshcData} the development branch is really useful because it means that we can all send PRs to a branch without worrying about untested code getting out into the wild.\nWith all that said how have we found it best to use it in practice? A few points:\nMerge to development and main as frequently as possible. Especially from my point of view, as the team manager, I spend a lot of time on everyone’s repos, and if everything is tucked away in branches I can’t see it.\nDocumentation should go straight to main, there is no need to use branches at all, or if a branch is used it can be continuously merged to main (in case anyone else is writing documentation)\nBranches should be deleted as soon as possible, for the same reason, especially if they include commits that are on main. If they are ahead of main that implies that they could have useful stuff on, or be a failed experiment that needs binning. If you’re off sick and we’re running your code we don’t want to be wondering which it is\nCollaboration\nThere should be one person in charge of each repo. That person is responsible for making sure that the main branch is clean, bug free, and properly documented. If a team member spots something on someone else’s repo that either doesn’t work or doesn’t look right they should either file an issue (if it’s something that might need discussing) or make a pull request (PR) - if it’s a simple fix to formatting or documentation - I send lots of PRs like this. It is the responsibility of the lead for the repo to check the PR and to merge it in in a timely fashion, request changes to it, or reject it.\nHaving a PR accepted is a privilege, not a right, so team members who want to improve a repo need to send a clean PR with a decent explanatory note in it. We might discuss over Slack or in person but in my opinion it’s better if that discussion happens on GitHub where possible because it means everyone, all team members and the open source community at large, can see it. I’m subscribed to all of the repos on our GitHub and I quite often spot areas for improvement or niggles by reading these discussions and it’s my job to understand the workflow and to suggest improvements.\nIssues\nBug reports and feature requests should all be filed as issues, again to increase transparency of ongoing work (within and without the team). Issues should be clearly worded with enough detail and should wherever possible include a reprex. We were bad with issues in the early days, jotting notes to each other that made a lot of sense at the time but within a month or so they were incomprehensible even to the author. Like code commenting and documentation a little work now can save a lot of confusion in the long run.\nReprexes\nI really can’t say enough about the reprex. The beauty of a reprex for me is that at least half the time making the reprex solves the problem. Boiling the problem down to its essential elements improves your understanding of the problem to such a degree that it either disappears completely or you can PR the bug fix yourself. And when it doesn’t it’s so valuable to have it as the person receiving the issue. If you don’t include a reprex, basically, don’t expect a fix, unless there’s a really good reason why you can’t include one.\nData security\nData security is obviously really important when you’re using GitHub, even if it’s a prviate repo. The simplest way to keep data security is don’t keep data in a git controlled folder. If you don’t do that, you will never accidentally push data to a repo. It’s impossible. A lot of our code just gets stuff straight from the server and processes it all in place, so there really is no need a lot of the time to save data anywhere.\nIf you absolutely must save data, for example if you have long running or complex data operations to complete, just make sure that you use .gitignore appropriately. My usual strategy is to have a folder called secret, and I add secret/ to .gitignore. Do that and check what you’re committing and pushing.\nIf the worst happens, and I should say the worst has never happened to us and I don’t think it ever will, you need to destroy all the commits with the data in.\nData security is a very important area and in my very inexpert opinion using GitHub doesn’t make data any less secure. Using GitHub means that you are always thinking about what’s in your data and it reduces sloppy practices like storing credentials in code (which you should NEVER do, even if you’re not using git- store them in environment variables). I read about so many information breaches from the NHS and they seem to be largely people emailing stuff about without concentrating. It’s complacency, they’re so used to everything being secure they forget what they’re doing.\nA note on packages\nThis is not strictly about GitHub, but it fits in the general area of collaboration and so I wanted to include it. We have adopted R packages almost wholesale in the team (another blog post that needs to be written), but there is one minor problem that they introduce that I wanted to flag up here.\nPeople get into the habit of building the package as they go, and then they use the built package for their analysis. This is fine as far as it goes, but you should never deploy or share anything based on code that you built yourself. Before you share or deploy you need to merge to main, switch to a new session and install the code from GitHub. This ensures that the code that you are running is the same code we all have access to. We’ve all spent far too long debugging code that only exists on one person’s laptop, and if you’re off sick and we redeploy your stuff you want to make sure we deploy what you actually want, because we can’t access the built stuff on your laptop.\nHelpful resources\nNHSX draft guidance\nGDS guidance\nHealth foundation guide to sharing code\nA collection of links I curate\n\n\n\n",
    "preview": "posts/2022-03-21-github-sop/preview_pic.jpg",
    "last_modified": "2022-03-22T11:43:20+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-04-open-source-within-the-NHS-benefits-and-possible-pitfalls/",
    "title": "Open source within the NHS - benefits and possible pitfalls",
    "description": "What does open source mean for the average staffmember in the NHS? What benefits does it bring to patients?",
    "author": [
      {
        "name": "Oluwasegun Apejoye",
        "url": {
          "https://uk.linkedin.com/in/oluwasegun-michael-apejoye": {}
        }
      }
    ],
    "date": "2022-03-04",
    "categories": [
      "Open source"
    ],
    "contents": "\nIntroduction\nHealthcare needs to prioritise clinical safety and data security and there have been concerns in the past about using and producing open source within the NHS. In a simple form, working in the open means making the source codes, that powers our software or application, freely available and downloadable from public platforms such as GitHub. This enables collaboration across the board to maximize benefits for the delivery of health and social care services.\nThe term “open source” has been in use for a long time, and it is increasingly gaining traction in the NHS. And, we the clinical developmental unit – data science team at NottsHC are part of the open-source campaign within the NHS. We are dedicated to working in the open and to using open source tools for our analysis.\nAs part of the effort to push for open source, the NSHX has announced that NHS Open Source Policy will officially become available by this summer. According to the draft document, the policy “aims to provide a single position and source of guidance for anyone developing open software for or with the NHS in England”. The announcement about the policy has been broadly embraced by the NHS-R community (an open community of R users).\nBut what does open source translate to for the average staff (with limited technical skills) in the NHS? What benefit does this bring to patients?\nA member of our team already did some justice to this subject matter in her post working-in-the-open, where she shared some of her personal experiences and did some justice to the benefit of open source. However, with this post, I hope to share the general benefit and touch on some more issues about working with open source.\nBenefits\nGreater collaboration: open source enables technical (IT experts, developers, analysts) and non-technical (clinicians, nurses, GPs) staff to input into projects and collaborate easily. It facilitates easy sharing of software, codes, analytical products etc, and it opens doors for reproducibility. With open source, a small idea can easily become a proof of concept which can grow to become a project that ends up improving care delivery.\nCost saving: using an already published code to serve some aspects of our task will lead to reduced duplication of effort, a reduction in staff time committed to projects and improved staff efficiency, a faster roll-out of cost-saving solutions and it enables teams to pursue the best approaches, not just those available locally. Also, the awareness of public scrutiny will ensure teams make their projects fit for purpose. In addition, embracing open-source tools means the NHS will benefit from savings from licensing fees as the cost of open-source tools tends to be lower and they also provide greater commitment/contract flexibility compared to proprietary tools which currently lock-in the organisation to the supplier.\nAvailability of cutting-edge tools: All open-source projects benefit from contributions from a wide variety of users. A good example is the Python and R programming languages, which are widely used tools for a wide range of cool stuff. These two projects have benefited from contributions that now make them a go-to tool for different tasks.\nCommunity effort: The open-source community runs on the philosophy of universally shared knowledge because it benefits from a large and growing network of developers and users that are continually contributing to projects. With an open-source approach to project development, teams from other organizations can take our project, make changes to it and make the changes available to the community (including the original author) to benefit from the additional feature or improvement. This accessibility allows for constant developments and improvements to the software.\nSafety and Security: Does open source mean we sacrifice patients’ privacy or lose control of projects? No. The organisation authoring the project maintains control of the data at all times and ensures it remains private and confidential, and can permit or restrict reuse of code using different software licences.\nAdditional income stream: The authoring team of an open source project can offer to provide subscription services, support and maintenance to interested users and therefore generate passive income for their trust.\nWhat to look out for\nIt’s a no-brainer that embracing open source will bring several benefits to teams within the healthcare system. However, there are still some pitfalls to look out for. Below are some of the ones I have identified:\nImportance of protecting data: Sharing open source in healthcare settings requires safeguards to protect data, and unless suitable synthetic data can be produced it can be hard to allow others to run code that depends on data.\nDocumentation and support: Sometimes documentation and support for open source is inferior to proprietary alternatives and this can lead to delays and extra work for teams.\nLack of sustainability: it is possible to use packages (set of codes) developed by another team in your project. However, there is a risk of the package being abandoned without prior notice by the team which can create problems with incompatibility and require rewriting of existing code.\nTo mitigate the above issues, there is a need for robust policies to help analysts, developers and other users follow a consistent approach to open source. And I believe that is where the NHS Open Source Policy proves useful.\nConclusion\nOpen source has many benefits to offer if embraced by all teams (both technical and non-technical) within the healthcare system. Despite these benefits, there is a need for robust open source policies to ensure all teams work in a consistent way that will maximize the benefits for the quality of care delivered to patients, and the efficiency of the health care systems without compromising patient safety and data governance.\nIf you want to read more about our open source journey, then keep in touch by following us on Twitter and GitHub.\nFurther reading\nNHSX open source policy\nNHS technology: Being open to open source\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-21T15:54:59+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-17-package-workflow/",
    "title": "Package Workflow",
    "description": "From a team time session discussing the workflow to contributing to a \n(currently) private package",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Letxuga007": {}
        }
      }
    ],
    "date": "2021-12-17",
    "categories": [
      "Packages",
      "Workflow",
      "Team Working"
    ],
    "contents": "\nTeam time\nOur team has regular team times but which don’t always focus on code. Quite often we use the opportunity to talk through processes and approaches to analysis and these notes reflect a recent code review where were discussed some of our learning on package development and workflow using GitHub for our (currently) private repository that takes data from the SQL data warehouses and makes it tidy following on from Milan Wiedemann’s nottshcverse.\nThese are some of the agreed points and discussions we had about some of the processes that have developed organically through working collaboratively. This process is very flexible and may change over time depending on our own developing skills or from others’ input (we are always keen to know how to improve our workflows!) so this blog may be changed or superseded in the future.\nCurrently we are using a basic workflow of:\nissue -> create a branch -> branch to development -> development to main -> package installed on people’s machines and on the R Studio Connect server\nStart with an issue\nIssues are not just for bugs and should also be used for features and questions. The benefits of issues are that they are highly searchable and can show thinking on subjects from something that may be a question becoming a feature. If people are following the repository they will be notified of any issues. You can also @ specific people in the issue itself which is good if you are the owner of the issue but want input from someone.\nScreenshot of GitHub repository with the Watch button in the top right highlightedIssues often have one thing detailed but can have many parts to them in a task list:\nScreenshot of an issue with tasks listed, some tickedThe tasks can be ticked in the viewing mode without having to go to the code. To get the task list box type - [ ] and when it is ticked it becomes - [X]. Note that this is sensitive to spaces so any extra spaces will just show the code on the view mode.\nWe agreed in our team time that all issues, even questions, should be closed with a good summary of why it’s closed. For example, we had an issue discussing whether it could be a feature to combine functions that were reliant on each other to save typing them out in analysis. This led to a discussion which resulted in no changes to the code (i.e. no branches) but the issue was closed with a detail on why no work was started on it. This is useful for knowledge sharing with other colleagues, and maybe even the future you, if the suggestion were ever to come up again\nCreate a branch\nFrom an issue a branch will need to be created and however you do this, either through R Studio or the terminal, ensure that the name starts with the number from the issue. The following is an image from the terminal in R Studio which shows all the branches in the project from the command git branch.\n$ git branch\n  101-test-colm-names\n  101-test-column-names\n  194-restrict-rio-demog\n  232-calc-ips\n  233-update-readme\n  243-add-ulysses\n* 262-calc-team\n  change_calc_tot\n  connections\n  development\nThe active branch has a star next to it or appears in a different colour depending on the terminal and settings being used.\nOne of the downsides of not linking issues to branches is that any information on the work being done (difficulties or thoughts on the work) could be somewhere like a README but that is version controlled in code. Issues allow a form of version control but more in a sense of a conversation.\nUpdating the branches\nSome work/features take a long time to complete, and in the meantime the development and/or main branches could have changed. This can be a good thing but can also mean that you may be repeating some work that has already been done, for example, formatting some code so that it is within 80 characters which is what our team has agreed to do for code scripts.\nTo update the local branch from another there are two git commands that can be used, either fetch or pull.\n$ git pull origin main does the fetch and merges. It only shows the changes if there is a conflict and is ok to use if you trust that the code works on the branch you are pulling from.\n$ git fetch origin main will not necessarily look like it’s done anything as the files will not be updated or changed, but it does update all the metadata on the changes. fetch is really good to use if you only want to take some of the changes, but not all.\nFurther information on how to use git fetch https://www.atlassian.com/git/tutorials/syncing/git-fetch.\nDevelopment branch\nFor this package we have agreed to have branches merge to development before being merged (once tested and used in actual analysis) into main. This means that the development branch can rapidly move on in versions compared to main. We originally did releases on both main (through GitHub) and on development using usethis::use_version() which updates the version in DESCRIPTION and creates/updates a NEWS.md file which is a good way of keeping track of changes.\nTo “release” on GitHub and match this documented version the steps are outlined here: https://docs.github.com/en/repositories/releasing-projects-on-github/managing-releases-in-a-repository\nTagging commits in git\nHowever, in team time we agreed that releases should only be on main and to keep a track on version in development we will instead use tags through Git.\nWe did a test tag in the Terminal: git tag -a v0.9.1 -m \"Testing (and there is a still a bug)\" and checked in R Studio, in the Commit History, to see if the tag was listed as this will be a good way to check people’s versions of the package when working collaboratively.\nFurther information on tagging commits in git: https://www.atlassian.com/git/tutorials/inspecting-a-repository/git-tag\nChecking versions\nWhen working collaboratively with someone on code it can be very easy to have a different version of the package so we’ve agreed to always build from the main branch. We do this directly from GitHub to ensure we are all building from the same code. However, when working locally from development or from another branch we suggest using {devtools} and loading the package to the session using Ctrl + Shift + L.\nIf the workflow was to always Build, it may cause issues with analysis as it will be the package all sessions use. It very easy to build from an experimental branch but then forget, do some analysis using the package and not realise that the package is different to main, which in turn is different to the team’s version and the R Studio Connect version.\nTagged commits\nTo check for tags, in R Studio, in the Git panel (top right as default) and the history icon of a clock.\nScreenshot of the top right panel icons with the clock highlighted for HistoryIt’s also possible to get to the same screen by selecting the Commit button and then the history tab.\nScreenshot of the Commit window with the tab history hightlightedIn the commit history pane the tagged commit has a slightly different colour as well as being its own commit.\nScreenshot with the tag version highlightedWords of caution with commits\nWhy you should write informative commits\nAs we were discussing how good messages in commits can help located older versions of code, we realised that the link to the issue or the branch is lost from the commits once merged to another branch. Consequently, commits like “Fixed” won’t necessarily make sense when removed from the context like it being on a branch called 207-code10-error. There is no reference in “Fixed” as to what was changed or what the error/problem was. It’s therefore advisable to get into the habit of writing informative commits as if you were writing them to someone else even if, at the moment, you may be the only one working on the project.\nWhen to commit\nWe had a divergence in the team about whether to commit chunks of working code or smaller steps. This was really down to personal preference but which were both based in differences in approaches to locating historic code changes. The ‘lots of commits’ approach can be a lot of text to go through to find when the code last ‘worked’. However, many individual commits for particular actions can make it easier to pull out one commit that needs removing rather than re-work whole chunks.\nOne thing we did all agree on was that the first commit is usually a large chunk of code and is often committed as --First commit or something similar. We also all had used the Amend previous commit in R Studio.\nScreenshot of the commit pan from R Studio with the Amend Previous Commit highlightedTicking this button takes you to the last commit which can be changed and it also means that you can add new files or changed files to the commit that you may have missed. Changing the last commit doesn’t work so well though if the commits have already been pushed as that will put the two last commits out of synch and that requires it’s own blog in how to rectify!\n\n\n\n",
    "preview": "posts/2021-12-17-package-workflow/img/system-of-mineralogy-comprehending-oryctognosie-p687.jpg",
    "last_modified": "2022-03-02T16:30:05+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-18-personal-access-tokens/",
    "title": "Personal Access Tokens",
    "description": "Connecting to the GitHub to install packages from a private GitHub repository requires security \nset ups and this blog details how to do it (and how not to do it).",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Letxuga007": {}
        }
      }
    ],
    "date": "2021-12-17",
    "categories": [
      "Packages",
      "Security"
    ],
    "contents": "\nInstallation\nBecause some of the packages we are using are (currently) private repositories it is not possible to build using just {remotes} as a Personal Access Token, specifically a GITHUB_PAT, is needed. The {usethis} package suggests using the local Git credential store but this wasn’t compatible with some of our team’s set ups so later in this blog are details on how to do it the “inadvisable” way to be used at your own discretion.\nCheck you have a token\nThis part is the same for whatever method you use on your own computer as this section details how to get the GITHUB_PAT set up. To check for existing GITHUB_PAT use code:\nSys.getenv(\"GITHUB_PAT\")\nIf nothing has been set up by you on GitHub this may return an empty string \"\".\nSet up a token\nFirstly, go to https://github.com/settings/tokens which takes you to your own personal GitHub account settings where you will need to Generate new token. Give this a suitable name like Repo access and tick the repo group for full control of private repositories.\nRemember to copy the code that is generated as this cannot be viewed again\nFrom the Git credential store\nPackages required\n\n\ninstall.packages(\"usethis\")\ninstall.packages(\"gitcreds\")\ninstall.packages(\"remotes\")\n\n\n\n{usethis} suggests using the local Git credential store quite strongly!\n\nIf you have previously set your GitHub PAT in .Renviron, stop doing that.\n\nSet up\n\n\n\nGitHub will open and, if you are not already logged in, you will need to enter your (GitHub) password.\nThe page for GitHub tokens management is https://github.com/settings/tokens and you can do the same as the code in this screen by selecting Generate new token. Give this a suitable name like Package installation and tick the repo group for full control of private repositories.\nRemember to copy the code that is generated as this cannot be viewed again\nNext set up a GITHUB_PAT in RStudio:\ngitcreds::gitcreds_set()\nYou will then be prompted to enter the copied token (no quotes are required). If you’ve already got a token entered the following message will show:\n-> Your current credentials for 'https://github.com':\n\n  protocol: https\n  host    : github.com\n  username: PersonalAccessToken\n  password: <-- hidden -->\n\n-> What would you like to do? \n\n1: Keep these credentials\n2: Replace these credentials\n3: See the password / token\n\nSelection: ```\nEnter an item from the menu, or 0 to exit\nSelection: \nReference: https://usethis.r-lib.org/articles/articles/git-credentials.html\nInstalling\nThe {usethis} documentation suggests using {pak} but this may not work on network drives/VPNs (there are a few issues that have been opened and quickly closed referring to these as potential problems) and so in the meantime combining {remotes} with {gitcreds} is a workaround:\n# install.packages(\"remotes\")\nremotes::install_github(\"<name of GitHub account>/<name of repository>\", \n    auth_token = gitcreds::gitcreds_get(use_cache = FALSE)$password)\nThe absolutely not recommended method\nPackages required\n\n\ninstall.packages(\"usethis\")\ninstall.packages(\"remotes\")\n\n\n\nIf you are sure this is what you want to do then type in RStudio:\nusethis::edit_r_environ()\nIn the file that opens, type into it (where YOUR-PAT is the generated code you’ve copied) and save the file:\nGITHUB_PAT=YOUR-PAT\nRestart R and run Sys.getenv(\"GITHUB_PAT\") to check that the \"\" has changed.\nReference: https://www.jumpingrivers.com/t/2019-user-git/03-githubpat.html#6\nInstalling\nNow, for as long as the token exists, the following code will install the package:\n# install.packages(\"remotes\")\nremotes::install_github(\"<name of GitHub account>/<name of repository>\", auth_token = Sys.getenv(\"GITHUB_PAT\"))\nBuilding package\nIt is preferable to build from the repository rather than locally as this should be what everyone in the team has access to. To build and test any new functions use Ctrl+Shift+L from the {devtools} package to load locally for the session.\n\n\n\n",
    "preview": "posts/2021-12-18-personal-access-tokens/img/british-mineralogy-or-coloured-figures-p132.jpg",
    "last_modified": "2022-03-21T16:02:17+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-06-nottshcverse/",
    "title": "Development of open tools for analysing healthcare data in R",
    "description": "***TLTR: (Too Long To Read)*** \nOur goal was to make it easier to work with healthcare data in a reproducible and collaborative way.\nWe wrote lots of R functions for recurring data manipulations and analytical tasks that magically translate into SQL code and communicate with large databases.\nAll our functions are grouped into R packages because this made it easier for us to: *(i)* write good documentation of our code and analytical tasks, *(ii)* easily distribute updates across all team members, *(iii)* formally test our code, and *(iv)* integrate common data manipulations (or analyses) into interactive dashboards in a modular way.",
    "author": [
      {
        "name": "Milan Wiedemann",
        "url": {}
      }
    ],
    "date": "2021-09-29",
    "categories": [
      "Open source",
      "Team working",
      "Packages"
    ],
    "contents": "\n\nContents\nChallenges and solutions\nOverview of the {nottshcverse}\nSimplified working example\nCreate example data\nCreate SQLite connection\nExample analysis\nWriting a function\nExample SQL code\nExample results\n\nWatch out now\nSummary\nRelated work and resources\n\n\n\n\nOur team works with routinely collected NHS patient data. Currently we focus on understanding better how patients are using the service, changes in clinical outcome measures, and analysis of patient experiences. The main questions that guide our work are ‘What works for whom and how does it work?’, ‘What doesn’t work?’, and ‘How can we integrate patient experiences into our analyses?’. We developed a set of different tools, the {nottshcverse}, to help us look at these questions by automating recurring and time-consuming tasks so that we can spend more time thinking the clinical questions.\nChallenges and solutions\nReal data is messy but we should do our best to tidy the mess, where possible in an automated way. There are many challenges when working with healthcare data, here is only a small selection of those that I think underlies most analyses. Figure 1 shows our solutions to these challenges.\n\n\n\nFigure 1: Main goals that guided the development of the {nottshcverse} packages.\n\n\n\n🤔 It’s really hard to get the clinical data that is needed in a reproducible way that is consistent across different people who work on the same (or related) analysis. This is particularly true because most of the data is stored on SQL servers and only starts to make sense after joining multiple different datasets. 🤓 We wrote functions that made it very easy and secure to (i) connect to and (ii) query data from different databases that we are working with. This way we could get the data we needed for our analyses using very few lines of code in R.\n\n\n# First, create connection to databases\nconn_s1 <- connect_sql(server = \"DB-one\")\nconn_iapt <- connect_sql(server = \"DB-two\")\n\n# Now we can use the connection to get contacts data from SystmOne ...\ndb_contacts_s1 <- get_s1_contacts(from = \"2020-01-01\", \n                                  to = \"2020-12-31\", \n                                  conn = conn_s1)\n\n# ... and IAPTus databases\ndb_contacts_iapt <- get_iapt_contacts(from = \"2020-01-01\", \n                                      to = \"2020-12-31\", \n                                      conn = conn_iapt)\n\n# Note that the objects 'contacts_s1' and 'contacts_iapt' are just pointing to \n# the databases (I like to use the prefix db) and not actually downloaded to the\n# environment on your computer.\n\n\n\n🤔 Most of the time, the data is not in the format that is needed for further analyses. There may be specific data manipulations that are needed to make sense of the data and analyse it properly. Also, different databases might be set up in ways that it is hard to merge data. 🤓 We wrote function that tidy the raw data from the databases so that it is more consistent across databases and easier to analyse. All of this still happens within the server so that our computers don’t have to do all of this work.\n\n\n# Each get_*_data() function comes with a tidy_*_data(), here tidy_s1_contacts()\n# Here I use the connection to the raw (messy) contacts  data that I created above\n# and tidy it using the tidy_s1_contacts() function\ndb_contacts_s1 <- db_contacts_s1 %>% \n  tidy_s1_contacts()\n\ndb_contacts_iapt <- db_contacts_iapt %>% \n  tidy_iapt_contacts()\n\n\n\n🤔 The methods of data analyses and visualisations should be understandable, reproducible, and available to other people. Unfortunately this is not always the case yet because the software tools that are used are not script based and often shared in private emails. 🤓 We wrote R functions for common analytical tasks and visualisations.\n🤔 Code should be really well documented so that it is easy to understand what’s going on. This includes the current version of the code as well as all previous versions and changes. This can be done using tools like Git and GitHub, but unfortunately most code is currently shared undocumented in private emails. 🤓 We created detailed documentations that are easily accessible to everyone who uses our R packages. Also, because develop our tools on GitHub, every change to our code is documented.\n🤔 Mistakes happen! Sometimes things that you have no control over can change and break your code that previously worked fine (e.g., the format of the raw data or a functions that someone else wrote). Therefore, it’s important to continuously test whether the code is still working the way it’s supposed to work. 🤓 R packages (or similar solutions in other statistical programming languages) are relatively easy to test, for example using the {testthat} package. We started to implement tests into our work so that we can check if changes that we make to our code don’t break anything.\nOverview of the {nottshcverse}\n\n\n\nFigure 2: Overview of some R packages developed by the Clinical Development Unit Data Science Team⭐and the NHS-R Community ❤️\n\n\n\n{nottshcData}: Unified framework to query, transform, and aggregate data from different databases\n{nottshcMethods}: Tools for performing common analytical tasks (e.g., grouping continuous age into groups)\n{honos}, {LSOApop}: Packages designed in generic way to help use and others {nottshcData} work with specific questionnaires (e.g., Health of the Nation Outcome Scales, HoNOS) or open data sets (e.g. LSOA population estimates)\n{outcomesdashboard}: Our dashboards use all the packages mentioned above + special packages developed specifically to support the dashboards with helper functions\nSimplified working example\nTo illustrate how R can be used to work with databases I’ll use the following example. Imagine we’re working with a database called SystmTwo (S2) and need to use two different tables for our analysis:\n[S2].[contacts]: Information about contacts with clinical teams\n[S2].[demographics]: Some demographic information\n\n\n\nCreate example data\n\n\n# Set up example contacts table\ncontacts_s2 <- tibble(client_id = c(1, 1, 1, 2), \n                      contact_id = c(123, 124, 125, 156), \n                      referral_id = c(456, 459, 500, 501), \n                      referral_date = c(\"2018-04-19\", \"2019-05-23\", \n                                        \"2020-06-01\", \"2018-12-11\"),\n                      contact_date = c(\"2018-05-19\", \"2019-06-05\", \n                                       \"2020-07-08\", \"2019-01-15\"),\n                      team_id = c(\"tm1\", \"tm2\", \"tm1\", \"tm1\"), \n                      hcp_id = c(\"hcp1\", \"hcp2\", \"hcp1\", \"hcp1\"), \n                      contact_type = c(\"phone\", \"f2f\", \"video\", \"phone\"),\n                      assessment_id = c(321, 322, 344, NA))\n\n# Set up example demographics table with 2 patients\ndemographics_s2 <- tibble(client_id = c(1, 2), \n                          dob = c(\"1988-01-01\", \"1965-01-01\"),\n                          dod = c(NA, NA),\n                          sex = c(\"f\", \"m\"))\n\n\n\nCreate SQLite connection\n\n\n# Create connection (conn) to \"local\" database called SystmTwo (s2)\nconn_s2 <- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\n\n# Copy local data frame to conn_s2 database\ndb_s2_contacts <- copy_to(conn_s2, contacts_s2)\ndb_s2_demographics <- copy_to(conn_s2, demographics_s2)\n\n\n\nExample analysis\nHere we join the contacts with the demographics information to calculate the age at the time a patient has their contact (age_at_contact).\n\n\n# Calculate age at time of contact\ndb_age_at_contacts <- db_s2_contacts %>% \n  left_join(db_s2_demographics) %>% \n  mutate(age_at_contact = as.Date(contact_date) - as.Date(dob))\n\n\n\nWriting a function\nWe can also write our own functions and use them in a modular way whenever we need them. Here’s a simple example to demonstrate how we can do the same calculation as shown above using our own function. In this example the function arguments take the variable names for date of birth (dob) and the contact date (contact_date).\n\n\ncalc_age_at_contact <- function(data, var_dob, var_contact_date) {\n  # Add code here to check that arguments are specified correctly\n  data %>% \n    dplyr::mutate(age_at_contact = as.Date({{var_contact_date}}) - as.Date({{var_dob}}))\n  }\n\n\n\nExample SQL code\nAs mentioned above (Challenges and solutions, Point 1), the object that we work with most of the time are just SQL queries and not real data stored in your R environment. We can look at the underlying SQL code using the dplyr::show_query() function. I don’t really know SQL very well myself, but some people who do have created a great package that translates R code into SQL code (see the dbplyr package for more).\n\n\n# Use dplyr::show_query() function to see underlying SQL code\nshow_query(db_age_at_contacts)\n\n\n<SQL>\nSELECT `client_id`, `contact_id`, `referral_id`, `referral_date`, `contact_date`, `team_id`, `hcp_id`, `contact_type`, `assessment_id`, `dob`, `dod`, `sex`, CAST(`contact_date` AS DATE) - CAST(`dob` AS DATE) AS `age_at_contact`\nFROM (SELECT `LHS`.`client_id` AS `client_id`, `contact_id`, `referral_id`, `referral_date`, `contact_date`, `team_id`, `hcp_id`, `contact_type`, `assessment_id`, `dob`, `dod`, `sex`\nFROM `contacts_s2` AS `LHS`\nLEFT JOIN `demographics_s2` AS `RHS`\nON (`LHS`.`client_id` = `RHS`.`client_id`)\n)\n\nNote that we can also see the SQL code from our own functions.\n\n\ndb_age_at_contacts %>% \n  calc_age_at_contact(var_dob = dob, \n                      var_contact_date = contact_date) %>% \n  show_query()\n\n\n<SQL>\nSELECT `client_id`, `contact_id`, `referral_id`, `referral_date`, `contact_date`, `team_id`, `hcp_id`, `contact_type`, `assessment_id`, `dob`, `dod`, `sex`, CAST(`contact_date` AS DATE) - CAST(`dob` AS DATE) AS `age_at_contact`\nFROM (SELECT `client_id`, `contact_id`, `referral_id`, `referral_date`, `contact_date`, `team_id`, `hcp_id`, `contact_type`, `assessment_id`, `dob`, `dod`, `sex`, CAST(`contact_date` AS DATE) - CAST(`dob` AS DATE) AS `age_at_contact`\nFROM (SELECT `LHS`.`client_id` AS `client_id`, `contact_id`, `referral_id`, `referral_date`, `contact_date`, `team_id`, `hcp_id`, `contact_type`, `assessment_id`, `dob`, `dod`, `sex`\nFROM `contacts_s2` AS `LHS`\nLEFT JOIN `demographics_s2` AS `RHS`\nON (`LHS`.`client_id` = `RHS`.`client_id`)\n))\n\nExample results\n\n\n# Look at results from SQL query shown above\ndb_age_at_contacts %>% \n  select(client_id, contact_date, dob, age_at_contact)\n\n\n# Source:   lazy query [?? x 4]\n# Database: sqlite 3.35.5 [:memory:]\n  client_id contact_date dob        age_at_contact\n      <dbl> <chr>        <chr>               <int>\n1         1 2018-05-19   1988-01-01             30\n2         1 2019-06-05   1988-01-01             31\n3         1 2020-07-08   1988-01-01             32\n4         2 2019-01-15   1965-01-01             54\n\nOf course this is a VERY simple example. This can get way more complex, think BIG and solve BIG problems. There are many other examples out there showing how to work with databases in RStudio. I added some links that I found useful at the end of this post.\nWatch out now\nSo what’s coming next and where can we take this? Is this perfect? I don’t know exactly what’s coming next and this is definitely far from perfect. But it’s the best approach my colleagues and I could come up with in the time that we spent working on this. Maybe I’ll improve this one day, maybe someone else will? Until then let’s share ideas and work together to improve healthcare analytics in the NHS. Ohhh, some people are already working like this 👀 it’s time others join them. I hope those who make decisions about the direction of healthcare analytics in the NHS will start to understand the problems and opportunities and act soon. If not now, when then? We need to move towards a more open and modern way of healthcare analytics!\n\n\n\n\n\n\n\nSummary\nHere’s a short summary in BOLD AND ALL CAPS:\nDOCUMENT everything, absolutely everything! Every function and every single change!\nAUTOMATE common analytical tasks! Write functions and packages!\nTEST everything! Expect mistakes, there will be 🐛🐛🐛\nSHARE as much as we can!\nOf course this doesn’t always work. There will always be some messy data, inconsistent variable names, undocumented code, and … blah blah blah.\nRelated work and resources\n📖 Chris Mainey (2019). SQL Server Database connections in R.\n📖 Emily Riederer (2021). Workflows for querying databases via R - Tricks for modularizing and refactoring your projects SQL/R interface.\n📖 Hadley Wickham, Maximilian Girlich and Edgar Ruiz (2021). dbplyr: A ‘dplyr’ Back End for Databases.\n📖 RStudio (2021). Databases using R from RStudio.\n📖 RStudio (2021). Using an ODBC driver\n📷 Edgar Ruiz (2018). Best practices for working with databases.\n🧙 Chris Beeley (2011 ’Til Infinity). Random bits of related and unrelated statistics, programming, and healthcare wizardry.\n\n\n\n",
    "preview": "posts/2021-08-06-nottshcverse/img/Principes-de-Logique-p435.jpg",
    "last_modified": "2021-12-10T11:16:43+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-14-text-mining-pipeline/",
    "title": "A Text Mining Pipeline for NHS Patient Experience Feedback",
    "description": "This blog post is a more technical description of the pipeline that we have built to analyse patient feedback text data from the NHS.",
    "author": [
      {
        "name": "Andreas Soteriades",
        "url": {}
      }
    ],
    "date": "2021-09-14",
    "categories": [
      "Text Mining",
      "Patient Experience"
    ],
    "contents": "\nThe pipeline consists of two distinct modules:\nA text classification pipeline for classifying patient feedback text into themes like Communication, Environment/facilities, Staff, etc.\nA text mining dashboard reporting results from text classification, sentiment analysis, and analysis of word frequencies to surface information about what patients most talk about, what frustrates them, what they most like in the service etc.\nFor the scope and more high-level descriptions of the different analyses carried out see this blog post and project description.\nIn terms of the nitty gritty, we are particularly excited about having combined super-cool packages in R and Python to build the pipeline! Two highlights of our work are the use of what we consider to be game-changer R packages:\n{golem}- “[…] an opinionated framework for building production-grade shiny applications”. Package {golem} automatically provides the structure for the {shiny} skeleton (app & ui) and makes it very easy to build Shiny apps that are modular, strict as to where the business logic goes, documented, tested, shareable, and agnostic to deployment.\n{reticulate}- an R interface to Python that opened up for us unique opportunities for using state-of-the-art Python packages for text classification and sentiment analysis directly in R.\nFor more details, refer to this presentation where I describe both packages in much enthusiasm!\nPipeline overview\nLet’s take a look at the whole pipeline:\n\nThe pipeline consists of an ecosystem of tailor-made packages in R ({experienceAnalysis}, {pxtextmineR}, {pxtextminingdashboard}) and Python (pxtextmining) that we designed to be both fit-for-purpose, but also as generic as possible for use by other NHS trusts or by anyone in general. Let’s break down the pipeline into smaller steps:\nThe pipeline reads the text data and sends it to the text classification pipeline, as well as to the dashboard.\nThe text classification pipeline uses Scikit-learn- fuelled pxtextmining to tune and train a Machine Learning model. It then writes the results (predictions, performance metrics, classifier performance bar plots, a SAV with the trained text classification pipeline etc.). These are then passed into dedicated modules in the {golem} dashboard that present predictions on unlabelled feedback, as well as tables and plots with performance metrics.\nMeanwhile, the text data is also passed into the dashboard for sentiment analysis and other text mining (e.g. TF-IDFs). The dashboard has dedicated modules that use our external packages to perform these analyses. In particular, {experienceAnalysis} makes extensive use of {tidytext}, although it offers functions that conveniently perform automatically a few data preprocessing and manipulation steps that would otherwise need to be done manually before passing them to the {tidytext} functions. On the other hand, {pxtextmineR} has {reticulate}- fuelled functions for doing sentiment analysis with Python packages TextBlob and vaderSentiment.\nInternally, the {golem} dashboard also has a series of R scripts containing simple utility functions that are useful for running small tasks (e.g.  sort a character vector) that would otherwise be run inside the modules themselves.\nNote that using external packages and utility functions to prepare the data keeps the modules clean from any business logic that would make the dashboard too specific to the dataset used. This is a key advantage of {golem}: we can use the dashboard as a framework for reporting results on any dataset that we would like to use! A simple example is the following: say we want to report averages for a number of categories. This could be mean sepal length for each plant species in the iris data, mean flipper length for each penguin species in the penguin data, and mean miles per gallon for each car engine type in the mtcars data. We can build a {golem} that produces the mean of a variable according to different categories and then pass either iris, penguin or mtcars to get a dashboard for each of these datasets.\nAnd this is exactly where the YAML file in {golem} comes in handy. The YAML file acts as a control panel where the user specifies what dataset and which columns from the dataset to use in the business logic. In our case, this means that we can produce a dashboard for our own data or for the patient feedback data of any NHS trust! As deployment with {golem} is pretty straightforward, we are able to host several dashboards on the server, each of which uses a dataset from a different NHS trust.\nAmazingly, {golem} ships the whole dashboard as an R package! We call our packaged dashboard pxtextminingdashboard. This package contains open patient experience data in RDA format that can be used to run the app. All you need to do is install pxtextminingdashboard, load it in R and run run_app. The dashboard is also available here.\nConclusion\nWe have built a truly revolutionary text mining pipeline that can be used for free by any NHS trust and will hopefully help surface business-critical information to guide improvements in healthcare services. We believe that the pipeline is a great example of how one can make the best of both R and Python. Both {golem} and {reticulate} are game-changers- try them out!\n\n\n\n",
    "preview": "posts/2021-09-14-text-mining-pipeline/img/a-treatise-on-map-projections-p88.jpg",
    "last_modified": "2021-10-13T10:30:45+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-30-pair-programming-code-review-journal-club-and-team-time/",
    "title": "Pair programming, code review, journal club, and team time",
    "description": "What can the CDU data science team do to verify its outputs, disseminate learning, and support individual development in team sessions?",
    "author": [
      {
        "name": "Chris Beeley",
        "url": {}
      }
    ],
    "date": "2021-07-30",
    "categories": [
      "Team working"
    ],
    "contents": "\nWe’ve been a proper data science team for a year now, there’s six of us at the moment which is GREAT and we’re starting to review some of the stuff that we’ve been doing to make sure that it’s serving a purpose. I’m sort of writing this for our benefit just as a way of recording what the plan is (and to discuss the plan via pull request 😎) but we work in the open so why not talk about this in the open too.\nFairly near the beginning of become A Proper Data Science Team we kicked off “Code review” sessions which would take place fortnightly and team members would bring stuff, taking it in turns to do one each. I hardly think I can improve on Google’s guide to code review but I’ll summarise the main points here for those who don’t click through. Code review is the process of one or more people who have not written the code in question to read it and check that it’s okay, before the code is merged into the codebase. Code reviewers are considering:\nDesign\nFunctionality\nComplexity\nTests\nGood naming practices (variables, functions)\nComments\nStyle (judged against a style guide like the tidyverse style guide)\nConsistency\nDocumentation\nIn the linked document Google emphasise that you should read every line- not scan over things and assume they’re okay. It’s worth remembering as well that we are doing data science- so we need to be reviewing statistical and ML methods as well. People (including me) fall into the trap of thinking that data science is just programming and they stop asking themselves hard questions about the methods they are using and that is very dangerous.\nI put code review in quotes advisedly because although we went into them thinking we would be doing code review over the weeks and months we ended up doing something totally different. Sometimes we would pretty much do code review. Sometimes we would spend a long time discussing code style. Other times we would start off doing code review and end up doing an ad hoc lesson in a particular method that the person who came to code review. Sometimes we would have such a good time doing one of these things that we couldn’t fit all the excitement into an hour and would call extraordinary code review meetings so we could carry on discussing it and not have to wait two weeks.\nIt was pretty anarchic but we were all learning and having fun in a supportive environment so I thought we may as well just see where we ended up. After a year I thought it was time to review what we were doing around assuring our code so I kicked off a discussion about it which this blog post is part of.\nWe have had a pretty wide ranging discussion about it and the first point of interest is that everybody wanted to keep the fortnightly sessions but everybody agreed that, however awesome they were, they weren’t really “code review”. We boiled down what we feel we need to four distinct activities. For the sake of brevity I will summarise them here- I could probably do a blog post on each and if anybody would like to hear more then find me on Twitter, have a look at the about page.\nPair programming\nPair programming is what it sounds like- programming in pairs. It’s a great way of teaching and helping each other, and solving problems together, but it’s also a way of reviewing code too. The maxim we came up with today is simple.\n\nEvery line of code should have been read by two people\n\nYou can do that in a pair, live, or you can do it by review (or you can do both). Interestingly enough the team just spontaneously started doing pair programming. Nobody mentioned the word, nobody asked them to. It just makes sense to do that so they just started doing it, which I absolutely love.\nCode review\nThis is the other review methodology that came out of the discussion. I already talked about what Google think this should be so there’s not much to add. One thing we said is that the other thing we need to change is doing it fortnightly as a group. Proper code review can only be done by someone who understands the code. We have a LOT of skills in the team (SQL, R, Python, Shiny, statistics, ML) and nobody understands it all (especially me). We decided that this would happen when people feel they’re ready and would take place with one or more designated people who understand the code, and not every fortnight with everybody.\nOne interesting thing that came out of this discussion was my realisation that we need to deepen and broaden the skills of the team. Several of us are writing code that nobody has the expertise to review. I’ve long been obsessed with truck factor but I hadn’t considered it from the position of validating code before. It’s entirely my failing as a manager and it’s something else to think about when we’re recruiting next (it’s also relevant for training and development, but that’s another post).\nJournal club\nSomething else that we spontaneously did as part of the code reviews that weren’t really code reviews was what we’re loosely calling “journal club”- basically one of “I know something that you need to know, I’m going to teach you” or “we all need to get better at x- let’s learn together”. We’re going to start with me: “Everything your data scientists wanted to know about managing servers and deploying in the cloud- but were afraid to ask” which will end up on GitHub somewhere.\nTeam time\nNot sure about the name for this one! The last thing that everybody seemed to want was something else we were already doing. It’s basically just the ability to just bring anything you like and talk about it. We’ve had all sorts of things. “This code is fine, it’s just horribly slow”, “I can’t figure out what is going to give the best experience for our users”, “I’ve done this analysis but it’s kind of shallow- what else can I do?”. I don’t think this is anything particular, it doesn’t have a proper name, it isn’t particularly for one purpose, it’s more just the team’s way of saying “We help and support each other and we carve out one hour every two weeks so team members can bring a problem and talk it through with us”.\nWrap up\nThat’s where we’re at now, once we’ve agreed this as the way forward (or agreed something else, obviously) then we’ll give it a go. As is probably clear, our Friday sessions and code review are just one part of trying to have a team that works well together and I’m totally new at this so if anybody out there has anything to add I’d be super grateful to hear it.\n\n\n\n",
    "preview": "posts/2021-07-30-pair-programming-code-review-journal-club-and-team-time/img/the-geographical-institutions-p84.jpg",
    "last_modified": "2021-09-21T10:57:55+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-22-age-bands-methodology/",
    "title": "Age bands methodology",
    "description": "A blog compiling all the age bands methodology that can be used for comparing analysis populations against.",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Lextuga007": {}
        }
      }
    ],
    "date": "2021-06-22",
    "categories": [
      "Resources"
    ],
    "contents": "\nAge Bands\nOn the face of it applying age bands to data analysis is simple, you need to consider the start and end ages and group up the rest into reasonable groups. However, if you want to use the data to compare to other sources of aggregate data it needs to be grouped in a similar way.\nFriends and Family questionnaires\nTrusts can control some of the data they collect for the Friends and Family questionnaires and Nottinghamshire Healthcare NHS Foundation Trust have used a grouped age question to anonymise responses from patients. Our feedback can be analysed through the shiny app with code here.\nSUCE (Service User and Carers Experience) survey:\nUnder 12 12-17 18-25 26-39 40-64 65-79 80+ years Refused Missing\nSUCE (Service User and Carers Experience) survey - under 12 years specific\nUnder 6 6 to 8 9 to 11 2 to 17 18+ years\nNational Workforce Dataset\nThe National Workforce Dataset (ESR values) use ages grouped like from the Age profile projection model which uses:\n<20 20-24 25-29 30-34 35-39 40-44 45-49 50-54 55-59 60-64 65-69 70+\nOffice of National Statistics (ONS) - population projections\nAlso see: https://cdu-data-science-team.github.io/team-blog/posts/2021-06-22-population-projections-websites/\nAge bands from population projections are:\n0-4 5-9 10-14 15-19 20-24 25-29 30-34 35-39 40-44 45-49 50-54 55-59 60-64 65-69 70-74 75-79 80-84 85-89 90 and over\nOffice of National Statistics (ONS) - survey best practice\nThere are various suggested age bands for surveys which are listed on this Wikimedia page.\nNHS Staff Survey\nInformation on the NHS Staff Survey has changed in how it was published but the latest website (as of June 2021) has the dashboard with age bands as:\n16-20 21-30 31-40 41-50 51-65 66+\n\n\n\n",
    "preview": "posts/2021-06-22-age-bands-methodology/voyageurs-anciens-et-modernes.jpg",
    "last_modified": "2021-07-30T15:24:10+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-22-population-projections-websites/",
    "title": "Population Projections",
    "description": "Links for population projection data.",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Lextuga007": {}
        }
      }
    ],
    "date": "2021-06-22",
    "categories": [
      "Resources"
    ],
    "contents": "\nPopulation projections\nThe ONS run the census once every 10 years in England and are estimated between censuses. These are used for resource allocation and planning as well as providing denominator population groups for some analyses.\nOffice of National Statistics (ONS) Mid Year Estimates\nThe mid year estimates are available here.\nThe Analysis Tool ins an interactive tool that creates a population pyramid in excel.\nBy CCG\nMid-year (30 June) estimates of the usual resident population for clinical commissioning groups (CCGs) in England: link\nPopulation figures over a 25-year period, by five-year age groups and sex for clinical commissioning groups (CCGs) in England. 2018-based estimates are the latest principal projection: link\nSpecific Population Projections\nPOPPI and PANSI\nProjecting Older People Population Information and Projecting Adult Needs and Service Information\nThere are two specific websites designed to help explore the possible impact that demography and certain conditions on populations either aged 65 and over: POPPI or populations aged 18 to 64: PANSI.\n\nOriginally developed for the Department of Health, this system provides population data by age band, gender, ethnic group, and tenure, for English local authorities.\n\nThese require an account to access but only one account is required to access both site and is free to register.\nProjections are by Region and cover age groups but also gender, ethnic groups, health (including mental health) and learning disability.\nJournal articles\nLancet Public Health\nForecasting the care needs of the older population in England over the next 20 years estimates from the Population Ageing and Care Simulation (PACSim) modelling study. Aug 2018)\nModelling the growing need for social care in older people\n\n\n\n",
    "preview": "posts/2021-06-22-population-projections-websites/img/allgemeine-erdbeschreibung.jpg",
    "last_modified": "2021-07-30T15:24:10+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-10-making-a-presentation-repo-github-template/",
    "title": "Making presentation slides into a GitHub repository template",
    "description": "One of a series of posts relating to creating presentation templates using {xaringan}, GitHub and R Studio.",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Lextuga007": {}
        }
      }
    ],
    "date": "2021-06-10",
    "categories": [
      "Open source",
      "GitHub",
      "Presentations"
    ],
    "contents": "\nCreating a template GitHub repository\nAfter creating {xaringan} presentations slides for the CDU Data Science Team using the branding from Nottinghamshire Healthcare NHS Foundation Trust, I wanted to share the files used as a template. There are quite a few used in {xaringan} slides because it relies upon CSS and images to give the ‘professional’ look similar to PowerPoint.\nOne of the new features in GitHub is to make your repository a template:\n\nAnd the nice thing about this, is when someone selects the template button:\n\nIt means they can fork the repository (get a copy) but all of the commit history is removed giving the next person - if they want - a clean repository to work from. People still have the option to clone the repository with the history by following the usual cloning:\n\n\n\n\n",
    "preview": "posts/2021-06-10-making-a-presentation-repo-github-template/img/grundzüge-der-mathematischen-geographie-und-der-landkartenprojection.jpg",
    "last_modified": "2021-07-30T15:24:10+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-10-making-an-rstudio-presentation-template/",
    "title": "Making an RStudio presentation template",
    "description": "One of a series of posts relating to creating presentation templates using {xaringan}, GitHub and R Studio.",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Lextuga007": {}
        }
      }
    ],
    "date": "2021-06-10",
    "categories": [
      "Open source",
      "RStudio",
      "Presentations"
    ],
    "contents": "\nCreating a template through R Studio\nDid you know it’s possible to set up your own templates into RStudio? I didn’t until I started looking into the presentation templates for the CDU Data Science Team. The benefit of this is that I no longer have to set up clone repositories each time which is awkward too if you want to keep all presentations in one repository like we do for the team’s presentations.\nA few in the team suggested setting these up as a package but {xaringan} slides come with many supporting files (images and CSS), and it wasn’t too obvious how to do this. Luckily a number of people have done it and one such person tweeted about it so I added the details to the repository’s issues for reference.\nThis led me to @DrMowinckels’s package {uiothemes} which is particularly useful as she uses this package for various templates and themes and so, following the layout for adding the xaringan slides I added the following folders to our own package {nottshcMethods} following our process of:\ncreate a branch with the issue number like 1-slide-templates\nmake the changes\nset up a pull request to the development branch\nFile structure\nI added the file structure:\nnottshcMethods/inst/rmarkdown/templates/Nottshc/skeleton\nand in the Nottshc folder I added a file called template.yaml containing:\nname: Nottshc Presentation template\ndescription: >\n   Standard xaringan Nottshc template for presentations\ncreate_dir: TRUE\nThe create_dir is particularly important as this is asking if a new folder directory should be created or not. As this is set to TRUE all the files and folder structure in the folder skeleton will be copied. Note that in {uiothemes} all xaringan files are within one folder but I prefer my files to be in subfolders so {nottshcMethods} has the subfolders css and img.\nname is what will appear in the RStudio templates later so this needs to be clear and concise.\nIn the folder skeleton the important file is the skeleton.Rmd which is the template RMarkdown file for the slides.\nAs this is within a package, to run the package Ctrl+Shift+B will build the package on your computer. When that’s run the template will appear in File/New File/R Markdown…/From Template\n\nAddendum and a plea to blog things like this\nI wrote this out a number of weeks before publishing and I’m so glad I did as I immediately forgot everything I did. It was only when I needed to do a presentation and went to my templates in RStudio that I realised how great this is and how I couldn’t remember how I had set it all up. Luckily I had also started this blog in order to share with others and so the moral of the story is, when you write a blog you are sharing your current knowledge with others, but also your future self.\nBe kind to your future self, share your thoughts and your technical wins, even if they seem small to you today they may be huge tomorrow.\n\n\n\n",
    "preview": "posts/2021-06-10-making-an-rstudio-presentation-template/img/bibliothek-geographischer-handbücher-107.jpg",
    "last_modified": "2021-07-30T15:24:10+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-14-index-of-multiple-deprivation/",
    "title": "Index of Multiple Deprivation",
    "description": "The measure of relative deprivation in small areas in England called lower-layer super output areas",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Letxuga007": {}
        }
      }
    ],
    "date": "2021-05-14",
    "categories": [
      "Resources"
    ],
    "contents": "\nIndex of Multiple Deprivation (2019)\nIMD is very useful for categorising the area a person lives in for deprivation. This information/links and code only cover England and deprivation is scored in relation to all the areas within England using the IMD (2019). This means that they have been ordered by the deprivation score and then ranked (IMDRank).\nThe common decile used is between 1 and 10 with 10 being the least deprived.\nData warehouses\nThe data is often held in 3 tables:\nthe postcodes of the data held (for example patients when in the healthcare sector)\na lookup postcode table (like a directory of postcodes) from\nhttps://digital.nhs.uk/services/organisation-data-service/data-downloads/ods-postcode-files\nthe IMD data from\nOlder version: http://www.gov.uk/government/statistics/english-indices-of-deprivation-2015 https://www.gov.uk/government/statistics/english-indices-of-deprivation-2019\nSelect File 7 for the dataset.\nOther data is available from this dataset including IDAOPI which relates to only older people.\nNote that the column headers change, in 2015 it was LADistrictCode2013 and in 2019 it is LADistrictCode2019. Also LADistrictName2013 has become LADistrictName2019.\nWatch for…\n‘Unknown’ in the data warehouse\nData warehouse tables may include a row for ‘unknown’ (https://www.sqlchick.com/entries/2011/5/16/usage-of-unknown-member-rows-in-a-data-warehouse.html) and this may create a valid join across tables: unknown is entered as a postcode that links to the postcode table that in turn returns data from the IMD table. This may result in a value being returned from the IMD table that isn’t valid like 0 which looks like it should be an IMD decile score, for example, but which is not.\nPostcode spaces\nAlso, postcode lengths vary according to how many spaces there are between the two parts. In the UK the postcode format can be 3 parts and then 3 or 4 then 3:\nNG1 1AA NG26 1AA\nJoining datasets is always better when the space between the postcode parts is removed. In SQL this can be:\nREPLACE(postcode, ’ ‘,’’)\nin R it can be\n\n\npostcode <- \"NG16 1AA\"\ngsub(\" \",\"\",postcode)\n\n\n\nPartial postcodes\nPartial postcodes will not give a sufficiently reliable IMD score.\nExample join code (SQL)\nTo get the IMD score the LSOA (Lower Super Output Area) code is required which is taken from the full postcode.\nThis code will not run and is dependent on the naming conventions of the SQL server. The column names of LSOA11 and LSOAcode2011 will have come from the data sources. PostCode_space will have been added to the table by the data warehouse administrator(s).\n\nSELECT Top 100 imd.*\nFROM DIM_AI.PatientData AS p\nLEFT JOIN DIM_AI.PostCodes AS pc ON p.PostCode = pc.PostCode_space                                              \nLEFT JOIN DIM_AI.IMD AS i ON pc.PC.LSOA11 = i.LSOAcode2011\nWHERE p.PostCode LIKE 'NG%'\n\nCreating quintiles\nSome publicly available data is in quintiles for IMD to remove small identifiable numbers.\nTo replicate that in local data the equation: floor((IMDDecile-1)/2) + 1 can be applied.\nQuintiles in SQL\n\nSELECT DISTINCT IMDDecile,\nFLOOR((IMDDecile-1)/2) + 1 AS IMDQuintile\nFROM DIM_AI.IMD\nORDER BY IMDDecile\n\nQuintiles in R\nThe following example Pubicly available data will not run download if this Rmarkdown script is run as the eval has been set to FALSE. Either change this to TRUE, remove eval=FALSE altogether or copy the code to another R script to run.\n\n\n# IMD by Ethnicity by Region 2007-2013\n# Latest: https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/livebirths/adhocs/006134birthsbyethnicitysexregionandimdquintilebyfinancialyear2007to2013\ndownload.file(\"https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/livebirths/adhocs/006134birthsbyethnicitysexregionandimdquintilebyfinancialyear2007to2013\",\n              destfile = \"regionbirthsbyethnicityIMD20072013.xls\",\n              method = \"wininet\", #use \"curl\" for OS X / Linux, \"wininet\" for Windows\n              mode = \"wb\") #wb means \"write binary\"\n\n\n\nApplying the formula:\n\n\nlibrary(tidyverse)\n# Generate a dataset\ndf <- structure(list(IMDDecile = c(0L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, \n8L, 9L, 10L, NA)), row.names = c(NA, -12L), class = c(\"tbl_df\", \n\"tbl\", \"data.frame\"))\n# Make the 0 generated into NA\ndf_quintile <- df %>% \n  replace_na(list(IMDDecile = 0)) %>% \n  mutate(IMDQuintile = floor((IMDDecile-1)/2) + 1,\n         IMDQuintile = as.character(IMDQuintile)\n         )\n\n\n\nCreating local IMDs\nIn Nottingham/Nottinghamshire the differences between the LSOA areas is diminished when ranked against England as a whole, but when ranked locally, the variation is much more pronounced. Consequently, for the majority of our analysis the Trust approach is to use Nottinghamshire quintiles of deprivation / IMD. There will be some instances where national quintiles (or deciles) will be required – namely any external analysis – however, these will be the exception.\nFor note, when using local quintiles anyone with a non-local postcode will come back unmatched as well as those people with proxy postcodes used often to denote homeless status (ZZ…).\nIMD in SQL\nTo create the local rankings in SQL the data needs to be restricted to the appropriate area, for example when joining to the Postcodes and restricting and then a windows partition applied to the data ROW_NUMBER() OVER(ORDER BY IMDRank) to create a new ranking score and NTILE(10) OVER (ORDER BY IMDRank) to create new deciles.\nTo replicate this in dplyr the code would be:\n\n\n # Data for Nottingham and Nottinghamshire taken from Postcodes and IMD. The original files are very large to download. \n# Note that there are columns in the Postcode data sample that don't exist in the source file. \n# These have been added locally but may be of use to others, such as CountyName and LocalAuthority_Name. # These are specifically added as Nottingham Local Authority is a Unitary Authority and appears in a different column to Nottinghamshire County's District Councils \nload(\"data/sampleDataNottingham.RData\")\nlibrary(dplyr)\n# Note on the join the two column names are different so are listed in the by = and they need to be in the correct order so the column from imdNottingham appears first.\nlocalRanking <- imdNottingham %>% \n  inner_join(postcodeData %>% \n               select(LSOA11) %>% \n               group_by(LSOA11) %>% \n               slice(1), by = c(\"LSOAcode2011\" = \"LSOA11\")) %>% \n  mutate(Notts_rank = row_number(IMDRank),\n         Notts_decile = ntile(IMDRank, 10)) \nlocalRanking %>% \n  select(LSOAcode2011:IMDDecile) %>% \n  head(5) %>% \n  knitr::kable(format = \"html\")\n\n\n\nLSOAcode2011\n\n\nLSOAname2011\n\n\nLADistrictCode2019\n\n\nLADistrictName2019\n\n\nIMDScore\n\n\nIMDRank\n\n\nIMDDecile\n\n\nE01013812\n\n\nNottingham 018C\n\n\nE06000018\n\n\nNottingham\n\n\n58.744\n\n\n1042\n\n\n1\n\n\nE01013814\n\n\nNottingham 022B\n\n\nE06000018\n\n\nNottingham\n\n\n42.893\n\n\n3499\n\n\n2\n\n\nE01013810\n\n\nNottingham 018A\n\n\nE06000018\n\n\nNottingham\n\n\n52.690\n\n\n1767\n\n\n1\n\n\nE01013811\n\n\nNottingham 018B\n\n\nE06000018\n\n\nNottingham\n\n\n53.234\n\n\n1688\n\n\n1\n\n\nE01013815\n\n\nNottingham 022C\n\n\nE06000018\n\n\nNottingham\n\n\n41.721\n\n\n3805\n\n\n2\n\n\nOther useful links\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/464430/English_Index_of_Multiple_Deprivation_2015_-_Guidance.pdf\nhttps://fingertips.phe.org.uk/search/imd\nhttp://dclgapps.communities.gov.uk/imd/idmap.html\nTechnical report for 2019: https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/833951/IoD2019_Technical_Report.pdf\nReferencing IMD in a paper or research\nFrom a journal check to see how IMD is referenced in published papers, this was from a 2016 BMJ article that cites the Index in the references as:\nDepartment for Communities and Local Government. English indices of deprivation 2015. 2015. https://www.gov.uk/government/statistics/ english-indices-of-deprivation-2015\n(Taken from https://bmjopen.bmj.com/content/bmjopen/6/11/e012750.full.pdf)\nAnother paper from 2016 cites as:\nDepartment for Communities and Local Government. English Indices of Deprivation 2015. Available online: http://www.gov.uk/government/statistics/english-indices-of-deprivation-2015 (accessed on 27 April 2016).\n(taken from https://www.mdpi.com/1660-4601/13/8/750)\nLooking at the Government page that lists the full text the library assistant said: “I would reference it from the Ministry of Housing, Communities and Local Government which would be more up to date for 2019 and with online references you should always put the date you accessed it. So I would suggest amending to the following:”\nMinistry of Housing, Communities and Local Government. English Indices of Deprivation 2015. 2015. https://www.gov.uk/government/statistics/english-indices-of-deprivation-2015 (Accessed 4 June 2019)\n\n\n\n",
    "preview": "posts/2021-05-14-index-of-multiple-deprivation/img/grundzüge-der-mathematischen-geographie-und-der-landkartenprojection-93.jpg",
    "last_modified": "2021-07-30T15:24:10+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-14-mapping/",
    "title": "Mapping",
    "description": "Mapping using public health tools",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Letxuga007": {}
        }
      }
    ],
    "date": "2021-05-14",
    "categories": [
      "Resources"
    ],
    "contents": "\nMapping\nPublic Health Tools\nAs Public Health is based within Local Authorities many of their boundaries are related to government rather than health boundaries. Smaller areas related to GPs will be included but data becomes patchy at Trust boundary level. For example, Nottinghamshire Healthcare NHS Foundation Trust covers Nottinghamshire, Nottingham and some areas outside of these boundaries.\nLocal Health\nPublic Health use the following to overlay data such as life expectancy over IMD scores. It is possible to upload data to this site but this has not been approved by IG.\nShape\nThis requires creating an account but is freely available to NHS staff.\nThis has Trust locations already in the account and data can be overlayed. Drive time and public transport within so many minutes is particularly useful.\nCentroid mapping\nSometimes you have a set of addresses but no way of mapping them. The Office for National Statistics’ Open Geography Portal provides the centroids for all UK postcodes.\nUsing R to get centroid information: https://www.trafforddatalab.io/recipes/gis/postcodes.html#\nOr weighted by LSOA area: https://geoportal.statistics.gov.uk/datasets/lower-layer-super-output-areas-december-2011-population-weighted-centroids\n\n\n\n",
    "preview": "posts/2021-05-14-mapping/img/half-hour-library-of-travel-nature-and-science-for-young-readers.jpg",
    "last_modified": "2021-07-30T15:24:10+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-27-working-in-the-open/",
    "title": "Working in the open",
    "description": "What does it mean to work in the open? What is open source? What problems can we solve if we share more openly?",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Letxuga007": {}
        }
      }
    ],
    "date": "2021-02-27",
    "categories": [
      "Open source"
    ],
    "contents": "\nWhat is working out in the open?\n“Working in the open” isn’t a technical term; it’s just my simple way of explaining a concept without having to mention all the tools that are available to do this.\n\n\n\nAs you can see from @ChrisBeeley’s tweet, he refers to “writing stuff in the open and making it reusable”; we use slightly different language but it covers the same principles.\nBeing open in the Public Sector\nI’ve always found that analysts working in the NHS and Local Authorities are always happy to share their methodologies, approaches to work and even code and how we shared this was often dictated by who we know and the tools we have to hand, like Excel or Word. If a change was made to the original I’d never know about it and, vice versa, if I improved the code I wouldn’t have an easy way to share back what I’d done.\nWhilst analysts were happy to share code I still built up a code repository for myself and for many years I recycled my own code. I often refer back to projects where I know I’ve written a particular bit of code that is useful and I’d rarely wrote out complete chunks of code that ran independently of project data. Working openly changes how you approach code because sharing projects that don’t work too well without some changes isn’t all that useful to others.\nWorking with an intention to be open makes you aware of public scrutiny and so, inevitably, you may take a bit longer to make code tidy, write a few more explanatory comments and ensure that code does what you think it should. What’s nice about doing this is that although the openness of work is intended for someone else’s benefit, often that person is still you.\nRepeatedly solving the same problem\nMany of the tasks that analysts and data scientists in the public sector are tasked with are the same. National Returns and benchmarking submissions are common and are completed by many trusts using slightly different approaches but, ultimately, leading to the same data output. This results in a constant cycle of problem solving where the solution, if not shared publicly, means others have to do the very same discovery work.\nIn a completely different context, it would be like finding a chemical compound, not sharing that knowledge and other people working hard in other labs to repeat the discovery. By constantly working in this ‘discovery’ phase we never further our collective knowledge by refining the techniques, analysing the results and, hopefully, using the “compound” to make a difference.\nPublishing code means that anyone in my team, my trust, the NHS, even the world can see what I have already discovered. In sharing to the world audience, I know I have something I myself can use. Of course, it takes time to write these things out but once it’s written it never needs to be rediscovered again - but it can be improved upon.\nHow do the CDU data science work in the open?\nThe CDU data science team have a strong desire to work openly and we have created a GitHub account to share code and knowledge like the pages on IMD1 and mapping.\nWe are also involved with the NHS-R Community, facilitating training, presenting webinars and talks as well as hosting the annual Hacktoberfest which was virtual last year. We had originally set up the Hacktoberfest to just be our team, setting aside one day to contribute to projects and practice using GitHub. Pretty quickly after agreeing this would be a good idea we extended this to the NHS-R Community as we felt that there really wasn’t any need for it to be restricted to just our team. We had a few people come in and out of the MS Teams meeting through the day and, like many things in the NHS-R Community, it was very supportive and informal.\nBuiding up skills\nWorking on other people’s projects in a Hackathon may seem, on the face of it, “non-essential” work, but it’s invaluable as it not only opens up connections with others who can help with your projects, but you invariably see useful code you can then use. Reviewing code is one thing, but to really understand a piece of code, debug or solve a problem you often have to break it apart and build it back up. In doing so you learn how it is constructed programmatically and how the other person/people have approached a problem. Doing this with others’ scripts has made me a better coder and none of the effort has been wasted.\nA recent example of valuable “non-essential” work for me was helping someone in the NHS-R Community Slack group who had an issue with their RMarkdown and getting a plotly chart to appear in the eventual html output, although it would appear when each chunk was run. I took the code and moved each chunk into a template RMarkdown to see if it ran, section by section. In doing so I located the problem but I also saw a new bit of code2\n\n\ncode_folding: \"hide\"\n\n\n\nwhich I’d never seen that before. Now I could have equally have learned about this from reading about RMarkdown but it will forever stick in my memory as it was in the context of solving a problem. We helped each other and now I’m sharing that learning in this blog - working out in the open.\nThe full YAML for reference\n---\ntitle: \"Test\"\ndate: \"25/02/2021\"\noutput: \n  html_document:\n    code_folding: \"hide\"\n    toc: true\n    toc_float: true\n    toc_collapsed: false\n---\nIf you want to read more about how our journey is going with working out in the open keep in touch by following us on Twitter and in these blogs.\n\nIndices of Multiple Deprivation↩︎\nThe problem was results=‘hide’ being in the knitr::opts_chunk$set() code which affects the output. I’m still learning how these codes work so didn’t spot that at first, so I learned more about RMarkdown by debugging.↩︎\n",
    "preview": "posts/2021-02-27-working-in-the-open/img/letters-from-high-latitudes.jpg",
    "last_modified": "2022-03-21T15:54:59+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-20-a-new-github-release-and-future-projects/",
    "title": "A new GitHub release and future projects",
    "description": "We have a new project out and would like to tell you about some more of our future work.",
    "author": [
      {
        "name": "Chris Beeley",
        "url": {}
      }
    ],
    "date": "2021-01-20",
    "categories": [
      "Open source"
    ],
    "contents": "\nIf you’ve read the about section of this blog then you’ll know that our team believes in (and practises!) open source data science. We strive to put as much code and (sometimes synthetic) data out as possible, with an open source licence (MIT, usually), and where we can we try to make our code reasonably easy to re-use (although this is not always simple). We have just pushed out a prototype version of an application and this seemed like a timely moment to talk about the application, and what else we have coming up on the open source side of things. It’s worth saying that some of our team members work really, really hard doing lots of stuff that is very difficult to share so although you might not see as much of them on the GitHub they’re doing sterling work for the Trust and the team is dependent on their expertise for all of our work, whether it’s open source or not.\nText mining application\nWe already have a blog post about this work and we have come to the point where we have produced a release version (0.1.0) for the dashboard which summarises the acccuracy of the models and helps to show the kinds of decisions that it’s making. Please read the blog post for details of this work but our ambition in brief is to produce a text mining algorithm for patient experience that can be used in any NHS organisation in the country. The actual algorithm work (which is in Python mainly) has not yet stabilised to a release version just yet but is available on GitHub.\nWe will be shipping another dashboard that helps trusts to visualise their feedback as part of the project. We’re currently working on that but we’re not quite ready to share it yet, keep an eye on our Twitter and blog for more details. We have a lot more to come in the way of working with staff and patient experience data, too, it’s not just this work, so please feel free to follow along with the code once it’s all open and maybe even send us a pull request 😉.\nForecasting of patient numbers\nWe’re also involved in a Health Foundation funded project which looks at predicting numbers of certain types of patients in the hospital. It takes the form of a dashboard which can predict the numbers of patients likely to fall into particular categories in the next 1-10 days based on previous data of this kind. The model has complex seasonality (although currently it achieves better results if you constrain it to results since April because of COVID) and a TBATS model produced the best results. The code is MIT open source and could be easily adapted to predict lots of different univariate series. There are lots of other people involved in this project and written materials from them are forthcoming, I will add them to this blog post once they are available. Our role was just to help with the forecasting and write the dashboard, lots of other work has gone into it.\nForecasting pharmacy dispensing\nThis is another Health Foundation funded forecasting project which attempts to predict the amount of many different medications which will be dispensed from a pharmacy in order to better manage stock levels. Again the code for our bit is open source MIT, although it is very early days for this project so there will be much more to come. There is much more to this project than just the code, and I will update this blog post with more details once more of the outputs are ready.\nA Shiny interface to EndomineR\nThis work relates to another Health Foundation funded project but it was funded by NHS-R. EndomineR is a clinical text mining system which works with endoscopic reports and helps to collate and analyse data from free text reports automatically. This work replicates an existing Shiny interface but uses the {golem} package to rewrite the code within modules and to make the application run as an R package. This will make the code easier to maintain, update, and generalise to other clinical settings. This project is currently in active development but it should be ready for a first release in February some time, the code again available open source on GitHub.\n{golem}, gitflow, and production data science\nWe are all still learning but we are trying to use good methods to make sure that our code is robust and easy to deploy, and to help us collaborate with each other. To this end we:\nUse the {golem} package for a lot of our Shiny work, and modularise our Shiny code\nUse RStudio Connect (I have written some stuff about this on my own blog)\nUse gitflow\nHave regular (two weekly) code review sessions\nI’m really interested in understanding how to get better at working together in the open and tools to help code easy to deploy and generalise. If you’re interested in that too, especially if you work in the NHS (some of the hurdles are the same size and height everywhere in the NHS 😆) then please get in touch.\nWe have lots of stuff planned including better analysis of HoNOS data, more to come on staff and patient experience, methods for summarising clinical outcomes and health inequalities, and other stuff from team members where I’m not quite sure how close they are to launch. Please watch out for regular updates if you would like to see more stuff from us, on our Twitter and on this blog.\n\n\n\n",
    "preview": "posts/2021-01-20-a-new-github-release-and-future-projects/img/admiralty-manual-for-the-deviations-of-the-compass.jpg",
    "last_modified": "2021-07-30T15:24:10+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-13-data-clinics-in-nottinghamshire-healthcare/",
    "title": "Data clinics in Nottinghamshire Healthcare",
    "description": "We have been working with teams to help them with their data problems. This post describes some of the clinics and what has come about as a result of this work.",
    "author": [
      {
        "name": "Lori Edwards Suárez",
        "url": {
          "https://twitter.com/Lori_E_S_": {}
        }
      }
    ],
    "date": "2021-01-13",
    "categories": [
      "Data clinics"
    ],
    "contents": "\nOur team recently piloted data clinics within the Trust in order to:\nImprove data quality and completeness\nImprove the means by which staff collect and record the essential information, making it more efficient and freeing them up to spend more time with their patients.\nThis was achieved by going “back to basics” with the people who collect and input data, the key principles are ensuring they know why they are collecting the data, making sure the data collection system works well for the teams and that the data can be used by both the data collectors and analysts. A variety of avenues were considered, such as reducing excess data collection and reducing duplication which make data gathering more laborious and tedious for clinicians.\nWhen data collection is difficult for clinicians it often results in the data not being filled properly, correcting this increases the accuracy and completeness of the dataset. Structuring clinical records and decreasing their reliaance on free text input is also beneficial for data analysis but is also often faster and easier for clinical staff. The clinics are a collaborative venture with the clinical team and others such as analysts and system admin, the type of staff varies depending on the needs of each teams. My role was to facilitate the conversations using skills learnt from working closely with the clinical teams to learn to “translate” between clinical language and data/IT language. Subtle differences in expression between the two groups often lead to misunderstandings which could stifle progress (more examples). The key element was that the problem was generated by the team themselves. This ensures that the clinic is focused on solving their difficulties which should help them to improve their own systems rather than forcing a change on them.\nAs a test run we had two teams go through the process:\nA forensic mental health team which wanted to move away from using Excel to collect their data\nA community mental health team which wanted to collect some extra information to better understand the impact of their team without adding too much to their workload\nThe forensic team was a new service which had a lot of data requested of them and they wished to improve their data collection and assess its quality. The Team Leader had used team-specific forms in RiO (the clinical database which they use) previously and was interested in seeing if it was possible here. However, they were having a tough time explaining to the managers who were not familiar with such a system how to approve it and get it built into the system. The spreadsheet was found to have a lot of duplication and data being requested that was not necessarily attainable by the team. We looked together at what the purpose was and changed some of the data from free text to a more structured pick list from the valid values for that piece of data. We also had to explain to the managers that this change was not going to affect their reporting adversely. The patient record system was able to provide the team with what they needed and to automate some aspects to reduce workload for clinicians. Some outcomes measures already existed but others were not yet available on the system, an Excel sheet was made to collect them (an improvement over a folder in the corner of the room) with a reduction in demand for clinicians with simple automation of score summations. The team are thrilled that they can collect the data necessary for reporting and understanding their service in a more intuitive way, project managers are content they are getting the same information and more data is readily available for service improvement. Reports are being built which give the clinicians easy access to data which allows them to engage better and feel ownership of the data.\nThe community mental health team wanted to collect some more information to improve their ability to understand their outcomes. They needed to be able to distinguish between the cohorts of patients that were being referred. This ended up having a simple solution that had not been known to the clinical team – adding in more specific referral reasons. The patient cohort was clear and defined and could be determined at referral. They wanted some more information on one of the cohorts to understand the group further and to see how specific patients within the group progressed. To gather this data, a short form on the electronic patient record was created which takes one minute to fill in but adds a wealth of information. The team also got to play with the form before it went live to gain familiarity and to help them feel ownership of it. They also wanted to be able to predict when referrals may come in. As we got to know the pathways that brought patients into the service, we learned that we had information about patientes in the previous stage of the pathway. So, we managed to collect some information to understand the time between the previous stage and the referral. This means we can see when there is an uptick in people passing through the previous stage and predict a spike in referrals for the team to prepare for.\n\n\n\n",
    "preview": "posts/2021-01-13-data-clinics-in-nottinghamshire-healthcare/img/grundzüge-der-mathematischen-geographie-und-der-landkartenprojection-106.jpg",
    "last_modified": "2021-07-30T15:24:10+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-14-classification-of-patient-feedback/",
    "title": "Classification of patient feedback",
    "description": "An NHSE funded project to devise an application to automatically tag the content of patient feedback",
    "author": [
      {
        "name": "Andreas Soteriades",
        "url": {}
      }
    ],
    "date": "2020-11-14",
    "categories": [
      "Patient feedback"
    ],
    "contents": "\nConsider the following problem. A NHS trust is devoted to improving their services to provide patients with the best possible experience. The trust sets up a Patient Feedback system in order to identify key areas of improvement. Specialized trust staff (the “coders”) read the feedback and decide what it is about. For example, if the feedback is “The doctor carefully listened to me and clearly explained me the possible solutions”, then the coders can safely conclude that the feedback is about communication.\nBut what happens when thousands and thousands of patient feedback records are populating the trust’s database every few days? Can the coders keep up with tagging such a high volume of records? After all, unless they read all of it, they cannot tag it!\nWe need to find a clever way to get some weight off the coders’ shoulders!\nHere in Nottinghamshire Healthcare NHS Foundation Trust, we (the Data Science team) have opted for a Machine Learning approach to help coders tag the incoming patient feedback. In particular, we are developing Text Classification algorithms that “read” the feedback and decide what it is about.\nFirst things first: what are Machine Learning and Text Classification?\nMachine Learning is a wider concept, but here we will talk about the so-called supervised Machine Learning. Say a child is playing with a hole cube:\nPhoto of a child’s sorting toy with the sorting shapes stacked to the side of the sorting boxBy trying to pass different shapes through different holes, the child follows a process of “training” or “learning”, through which they learn to identify the right shape for each hole. Once they have been “trained”, they can easily predict what shape is the right one for a specific hole, on this or any other hole cube.\nThe process that the child has just followed is very similar to Machine Learning: see the child as an algorithm, the shapes as a dataset, and the holes as tags and you have a supervised Machine Learning problem. In other words, in supervised Machine Learning the algorithm “learns from” or “is trained on” the dataset, and thus becomes able to predict what tag corresponds to each record.\nSo when we have patient feedback data that have already been tagged by our coders, we can train an algorithm to assign the most appropriate tag to each feedback record, based on the content of the feedback text. As fresh, untagged feedback populates the trust’s database, the algorithm is then able to predict the most appropriate tag for it. In other words, the algorithm learns to automatically classify the text according its content, which is what Text Classification is about: a form of supervised Machine Learning that is about predicting the appropriate tag for the given text.\nHow can Text Classification improve NHS services?\nIncrease tagging speed. As mentioned earlier, the idea is to have the algorithm automatically tag feedback that the coders simply do not have time to read and tag themselves. To begin with, it will make the process of tagging much more efficient.\nNarrow down searches for NHS staff. If a member of staff (e.g. manager, doctor, nurse) wishes to focus on improve patient experience that has to do with, e.g. communication, they will want to read some or all of the incoming feedback about it. The algorithm will crunch the incoming feedback, decide which records are about communication, and feed them back to the member of staff.\nAre there any cons?\nAlgorithms make errors. For example, an algorithm may incorrectly classify feedback about smoking as being about communication. This is to be expected as no algorithm can ever be 100% accurate. What is key then is to make the algorithm as accurate as possible for the task at hand. This is an area where we focus on on a daily basis.\nDespite some inaccuracies, Machine Learning will still offer the great advantage of narrowing down NHS staff searches almost exclusively to the feedback of interest. If a manager has 100 feedback records of which only 20 are potentially relevant, and the algorithm predicts that 30 are potentially relevant (because it will make a few mistakes), this would still be a 70% reduction in the number of feedback records to be read by the manager!\n\n\n\n",
    "preview": "posts/2020-12-14-classification-of-patient-feedback/img/grundzüge-der-mathematischen-geographie-und-der-landkartenprojection-96.jpg",
    "last_modified": "2021-07-30T15:24:10+01:00",
    "input_file": {}
  }
]
