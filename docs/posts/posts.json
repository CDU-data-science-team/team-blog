[
  {
    "path": "posts/2022-04-12-settings-in-ssms/",
    "title": "Settings in SSMS",
    "description": "Some of the options that may be useful in SSMS",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/letxuga007": {}
        }
      }
    ],
    "date": "2022-04-12",
    "categories": [
      "SQL",
      "Accessibility"
    ],
    "contents": "\r\nIn my team I tend to still use a lot of SSMS for SQL coding. Having\r\njust got a new computer I quickly realised that the settings I’d made to\r\nmake the layout a bit easier on my eyes had been lost but thankfully I’d\r\nnoted some of the options which I’m going to share here for future me or\r\nanyone else interested in the settings in SSMS. In terms of IDE I,\r\npersonally, find R Studio much better to set up for accessibility and\r\nappreciate their regular updates around this area. However, whilst I use\r\nR Studio (and specifically RMarkdown) for SQL workflows I don’t always\r\nget the speed or error notifications like SSMS depending on the packages\r\nI use.\r\nChanging the back screen\r\ncolour\r\nThis was a tip shared with me as a background that can help some\r\npeople who are dyslexic. I’m not dyslexic but much prefer this colour as\r\nit offsets the glare of the default white background. There aren’t set\r\noptions in SSMS so you have to set the colour manually and the options\r\ncan be found in Tools/Options/Font and colors.\r\nScreenshot of the menu from\r\nTools/Options/Font and colors with the background colour already set to\r\nyellowThe default yellow in theItem background is too bright\r\nfor me so I change it in Custom... setting the\r\nfollowing:\r\nHue: 35\r\nSat 205\r\nLum 194\r\nThe change will be implemented after the program is shut down and\r\nrestarted.\r\nChanging position of the\r\nstatus bar\r\nThe status bar defaults to the bottom of the screen and says which\r\nserver you are accessing, the database and it’s also the information\r\nwhere how long a query took to run and how many rows are returned. I\r\ndidn’t like the position being at the bottom so I move this to the top\r\nin  Tools/Options/Text Editor/Editor Tab changing the\r\nStatus bar location to Top.\r\n\r\nThe change will be implemented after the program is shut down and\r\nrestarted.\r\nChanging the colour of\r\nthe status bar\r\nThis is really useful if you have access to other databases that\r\nshare names and so can help with navigation. For example, I use a\r\nserver, a dev(elopment) server and a staging server with similar names.\r\nIt means that if I query the wrong one I’ll get strange results.\r\nChanging the colour can help when I need the server and dev open at the\r\nsame time and could get muddled.\r\nThis needs to be done when first connecting to the server when you\r\nselect the Connect to database. After choosing the server you\r\nwill want the colour bar to change for select the button called\r\nOptions >> that follows Connect,\r\nCancel and Help.\r\nIn the menu that appears you can tick Use custom color:\r\nand then Select to go to the colour’s menu.\r\nScreenshot of the menu for the SQL Server\r\nwith a custom colour already selectedAdding lines to the SQL\r\nscript\r\nWhen SQL gives errors it often refers to the line of code the error\r\nrelates to. One trick is to double click on the red text in the query\r\nresults and it will take you to the line (this doesn’t always work well\r\nfor CTEs Common Table Expressions and will just take you to the top line\r\nof the CTE).\r\nLine numbers on the script is not a default setting and have to be\r\nturned on, again in Tools/Options/Text\r\neditor/Transact-SQL/General:\r\nScreenshot of the menu with the line\r\nnumbers tickedThe change will be implemented after the program is shut down and\r\nrestarted.\r\nQuery results include the\r\nheader\r\nOften when I copy out the results from SSMS I also need the column\r\nheaders and this may not be default. Got to Tools/Options/Query\r\nResults/SQL Server/Results to Grid and select\r\nInclude column headers when copying or saving the results.\r\nAny other tips?\r\nThere are probably plenty of other settings that are useful to know\r\nor sites that list these out and we are always keen to learn more so\r\nplease do get in touch with creating an issue\r\nor emailing the\r\nteam.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-12-settings-in-ssms/img/notes_on_the_treatment_of_gold_ore.jpg",
    "last_modified": "2022-04-12T11:25:28+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-01-using-usethis-to-set-up-gitignore/",
    "title": "Using {usethis} to set up .gitignore",
    "description": "How to ensure that certain files cannot be accidentally committed to GitHub (or any other version controlled area).",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/letxuga007": {}
        }
      }
    ],
    "date": "2022-04-01",
    "categories": [
      "Open source",
      "GitHub"
    ],
    "contents": "\r\nUse .gitignore\r\n.gitignore is used to prevent committing certain files or folders to\r\nGitHub and can be set both locally in a project and globally.\r\nUsing the {usethis} package a lot of the set up of these files can be\r\ndone from the RStudio Console command line.\r\nSet up\r\nThese slides\r\nfrom Forwards are for package development but are fantastic to follow to\r\nget your computer set up with GitHub.\r\nCheck the Git connections are set up with:\r\n\r\n\r\nusethis::git_sitrep()\r\n\r\n\r\n\r\nIf Vaccinated: FALSE which it is most likely to be if\r\nthis is the first time {usethis} was used type:\r\n\r\n\r\nusethis::git_vaccinate()\r\n\r\n\r\n\r\nThis adds .DS_Store, .Rproj.user,\r\n.Rdata, .Rhistory, and\r\n.httr-oauth to the global .gitignore which can\r\nbe checked with:\r\n\r\n\r\nusethis::edit_git_ignore(scope = \"user\")\r\n\r\n\r\n\r\nThis open file can be altered directly and saved.\r\nChanging project .gitignore\r\nTo view and edit directly:\r\n\r\n\r\nusethis::edit_git_ignore(scope = \"project\")\r\n\r\n\r\n\r\nOr use the following to add individual files and folders like\r\nsecrets/:\r\n\r\n\r\nusethis::use_git_ignore(\"secrets/\")\r\n\r\n\r\n\r\nPotential issues with\r\nnetworks/VPNs\r\nIf git_vaccinate has and error starting with\r\n\r\nlibgit2::git_config_set_string\r\n\r\nand then a reference to a lock to a private network drive (usually\r\nwhen off the VPN/Network) then type:\r\n\r\n\r\nusethis::edit_git_config()\r\n\r\n\r\n\r\nand copy the following code, with <> changed as\r\nappropriate:\r\n[user]\r\n    email = <Email>\r\n    name = <GitHub Account name>\r\n[core]\r\n    excludesFile = C:/Users/<own folder>/.gitignore\r\n[init]\r\n    defaultBranch = main\r\n\r\nOverwriting a global\r\n.gitignore\r\nThis was a question that came out of a team Code Review and we found\r\nthe following blog\r\nfrom Scott Radcliff with the answer that yes, it is possible to\r\noverwrite a global setting if this is ever required.\r\nNot tested but examples using the ! to exempt:\r\nfor single files !data.csv\r\nfor a group of files !*.csv\r\nfor a subfolder !secrets/not-secret/ from Stackoverflow\r\noverride all rules in global !* from Stackoverflow\r\n.rda and .RData files\r\nWe also discussed .rda and .Rdata files\r\nwhich are save R data files. gitvaccinate() adds the\r\n.Rdata but doesn’t mention .rda files so be\r\naware of that.\r\nThe difference in the types of files are that .Rdata can\r\nstore single or multiple R objects but .rda can only save\r\nsingle.\r\nIt might be worthy of a blog/team review on its own but there are\r\nissues with saving multiple R objects to one place if they are updated\r\nregularly. It’s very easy to not re-save/update an R object and affect\r\nthe overall data workflow and so I, particularly as I’ve got into this\r\nvery muddle, only save one R object per .Rdata file.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-01-using-usethis-to-set-up-gitignore/img/grundzüge_der_mathematischen_geographie_und_der_landkartenprojection.jpg",
    "last_modified": "2022-04-12T11:40:02+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-21-github-sop/",
    "title": "GitHub Standard Operating Procedure",
    "description": "How we use GitHub in the CDU data science team.",
    "author": [
      {
        "name": "Chris Beeley",
        "url": {
          "https://twitter.com/chrisbeeley": {}
        }
      }
    ],
    "date": "2022-03-21",
    "categories": [
      "Open source",
      "GitHub",
      "Teamwork"
    ],
    "contents": "\r\nIntroduction\r\nWe’ve been using GitHub in the CDU data science team since 8th May 2020 (I think- that’s the oldest commit I could find). Team members past and present have used git and GitHub to varying degrees, often solo. Data science is a team sport, without question, but like any team sport it requires proper coordination between participants to be effective. Git and GitHub are very powerful tools, but they are hard to use well even when you’re collaborating just with yourself, and working well with others is another set of skills in itself. We’ve all made a lot of mistakes and learned a lot in the last two years, and this post is designed to codify some of this learning into a Standard Operating Procedure (SOP) for the benefit of team members past and present. And of course, we share it for the general benefit of others who might learn something from our experiences, and also in the hope that those with more/ different skills and knowledge might be able to help us refine it further (get in touch through the usual channels ☺️).\r\nStandard operating procedure\r\nBranches\r\nWe adopted gitflow fairly early on as the team expanded and it worked very well for our purposes. I now see that there is a health warning on the linked post and gitflow is deprecated elsewhere in favour of something like GitHub flow. I’m not going to go into the differences here because it is not the focus of this post but there are some important differences and I can feel another blog post coming on about different models of using git and GitHub once I’ve discussed the matter with team members.\r\nThe essential feature of gitflow as we’re using it is that there are three types of branches- main branch, which is the working code, development, which is a kind of “staging” branch where code can be worked on and tested and feature branches which either fix bugs or add new features. The main idea is that feature code is merged into the development branch and the development branch is periodically merged into the main branch. Each time you merge to main you do a point release with semantic versioning.\r\nWe’ve all found that it works pretty well but it is overkill for simpler repos. The development branch is not necessary, and you can merge straight to main from a feature branch for simpler things, especially where it is mainly one person writing, maintaining, and using the code. For important stuff that we all use though, particularly {nottshcData} the development branch is really useful because it means that we can all send PRs to a branch without worrying about untested code getting out into the wild.\r\nWith all that said how have we found it best to use it in practice? A few points:\r\nMerge to development and main as frequently as possible. Especially from my point of view, as the team manager, I spend a lot of time on everyone’s repos, and if everything is tucked away in branches I can’t see it.\r\nDocumentation should go straight to main, there is no need to use branches at all, or if a branch is used it can be continuously merged to main (in case anyone else is writing documentation)\r\nBranches should be deleted as soon as possible, for the same reason, especially if they include commits that are not on main. If they are ahead of main that implies that they could have useful stuff on, or be a failed experiment that needs binning. If you’re off sick and we’re running your code we don’t want to be wondering which it is\r\nCollaboration\r\nThere should be one person in charge of each repo. That person is responsible for making sure that the main branch is clean, bug free, and properly documented. If a team member spots something on someone else’s repo that either doesn’t work or doesn’t look right they should either file an issue (if it’s something that might need discussing) or make a pull request (PR) - if it’s a simple fix to formatting or documentation - I send lots of PRs like this. It is the responsibility of the lead for the repo to check the PR and to merge it in in a timely fashion, request changes to it, or reject it.\r\nHaving a PR accepted is a privilege, not a right, so team members who want to improve a repo need to send a clean PR with a decent explanatory note in it. We might discuss over Slack or in person but in my opinion it’s better if that discussion happens on GitHub where possible because it means everyone, all team members and the open source community at large, can see it. I’m subscribed to all of the repos on our GitHub and I quite often spot areas for improvement or niggles by reading these discussions and it’s my job to understand the workflow and to suggest improvements.\r\nIssues\r\nBug reports and feature requests should all be filed as issues, again to increase transparency of ongoing work (within and without the team). Issues should be clearly worded with enough detail and should wherever possible include a reprex. We were bad with issues in the early days, jotting notes to each other that made a lot of sense at the time but within a month or so they were incomprehensible even to the author. Like code commenting and documentation a little work now can save a lot of confusion in the long run.\r\nReprexes\r\nI really can’t say enough about the reprex. The beauty of a reprex for me is that at least half the time making the reprex solves the problem. Boiling the problem down to its essential elements improves your understanding of the problem to such a degree that it either disappears completely or you can PR the bug fix yourself. And when it doesn’t it’s so valuable to have it as the person receiving the issue. If you don’t include a reprex, basically, don’t expect a fix, unless there’s a really good reason why you can’t include one.\r\nData security\r\nData security is obviously really important when you’re using GitHub, even if it’s a prviate repo. The simplest way to keep data security is don’t keep data in a git controlled folder. If you don’t do that, you will never accidentally push data to a repo. It’s impossible. A lot of our code just gets stuff straight from the server and processes it all in place, so there really is no need a lot of the time to save data anywhere.\r\nIf you absolutely must save data, for example if you have long running or complex data operations to complete, just make sure that you use .gitignore appropriately. My usual strategy is to have a folder called secret, and I add secret/ to .gitignore. Do that and check what you’re committing and pushing.\r\nIf the worst happens, and I should say the worst has never happened to us and I don’t think it ever will, you need to destroy all the commits with the data in.\r\nData security is a very important area and in my very inexpert opinion using GitHub doesn’t make data any less secure. Using GitHub means that you are always thinking about what’s in your data and it reduces sloppy practices like storing credentials in code (which you should NEVER do, even if you’re not using git- store them in environment variables). I read about so many information breaches from the NHS and they seem to be largely people emailing stuff about without concentrating. It’s complacency, they’re so used to everything being secure they forget what they’re doing.\r\nA note on packages\r\nThis is not strictly about GitHub, but it fits in the general area of collaboration and so I wanted to include it. We have adopted R packages almost wholesale in the team (another blog post that needs to be written), but there is one minor problem that they introduce that I wanted to flag up here.\r\nPeople get into the habit of building the package as they go, and then they use the built package for their analysis. This is fine as far as it goes, but you should never deploy or share anything based on code that you built yourself. Before you share or deploy you need to merge to main, switch to a new session and install the code from GitHub. This ensures that the code that you are running is the same code we all have access to. We’ve all spent far too long debugging code that only exists on one person’s laptop, and if you’re off sick and we redeploy your stuff you want to make sure we deploy what you actually want, because we can’t access the built stuff on your laptop.\r\nHelpful resources\r\nNHSX draft guidance\r\nGDS guidance\r\nHealth foundation guide to sharing code\r\nA collection of links I curate\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-03-21-github-sop/preview_pic.jpg",
    "last_modified": "2022-04-01T20:16:11+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-04-open-source-within-the-NHS-benefits-and-possible-pitfalls/",
    "title": "Open source within the NHS - benefits and possible pitfalls",
    "description": "What does open source mean for the average staff member in the NHS? What benefits does it bring to patients?",
    "author": [
      {
        "name": "Oluwasegun Apejoye",
        "url": {
          "https://uk.linkedin.com/in/oluwasegun-michael-apejoye": {}
        }
      }
    ],
    "date": "2022-03-04",
    "categories": [
      "Open source"
    ],
    "contents": "\r\nIntroduction\r\nHealthcare needs to prioritise clinical safety and data security and\r\nthere have been concerns in the past about using and producing open\r\nsource within the NHS. In a simple form, working in the open means\r\nmaking the source codes, that powers our software or application, freely\r\navailable and downloadable from public platforms such as GitHub. This\r\nenables collaboration across the board to maximize benefits for the\r\ndelivery of health and social care services.\r\nThe term “open source” has been in use for a long time, and\r\nit is increasingly gaining traction in the NHS. And, we the clinical developmental\r\nunit – data science team at NottsHC are part\r\nof the open-source campaign within the NHS. We are dedicated to\r\nworking in the open and to using open source tools for our analysis.\r\nAs part of the effort to push for open source, the NSHX has announced\r\nthat NHS Open\r\nSource Policy will officially become available by this summer.\r\nAccording to the draft document, the policy “aims to provide a single\r\nposition and source of guidance for anyone developing open software for\r\nor with the NHS in England”. The announcement about the policy has been\r\nbroadly embraced by the NHS-R community (an open community of R\r\nusers).\r\nBut what does open source translate to for the average staff (with\r\nlimited technical skills) in the NHS? What benefit does this bring to\r\npatients?\r\nA member of our team already did some justice to this subject matter\r\nin her post working-in-the-open,\r\nwhere she shared some of her personal experiences and did some justice\r\nto the benefit of open source. However, with this post, I hope to share\r\nthe general benefit and touch on some more issues about working with\r\nopen source.\r\nBenefits\r\nGreater collaboration: open source enables technical (IT\r\nexperts, developers, analysts) and non-technical (clinicians, nurses,\r\nGPs) staff to input into projects and collaborate easily. It facilitates\r\neasy sharing of software, codes, analytical products etc, and it opens\r\ndoors for reproducibility. With open source, a small idea can easily\r\nbecome a proof of concept which can grow to become a project that ends\r\nup improving care delivery.\r\nCost saving: using an already published code to serve some\r\naspects of our task will lead to reduced duplication of effort, a\r\nreduction in staff time committed to projects and improved staff\r\nefficiency, a faster roll-out of cost-saving solutions and it enables\r\nteams to pursue the best approaches, not just those available locally.\r\nAlso, the awareness of public scrutiny will ensure teams make their\r\nprojects fit for purpose. In addition, embracing open-source tools means\r\nthe NHS will benefit from savings from licensing fees as the cost of\r\nopen-source tools tends to be lower and they also provide greater\r\ncommitment/contract flexibility compared to proprietary tools which\r\ncurrently lock-in the organisation to the supplier.\r\nAvailability of cutting-edge tools: All open-source projects\r\nbenefit from contributions from a wide variety of users. A good example\r\nis the Python and R programming languages, which are widely used tools\r\nfor a wide range of cool stuff. These two projects have benefited from\r\ncontributions that now make them a go-to tool for different tasks.\r\nCommunity effort: The open-source community runs on the\r\nphilosophy of universally shared knowledge because it benefits from a\r\nlarge and growing network of developers and users that are continually\r\ncontributing to projects. With an open-source approach to project\r\ndevelopment, teams from other organizations can take our project, make\r\nchanges to it and make the changes available to the community (including\r\nthe original author) to benefit from the additional feature or\r\nimprovement. This accessibility allows for constant developments and\r\nimprovements to the software.\r\nSafety and Security: Does open source mean we sacrifice\r\npatients’ privacy or lose control of projects? No. The organisation\r\nauthoring the project maintains control of the data at all times and\r\nensures it remains private and confidential, and can permit or restrict\r\nreuse of code using different software licences.\r\nAdditional income stream: The authoring team of an open\r\nsource project can offer to provide subscription services, support and\r\nmaintenance to interested users and therefore generate passive income\r\nfor their trust.\r\nWhat to look out for\r\nIt’s a no-brainer that embracing open source will bring several\r\nbenefits to teams within the healthcare system. However, there are still\r\nsome pitfalls to look out for. Below are some of the ones I have\r\nidentified:\r\nImportance of protecting data: Sharing open source in\r\nhealthcare settings requires safeguards to protect data, and unless\r\nsuitable synthetic data can be produced it can be hard to allow others\r\nto run code that depends on data.\r\nDocumentation and support: Sometimes documentation and\r\nsupport for open source is inferior to proprietary alternatives and this\r\ncan lead to delays and extra work for teams.\r\nLack of sustainability: it is possible to use packages (set\r\nof codes) developed by another team in your project. However, there is a\r\nrisk of the package being abandoned without prior notice by the team\r\nwhich can create problems with incompatibility and require rewriting of\r\nexisting code.\r\nTo mitigate the above issues, there is a need for robust policies to\r\nhelp analysts, developers and other users follow a consistent approach\r\nto open source. And I believe that is where the NHS Open Source\r\nPolicy proves useful.\r\nConclusion\r\nOpen source has many benefits to offer if embraced by all teams (both\r\ntechnical and non-technical) within the healthcare system. Despite these\r\nbenefits, there is a need for robust open source policies to ensure all\r\nteams work in a consistent way that will maximize the benefits for the\r\nquality of care delivered to patients, and the efficiency of the health\r\ncare systems without compromising patient safety and data\r\ngovernance.\r\nIf you want to read more about our open source journey, then keep in\r\ntouch by following us on Twitter and GitHub.\r\nFurther reading\r\nNHSX open source\r\npolicy\r\nNHS\r\ntechnology: Being open to open source\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-03-04-open-source-within-the-NHS-benefits-and-possible-pitfalls/img/image.jpg",
    "last_modified": "2022-04-01T20:19:40+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-17-package-workflow/",
    "title": "Package Workflow",
    "description": "From a team time session discussing the workflow to contributing to a \n(currently) private package",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Letxuga007": {}
        }
      }
    ],
    "date": "2021-12-17",
    "categories": [
      "Packages",
      "Workflow",
      "Teamwork"
    ],
    "contents": "\r\nTeam time\r\nOur team has regular team times but which don’t always focus on code.\r\nQuite often we use the opportunity to talk through processes and\r\napproaches to analysis and these notes reflect a recent code review\r\nwhere were discussed some of our learning on package development and\r\nworkflow using GitHub for our (currently) private repository that takes\r\ndata from the SQL data warehouses and makes it tidy following on from\r\nMilan Wiedemann’s nottshcverse.\r\nThese are some of the agreed points and discussions we had about some\r\nof the processes that have developed organically through working\r\ncollaboratively. This process is very flexible and may change over time\r\ndepending on our own developing skills or from others’ input (we are\r\nalways keen to know how to improve our workflows!) so this blog may be\r\nchanged or superseded in the future.\r\nCurrently we are using a basic workflow of:\r\nissue -> create a branch -> branch to development ->\r\ndevelopment to main -> package installed on people’s machines and on\r\nthe R Studio Connect server\r\nStart with an issue\r\nIssues are not just for bugs and should also be used for features and\r\nquestions. The benefits of issues are that they are highly searchable\r\nand can show thinking on subjects from something that may be a question\r\nbecoming a feature. If people are following the repository they will be\r\nnotified of any issues. You can also @ specific people in the issue\r\nitself which is good if you are the owner of the issue but want input\r\nfrom someone.\r\nScreenshot of GitHub repository with the\r\nWatch button in the top right highlightedIssues often have one thing detailed but can have many parts to them\r\nin a task list:\r\nScreenshot of an issue with tasks listed,\r\nsome tickedThe tasks can be ticked in the viewing mode without having to go to\r\nthe code. To get the task list box type - [ ] and when it\r\nis ticked it becomes - [X]. Note that this is sensitive to\r\nspaces so any extra spaces will just show the code on the view mode.\r\nWe agreed in our team time that all issues, even questions, should be\r\nclosed with a good summary of why it’s closed. For example, we had an\r\nissue discussing whether it could be a feature to combine functions that\r\nwere reliant on each other to save typing them out in analysis. This led\r\nto a discussion which resulted in no changes to the code (i.e. no\r\nbranches) but the issue was closed with a detail on why no work was\r\nstarted on it. This is useful for knowledge sharing with other\r\ncolleagues, and maybe even the future you, if the suggestion were ever\r\nto come up again\r\nCreate a branch\r\nFrom an issue a branch will need to be created and however you do\r\nthis, either through R Studio or the terminal, ensure that the name\r\nstarts with the number from the issue. The following is an image from\r\nthe terminal in R Studio which shows all the branches in the project\r\nfrom the command git branch.\r\n$ git branch\r\n  101-test-colm-names\r\n  101-test-column-names\r\n  194-restrict-rio-demog\r\n  232-calc-ips\r\n  233-update-readme\r\n  243-add-ulysses\r\n* 262-calc-team\r\n  change_calc_tot\r\n  connections\r\n  development\r\nThe active branch has a star next to it or appears in a different\r\ncolour depending on the terminal and settings being used.\r\nOne of the downsides of not linking issues to branches is that any\r\ninformation on the work being done (difficulties or thoughts on the\r\nwork) could be somewhere like a README but that is version controlled in\r\ncode. Issues allow a form of version control but more in a sense of a\r\nconversation.\r\nUpdating the branches\r\nSome work/features take a long time to complete, and in the meantime\r\nthe development and/or main branches could\r\nhave changed. This can be a good thing but can also mean that you may be\r\nrepeating some work that has already been done, for example, formatting\r\nsome code so that it is within 80 characters which is what our team has\r\nagreed to do for code scripts.\r\nTo update the local branch from another there are two git commands\r\nthat can be used, either fetch or pull.\r\n$ git pull origin main does the fetch and\r\nmerges. It only shows the changes if there is a conflict and is ok to\r\nuse if you trust that the code works on the branch you are pulling\r\nfrom.\r\n$ git fetch origin main will not necessarily look like\r\nit’s done anything as the files will not be updated or changed, but it\r\ndoes update all the metadata on the changes. fetch is\r\nreally good to use if you only want to take some of the changes, but not\r\nall.\r\nFurther information on how to use git fetch https://www.atlassian.com/git/tutorials/syncing/git-fetch.\r\nDevelopment branch\r\nFor this package we have agreed to have branches merge to\r\ndevelopment before being merged (once tested and used in\r\nactual analysis) into main. This means that the\r\ndevelopment branch can rapidly move on in versions compared\r\nto main. We originally did releases on both\r\nmain (through GitHub) and on development using\r\nusethis::use_version() which updates the version in\r\nDESCRIPTION and creates/updates a NEWS.md file\r\nwhich is a good way of keeping track of changes.\r\nTo “release” on GitHub and match this documented version the steps\r\nare outlined here: https://docs.github.com/en/repositories/releasing-projects-on-github/managing-releases-in-a-repository\r\nTagging commits in git\r\nHowever, in team time we agreed that releases should only be on\r\nmain and to keep a track on version in\r\ndevelopment we will instead use tags through Git.\r\nWe did a test tag in the Terminal:\r\ngit tag -a v0.9.1 -m \"Testing (and there is a still a bug)\"\r\nand checked in R Studio, in the Commit History, to see if the tag was\r\nlisted as this will be a good way to check people’s versions of the\r\npackage when working collaboratively.\r\nFurther information on tagging commits in git: https://www.atlassian.com/git/tutorials/inspecting-a-repository/git-tag\r\nChecking versions\r\nWhen working collaboratively with someone on code it can be very easy\r\nto have a different version of the package so we’ve agreed to always\r\nbuild from the main branch. We do this directly from GitHub\r\nto ensure we are all building from the same code. However, when working\r\nlocally from development or from another branch we suggest\r\nusing {devtools} and loading the package to the session using\r\nCtrl + Shift + L.\r\nIf the workflow was to always Build, it may cause issues\r\nwith analysis as it will be the package all sessions use. It very easy\r\nto build from an experimental branch but then forget, do some analysis\r\nusing the package and not realise that the package is different to\r\nmain, which in turn is different to the team’s version and\r\nthe R Studio Connect version.\r\nTagged commits\r\nTo check for tags, in R Studio, in the Git panel (top right as\r\ndefault) and the history icon of a clock.\r\nScreenshot of the top right panel icons\r\nwith the clock highlighted for HistoryIt’s also possible to get to the same screen by selecting the Commit\r\nbutton and then the history tab.\r\nScreenshot of the Commit window with the\r\ntab history hightlightedIn the commit history pane the tagged commit has a slightly different\r\ncolour as well as being its own commit.\r\nScreenshot with the tag version\r\nhighlightedWords of caution with\r\ncommits\r\nWhy you should write\r\ninformative commits\r\nAs we were discussing how good messages in commits can help located\r\nolder versions of code, we realised that the link to the issue or the\r\nbranch is lost from the commits once merged to another branch.\r\nConsequently, commits like “Fixed” won’t necessarily make sense when\r\nremoved from the context like it being on a branch called\r\n207-code10-error. There is no reference in “Fixed” as to\r\nwhat was changed or what the error/problem was. It’s therefore advisable\r\nto get into the habit of writing informative commits as if you were\r\nwriting them to someone else even if, at the moment, you may be the only\r\none working on the project.\r\nWhen to commit\r\nWe had a divergence in the team about whether to commit chunks of\r\nworking code or smaller steps. This was really down to personal\r\npreference but which were both based in differences in approaches to\r\nlocating historic code changes. The ‘lots of commits’ approach can be a\r\nlot of text to go through to find when the code last ‘worked’. However,\r\nmany individual commits for particular actions can make it easier to\r\npull out one commit that needs removing rather than re-work whole\r\nchunks.\r\nOne thing we did all agree on was that the first commit is usually a\r\nlarge chunk of code and is often committed as\r\n--First commit or something similar. We also all had used\r\nthe Amend previous commit in R Studio.\r\nScreenshot of the commit pan from R\r\nStudio with the Amend Previous Commit highlightedTicking this button takes you to the last commit which can be changed\r\nand it also means that you can add new files or changed files to the\r\ncommit that you may have missed. Changing the last commit doesn’t work\r\nso well though if the commits have already been pushed as that will put\r\nthe two last commits out of synch and that requires it’s own blog in how\r\nto rectify!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-17-package-workflow/img/system-of-mineralogy-comprehending-oryctognosie-p687.jpg",
    "last_modified": "2022-04-01T21:08:52+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-18-personal-access-tokens/",
    "title": "Personal Access Tokens",
    "description": "Connecting to the GitHub to install packages from a private GitHub repository requires security \nset ups and this blog details how to do it (and how not to do it).",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Letxuga007": {}
        }
      }
    ],
    "date": "2021-12-17",
    "categories": [
      "Packages",
      "Security"
    ],
    "contents": "\r\nInstallation\r\nBecause some of the packages we are using are (currently) private repositories it is not possible to build using just {remotes} as a Personal Access Token, specifically a GITHUB_PAT, is needed. The {usethis} package suggests using the local Git credential store but this wasn’t compatible with some of our team’s set ups so later in this blog are details on how to do it the “inadvisable” way to be used at your own discretion.\r\nCheck you have a token\r\nThis part is the same for whatever method you use on your own computer as this section details how to get the GITHUB_PAT set up. To check for existing GITHUB_PAT use code:\r\nSys.getenv(\"GITHUB_PAT\")\r\nIf nothing has been set up by you on GitHub this may return an empty string \"\".\r\nSet up a token\r\nFirstly, go to https://github.com/settings/tokens which takes you to your own personal GitHub account settings where you will need to Generate new token. Give this a suitable name like Repo access and tick the repo group for full control of private repositories.\r\nRemember to copy the code that is generated as this cannot be viewed again\r\nFrom the Git credential store\r\nPackages required\r\n\r\n\r\ninstall.packages(\"usethis\")\r\ninstall.packages(\"gitcreds\")\r\ninstall.packages(\"remotes\")\r\n\r\n\r\n\r\n{usethis} suggests using the local Git credential store quite strongly!\r\n\r\nIf you have previously set your GitHub PAT in .Renviron, stop doing that.\r\n\r\nSet up\r\n\r\n\r\n\r\nGitHub will open and, if you are not already logged in, you will need to enter your (GitHub) password.\r\nThe page for GitHub tokens management is https://github.com/settings/tokens and you can do the same as the code in this screen by selecting Generate new token. Give this a suitable name like Package installation and tick the repo group for full control of private repositories.\r\nRemember to copy the code that is generated as this cannot be viewed again\r\nNext set up a GITHUB_PAT in RStudio:\r\ngitcreds::gitcreds_set()\r\nYou will then be prompted to enter the copied token (no quotes are required). If you’ve already got a token entered the following message will show:\r\n-> Your current credentials for 'https://github.com':\r\n\r\n  protocol: https\r\n  host    : github.com\r\n  username: PersonalAccessToken\r\n  password: <-- hidden -->\r\n\r\n-> What would you like to do? \r\n\r\n1: Keep these credentials\r\n2: Replace these credentials\r\n3: See the password / token\r\n\r\nSelection: ```\r\nEnter an item from the menu, or 0 to exit\r\nSelection: \r\nReference: https://usethis.r-lib.org/articles/articles/git-credentials.html\r\nInstalling\r\nThe {usethis} documentation suggests using {pak} but this may not work on network drives/VPNs (there are a few issues that have been opened and quickly closed referring to these as potential problems) and so in the meantime combining {remotes} with {gitcreds} is a workaround:\r\n# install.packages(\"remotes\")\r\nremotes::install_github(\"<name of GitHub account>/<name of repository>\", \r\n    auth_token = gitcreds::gitcreds_get(use_cache = FALSE)$password)\r\nThe absolutely not recommended method\r\nPackages required\r\n\r\n\r\ninstall.packages(\"usethis\")\r\ninstall.packages(\"remotes\")\r\n\r\n\r\n\r\nIf you are sure this is what you want to do then type in RStudio:\r\nusethis::edit_r_environ()\r\nIn the file that opens, type into it (where YOUR-PAT is the generated code you’ve copied) and save the file:\r\nGITHUB_PAT=YOUR-PAT\r\nRestart R and run Sys.getenv(\"GITHUB_PAT\") to check that the \"\" has changed.\r\nReference: https://www.jumpingrivers.com/t/2019-user-git/03-githubpat.html#6\r\nInstalling\r\nNow, for as long as the token exists, the following code will install the package:\r\n# install.packages(\"remotes\")\r\nremotes::install_github(\"<name of GitHub account>/<name of repository>\", auth_token = Sys.getenv(\"GITHUB_PAT\"))\r\nBuilding package\r\nIt is preferable to build from the repository rather than locally as this should be what everyone in the team has access to. To build and test any new functions use Ctrl+Shift+L from the {devtools} package to load locally for the session.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-18-personal-access-tokens/img/british-mineralogy-or-coloured-figures-p132.jpg",
    "last_modified": "2022-04-01T20:16:11+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-06-nottshcverse/",
    "title": "Development of open tools for analysing healthcare data in R",
    "description": "***TLTR: (Too Long To Read)*** \nOur goal was to make it easier to work with healthcare data in a reproducible and collaborative way.\nWe wrote lots of R functions for recurring data manipulations and analytical tasks that magically translate into SQL code and communicate with large databases.\nAll our functions are grouped into R packages because this made it easier for us to: *(i)* write good documentation of our code and analytical tasks, *(ii)* easily distribute updates across all team members, *(iii)* formally test our code, and *(iv)* integrate common data manipulations (or analyses) into interactive dashboards in a modular way.",
    "author": [
      {
        "name": "Milan Wiedemann",
        "url": {}
      }
    ],
    "date": "2021-09-29",
    "categories": [
      "Open source",
      "Teamwork",
      "Packages"
    ],
    "contents": "\r\n\r\nContents\r\nChallenges and\r\nsolutions\r\nOverview of the\r\n{nottshcverse}\r\nSimplified working example\r\nCreate example data\r\nCreate SQLite\r\nconnection\r\nExample analysis\r\nWriting a function\r\nExample SQL code\r\nExample results\r\n\r\nWatch out now\r\nSummary\r\nRelated work and resources\r\n\r\n\r\n\r\n\r\nOur team works with routinely collected NHS patient data. Currently\r\nwe focus on understanding better how patients are using the service,\r\nchanges in clinical outcome measures, and analysis of patient\r\nexperiences. The main questions that guide our work are ‘What works\r\nfor whom and how does it work?’, ‘What doesn’t work?’, and\r\n‘How can we integrate patient experiences into our analyses?’.\r\nWe developed a set of different tools, the\r\n{nottshcverse}, to help us look at these\r\nquestions by automating recurring and time-consuming tasks so that we\r\ncan spend more time thinking the clinical questions.\r\nChallenges and solutions\r\nReal data is messy but we should do our best to tidy the mess, where\r\npossible in an automated way. There are many challenges when working\r\nwith healthcare data, here is only a small selection of those that I\r\nthink underlies most analyses. Figure\r\n1 shows our solutions to these\r\nchallenges.\r\n\r\n\r\n\r\nFigure 1: Main goals that guided the\r\ndevelopment of the {nottshcverse} packages.\r\n\r\n\r\n\r\n🤔 It’s really hard to get the clinical data that is needed in a\r\nreproducible way that is consistent across different people who work on\r\nthe same (or related) analysis. This is particularly true because most\r\nof the data is stored on SQL servers and only starts to make sense after\r\njoining multiple different datasets. 🤓 We wrote functions that\r\nmade it very easy and secure to (i) connect to and\r\n(ii) query data from different databases that we are working\r\nwith. This way we could get the data we needed for our analyses using\r\nvery few lines of code in R.\r\n\r\n\r\n# First, create connection to databases\r\nconn_s1 <- connect_sql(server = \"DB-one\")\r\nconn_iapt <- connect_sql(server = \"DB-two\")\r\n\r\n# Now we can use the connection to get contacts data from SystmOne ...\r\ndb_contacts_s1 <- get_s1_contacts(from = \"2020-01-01\", \r\n                                  to = \"2020-12-31\", \r\n                                  conn = conn_s1)\r\n\r\n# ... and IAPTus databases\r\ndb_contacts_iapt <- get_iapt_contacts(from = \"2020-01-01\", \r\n                                      to = \"2020-12-31\", \r\n                                      conn = conn_iapt)\r\n\r\n# Note that the objects 'contacts_s1' and 'contacts_iapt' are just pointing to \r\n# the databases (I like to use the prefix db) and not actually downloaded to the\r\n# environment on your computer.\r\n\r\n\r\n\r\n🤔 Most of the time, the data is not in the format that is needed\r\nfor further analyses. There may be specific data manipulations that are\r\nneeded to make sense of the data and analyse it properly. Also,\r\ndifferent databases might be set up in ways that it is hard to merge\r\ndata. 🤓 We wrote function that tidy the raw data from the\r\ndatabases so that it is more consistent across databases and easier to\r\nanalyse. All of this still happens within the server so that our\r\ncomputers don’t have to do all of this work.\r\n\r\n\r\n# Each get_*_data() function comes with a tidy_*_data(), here tidy_s1_contacts()\r\n# Here I use the connection to the raw (messy) contacts  data that I created above\r\n# and tidy it using the tidy_s1_contacts() function\r\ndb_contacts_s1 <- db_contacts_s1 %>% \r\n  tidy_s1_contacts()\r\n\r\ndb_contacts_iapt <- db_contacts_iapt %>% \r\n  tidy_iapt_contacts()\r\n\r\n\r\n\r\n🤔 The methods of data analyses and visualisations should be\r\nunderstandable, reproducible, and available to other people.\r\nUnfortunately this is not always the case yet because the software tools\r\nthat are used are not script based and often shared in private emails.\r\n🤓 We wrote R functions for common analytical tasks and\r\nvisualisations.\r\n🤔 Code should be really well documented so that it is easy to\r\nunderstand what’s going on. This includes the current version of the\r\ncode as well as all previous versions and changes. This can be done\r\nusing tools like Git and GitHub, but unfortunately most code is\r\ncurrently shared undocumented in private emails. 🤓 We created\r\ndetailed documentations that are easily accessible to everyone who uses\r\nour R packages. Also, because develop our tools on GitHub, every change\r\nto our code is documented.\r\n🤔 Mistakes happen! Sometimes things that you have no control\r\nover can change and break your code that previously worked fine (e.g.,\r\nthe format of the raw data or a functions that someone else wrote).\r\nTherefore, it’s important to continuously test whether the code is still\r\nworking the way it’s supposed to work. 🤓 R packages (or similar\r\nsolutions in other statistical programming languages) are relatively\r\neasy to test, for example using the {testthat} package. We started to\r\nimplement tests into our work so that we can check if changes that we\r\nmake to our code don’t break anything.\r\nOverview of the\r\n{nottshcverse}\r\n\r\n\r\n\r\nFigure 2: Overview of some R packages\r\ndeveloped by the Clinical Development Unit Data Science Team⭐and the\r\nNHS-R Community ❤️\r\n\r\n\r\n\r\n{nottshcData}: Unified framework to query, transform,\r\nand aggregate data from different databases\r\n{nottshcMethods}: Tools for performing common\r\nanalytical tasks (e.g., grouping continuous age into groups)\r\n{honos}, {LSOApop}: Packages designed in\r\ngeneric way to help use and others {nottshcData} work with\r\nspecific questionnaires (e.g., Health of the Nation Outcome Scales,\r\nHoNOS) or open data sets (e.g. LSOA population estimates)\r\n{outcomesdashboard}: Our dashboards use all the\r\npackages mentioned above + special packages developed specifically to\r\nsupport the dashboards with helper functions\r\nSimplified working example\r\nTo illustrate how R can be used to work with databases I’ll use the\r\nfollowing example. Imagine we’re working with a database called\r\nSystmTwo (S2) and need to use two different\r\ntables for our analysis:\r\n[S2].[contacts]: Information about contacts with\r\nclinical teams\r\n[S2].[demographics]: Some demographic information\r\n\r\n\r\n\r\nCreate example data\r\n\r\n\r\n# Set up example contacts table\r\ncontacts_s2 <- tibble(client_id = c(1, 1, 1, 2), \r\n                      contact_id = c(123, 124, 125, 156), \r\n                      referral_id = c(456, 459, 500, 501), \r\n                      referral_date = c(\"2018-04-19\", \"2019-05-23\", \r\n                                        \"2020-06-01\", \"2018-12-11\"),\r\n                      contact_date = c(\"2018-05-19\", \"2019-06-05\", \r\n                                       \"2020-07-08\", \"2019-01-15\"),\r\n                      team_id = c(\"tm1\", \"tm2\", \"tm1\", \"tm1\"), \r\n                      hcp_id = c(\"hcp1\", \"hcp2\", \"hcp1\", \"hcp1\"), \r\n                      contact_type = c(\"phone\", \"f2f\", \"video\", \"phone\"),\r\n                      assessment_id = c(321, 322, 344, NA))\r\n\r\n# Set up example demographics table with 2 patients\r\ndemographics_s2 <- tibble(client_id = c(1, 2), \r\n                          dob = c(\"1988-01-01\", \"1965-01-01\"),\r\n                          dod = c(NA, NA),\r\n                          sex = c(\"f\", \"m\"))\r\n\r\n\r\n\r\nCreate SQLite\r\nconnection\r\n\r\n\r\n# Create connection (conn) to \"local\" database called SystmTwo (s2)\r\nconn_s2 <- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\r\n\r\n# Copy local data frame to conn_s2 database\r\ndb_s2_contacts <- copy_to(conn_s2, contacts_s2)\r\ndb_s2_demographics <- copy_to(conn_s2, demographics_s2)\r\n\r\n\r\n\r\nExample analysis\r\nHere we join the contacts with the demographics information to\r\ncalculate the age at the time a patient has their contact\r\n(age_at_contact).\r\n\r\n\r\n# Calculate age at time of contact\r\ndb_age_at_contacts <- db_s2_contacts %>% \r\n  left_join(db_s2_demographics) %>% \r\n  mutate(age_at_contact = as.Date(contact_date) - as.Date(dob))\r\n\r\n\r\n\r\nWriting a function\r\nWe can also write our own functions and use them in a modular way\r\nwhenever we need them. Here’s a simple example to demonstrate how we can\r\ndo the same calculation as shown above using our own function. In this\r\nexample the function arguments take the variable names for date of birth\r\n(dob) and the contact date (contact_date).\r\n\r\n\r\ncalc_age_at_contact <- function(data, var_dob, var_contact_date) {\r\n  # Add code here to check that arguments are specified correctly\r\n  data %>% \r\n    dplyr::mutate(age_at_contact = as.Date({{var_contact_date}}) - as.Date({{var_dob}}))\r\n  }\r\n\r\n\r\n\r\nExample SQL code\r\nAs mentioned above (Challenges and solutions, Point 1), the object\r\nthat we work with most of the time are just SQL queries and not real\r\ndata stored in your R environment. We can look at the underlying SQL\r\ncode using the dplyr::show_query() function. I don’t really\r\nknow SQL very well myself, but some people who do have created a great\r\npackage that translates R code into SQL code (see the dbplyr\r\npackage for more).\r\n\r\n\r\n# Use dplyr::show_query() function to see underlying SQL code\r\nshow_query(db_age_at_contacts)\r\n\r\n\r\n<SQL>\r\nSELECT `client_id`, `contact_id`, `referral_id`, `referral_date`, `contact_date`, `team_id`, `hcp_id`, `contact_type`, `assessment_id`, `dob`, `dod`, `sex`, CAST(`contact_date` AS DATE) - CAST(`dob` AS DATE) AS `age_at_contact`\r\nFROM (SELECT `LHS`.`client_id` AS `client_id`, `contact_id`, `referral_id`, `referral_date`, `contact_date`, `team_id`, `hcp_id`, `contact_type`, `assessment_id`, `dob`, `dod`, `sex`\r\nFROM `contacts_s2` AS `LHS`\r\nLEFT JOIN `demographics_s2` AS `RHS`\r\nON (`LHS`.`client_id` = `RHS`.`client_id`)\r\n)\r\n\r\nNote that we can also see the SQL code from our own functions.\r\n\r\n\r\ndb_age_at_contacts %>% \r\n  calc_age_at_contact(var_dob = dob, \r\n                      var_contact_date = contact_date) %>% \r\n  show_query()\r\n\r\n\r\n<SQL>\r\nSELECT `client_id`, `contact_id`, `referral_id`, `referral_date`, `contact_date`, `team_id`, `hcp_id`, `contact_type`, `assessment_id`, `dob`, `dod`, `sex`, CAST(`contact_date` AS DATE) - CAST(`dob` AS DATE) AS `age_at_contact`\r\nFROM (SELECT `client_id`, `contact_id`, `referral_id`, `referral_date`, `contact_date`, `team_id`, `hcp_id`, `contact_type`, `assessment_id`, `dob`, `dod`, `sex`, CAST(`contact_date` AS DATE) - CAST(`dob` AS DATE) AS `age_at_contact`\r\nFROM (SELECT `LHS`.`client_id` AS `client_id`, `contact_id`, `referral_id`, `referral_date`, `contact_date`, `team_id`, `hcp_id`, `contact_type`, `assessment_id`, `dob`, `dod`, `sex`\r\nFROM `contacts_s2` AS `LHS`\r\nLEFT JOIN `demographics_s2` AS `RHS`\r\nON (`LHS`.`client_id` = `RHS`.`client_id`)\r\n))\r\n\r\nExample results\r\n\r\n\r\n# Look at results from SQL query shown above\r\ndb_age_at_contacts %>% \r\n  select(client_id, contact_date, dob, age_at_contact)\r\n\r\n\r\n# Source:   lazy query [?? x 4]\r\n# Database: sqlite 3.38.0 [:memory:]\r\n  client_id contact_date dob        age_at_contact\r\n      <dbl> <chr>        <chr>               <int>\r\n1         1 2018-05-19   1988-01-01             30\r\n2         1 2019-06-05   1988-01-01             31\r\n3         1 2020-07-08   1988-01-01             32\r\n4         2 2019-01-15   1965-01-01             54\r\n\r\nOf course this is a VERY simple example. This can get way more\r\ncomplex, think BIG and solve BIG problems. There are many other examples\r\nout there showing how to work with databases in RStudio. I added some\r\nlinks that I found useful at the end of this post.\r\nWatch out now\r\nSo what’s coming next and where can we take this? Is this perfect? I\r\ndon’t know exactly what’s coming next and this is definitely far from\r\nperfect. But it’s the best approach my colleagues and I could come up\r\nwith in the time that we spent working on this. Maybe I’ll improve this\r\none day, maybe someone else will? Until then let’s share ideas and work\r\ntogether to improve healthcare analytics in the NHS. Ohhh, some people\r\nare already working like this 👀 it’s time others join them. I hope\r\nthose who make decisions about the direction of healthcare analytics in\r\nthe NHS will start to understand the problems and opportunities and act\r\nsoon. If not now, when then? We need to move towards a more open and\r\nmodern way of healthcare analytics!\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nSummary\r\nHere’s a short summary in BOLD AND ALL CAPS:\r\nDOCUMENT everything, absolutely everything! Every\r\nfunction and every single change!\r\nAUTOMATE common analytical tasks! Write functions\r\nand packages!\r\nTEST everything! Expect mistakes, there will be\r\n🐛🐛🐛\r\nSHARE as much as we can!\r\nOf course this doesn’t always work. There will always be some messy\r\ndata, inconsistent variable names, undocumented code, and … blah blah\r\nblah.\r\nRelated work and resources\r\n📖 Chris\r\nMainey (2019). SQL Server Database connections in R.\r\n📖 Emily Riederer\r\n(2021). Workflows for querying databases via R - Tricks for modularizing\r\nand refactoring your projects SQL/R interface.\r\n📖 Hadley Wickham,\r\nMaximilian Girlich and Edgar Ruiz (2021). dbplyr: A ‘dplyr’ Back End for\r\nDatabases.\r\n📖 RStudio (2021). Databases using\r\nR from RStudio.\r\n📖 RStudio (2021).\r\nUsing an ODBC driver\r\n📷 Edgar\r\nRuiz (2018). Best practices for working with databases.\r\n🧙 Chris Beeley (2011 ’Til\r\nInfinity). Random bits of related and unrelated statistics, programming,\r\nand healthcare wizardry.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-08-06-nottshcverse/img/Principes-de-Logique-p435.jpg",
    "last_modified": "2022-04-01T21:09:38+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-14-text-mining-pipeline/",
    "title": "A Text Mining Pipeline for NHS Patient Experience Feedback",
    "description": "This blog post is a more technical description of the pipeline that we have built to analyse patient feedback text data from the NHS.",
    "author": [
      {
        "name": "Andreas Soteriades",
        "url": {}
      }
    ],
    "date": "2021-09-14",
    "categories": [
      "Text Mining",
      "Patient Experience"
    ],
    "contents": "\r\nThe pipeline consists of two distinct modules:\r\nA text classification pipeline for classifying patient feedback text into themes like Communication, Environment/facilities, Staff, etc.\r\nA text mining dashboard reporting results from text classification, sentiment analysis, and analysis of word frequencies to surface information about what patients most talk about, what frustrates them, what they most like in the service etc.\r\nFor the scope and more high-level descriptions of the different analyses carried out see this blog post and project description.\r\nIn terms of the nitty gritty, we are particularly excited about having combined super-cool packages in R and Python to build the pipeline! Two highlights of our work are the use of what we consider to be game-changer R packages:\r\n{golem}- “[…] an opinionated framework for building production-grade shiny applications”. Package {golem} automatically provides the structure for the {shiny} skeleton (app & ui) and makes it very easy to build Shiny apps that are modular, strict as to where the business logic goes, documented, tested, shareable, and agnostic to deployment.\r\n{reticulate}- an R interface to Python that opened up for us unique opportunities for using state-of-the-art Python packages for text classification and sentiment analysis directly in R.\r\nFor more details, refer to this presentation where I describe both packages in much enthusiasm!\r\nPipeline overview\r\nLet’s take a look at the whole pipeline:\r\n\r\nThe pipeline consists of an ecosystem of tailor-made packages in R ({experienceAnalysis}, {pxtextmineR}, {pxtextminingdashboard}) and Python (pxtextmining) that we designed to be both fit-for-purpose, but also as generic as possible for use by other NHS trusts or by anyone in general. Let’s break down the pipeline into smaller steps:\r\nThe pipeline reads the text data and sends it to the text classification pipeline, as well as to the dashboard.\r\nThe text classification pipeline uses Scikit-learn- fuelled pxtextmining to tune and train a Machine Learning model. It then writes the results (predictions, performance metrics, classifier performance bar plots, a SAV with the trained text classification pipeline etc.). These are then passed into dedicated modules in the {golem} dashboard that present predictions on unlabelled feedback, as well as tables and plots with performance metrics.\r\nMeanwhile, the text data is also passed into the dashboard for sentiment analysis and other text mining (e.g. TF-IDFs). The dashboard has dedicated modules that use our external packages to perform these analyses. In particular, {experienceAnalysis} makes extensive use of {tidytext}, although it offers functions that conveniently perform automatically a few data preprocessing and manipulation steps that would otherwise need to be done manually before passing them to the {tidytext} functions. On the other hand, {pxtextmineR} has {reticulate}- fuelled functions for doing sentiment analysis with Python packages TextBlob and vaderSentiment.\r\nInternally, the {golem} dashboard also has a series of R scripts containing simple utility functions that are useful for running small tasks (e.g.  sort a character vector) that would otherwise be run inside the modules themselves.\r\nNote that using external packages and utility functions to prepare the data keeps the modules clean from any business logic that would make the dashboard too specific to the dataset used. This is a key advantage of {golem}: we can use the dashboard as a framework for reporting results on any dataset that we would like to use! A simple example is the following: say we want to report averages for a number of categories. This could be mean sepal length for each plant species in the iris data, mean flipper length for each penguin species in the penguin data, and mean miles per gallon for each car engine type in the mtcars data. We can build a {golem} that produces the mean of a variable according to different categories and then pass either iris, penguin or mtcars to get a dashboard for each of these datasets.\r\nAnd this is exactly where the YAML file in {golem} comes in handy. The YAML file acts as a control panel where the user specifies what dataset and which columns from the dataset to use in the business logic. In our case, this means that we can produce a dashboard for our own data or for the patient feedback data of any NHS trust! As deployment with {golem} is pretty straightforward, we are able to host several dashboards on the server, each of which uses a dataset from a different NHS trust.\r\nAmazingly, {golem} ships the whole dashboard as an R package! We call our packaged dashboard pxtextminingdashboard. This package contains open patient experience data in RDA format that can be used to run the app. All you need to do is install pxtextminingdashboard, load it in R and run run_app. The dashboard is also available here.\r\nConclusion\r\nWe have built a truly revolutionary text mining pipeline that can be used for free by any NHS trust and will hopefully help surface business-critical information to guide improvements in healthcare services. We believe that the pipeline is a great example of how one can make the best of both R and Python. Both {golem} and {reticulate} are game-changers- try them out!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-09-14-text-mining-pipeline/img/a-treatise-on-map-projections-p88.jpg",
    "last_modified": "2022-04-01T20:16:11+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-30-pair-programming-code-review-journal-club-and-team-time/",
    "title": "Pair programming, code review, journal club, and team time",
    "description": "What can the CDU data science team do to verify its outputs, disseminate learning, and support individual development in team sessions?",
    "author": [
      {
        "name": "Chris Beeley",
        "url": {}
      }
    ],
    "date": "2021-07-30",
    "categories": [
      "Teamwork"
    ],
    "contents": "\r\nWe’ve been a proper data science team for a year now, there’s six\r\nof us at the moment which is GREAT and we’re starting to review some\r\nof the stuff that we’ve been doing to make sure that it’s serving a\r\npurpose. I’m sort of writing this for our benefit just as a way of\r\nrecording what the plan is (and to discuss the plan via pull request 😎)\r\nbut we work in the open so why not talk about this in the open too.\r\nFairly near the beginning of become A Proper Data Science Team we\r\nkicked off “Code review” sessions which would take place fortnightly and\r\nteam members would bring stuff, taking it in turns to do one each. I\r\nhardly think I can improve on Google’s\r\nguide to code review but I’ll summarise the main points here for\r\nthose who don’t click through. Code review is the process of one or more\r\npeople who have not written the code in question to read it and check\r\nthat it’s okay, before the code is merged into the codebase. Code\r\nreviewers are considering:\r\nDesign\r\nFunctionality\r\nComplexity\r\nTests\r\nGood naming practices (variables, functions)\r\nComments\r\nStyle (judged against a style guide like the tidyverse style guide)\r\nConsistency\r\nDocumentation\r\nIn the linked document Google emphasise that you should read every\r\nline- not scan over things and assume they’re okay. It’s worth\r\nremembering as well that we are doing data science- so we need to be\r\nreviewing statistical and ML methods as well. People (including me) fall\r\ninto the trap of thinking that data science is just programming and they\r\nstop asking themselves hard questions about the methods they are using\r\nand that is very dangerous.\r\nI put code review in quotes advisedly because although we went into\r\nthem thinking we would be doing code review over the weeks and months we\r\nended up doing something totally different. Sometimes we would pretty\r\nmuch do code review. Sometimes we would spend a long time discussing\r\ncode style. Other times we would start off doing code review and end up\r\ndoing an ad hoc lesson in a particular method that the person who came\r\nto code review. Sometimes we would have such a good time doing one of\r\nthese things that we couldn’t fit all the excitement into an hour and\r\nwould call extraordinary code review meetings so we could carry on\r\ndiscussing it and not have to wait two weeks.\r\nIt was pretty anarchic but we were all learning and having fun in a\r\nsupportive environment so I thought we may as well just see where we\r\nended up. After a year I thought it was time to review what we were\r\ndoing around assuring our code so I kicked off a discussion about it\r\nwhich this blog post is part of.\r\nWe have had a pretty wide ranging discussion about it and the first\r\npoint of interest is that everybody wanted to keep the fortnightly\r\nsessions but everybody agreed that, however awesome they were, they\r\nweren’t really “code review”. We boiled down what we feel we need to\r\nfour distinct activities. For the sake of brevity I will summarise them\r\nhere- I could probably do a blog post on each and if anybody would like\r\nto hear more then find me on Twitter, have a look at the about page.\r\nPair programming\r\nPair programming is what it sounds like- programming in pairs. It’s a\r\ngreat way of teaching and helping each other, and solving problems\r\ntogether, but it’s also a way of reviewing code too. The maxim we came\r\nup with today is simple.\r\n\r\nEvery line of code should have been read by two people\r\n\r\nYou can do that in a pair, live, or you can do it by review (or you\r\ncan do both). Interestingly enough the team just spontaneously started\r\ndoing pair programming. Nobody mentioned the word, nobody asked them to.\r\nIt just makes sense to do that so they just started doing it, which I\r\nabsolutely love.\r\nCode review\r\nThis is the other review methodology that came out of the discussion.\r\nI already talked about what Google think this should be so there’s not\r\nmuch to add. One thing we said is that the other thing we need to change\r\nis doing it fortnightly as a group. Proper code review can only be done\r\nby someone who understands the code. We have a LOT of skills in the team\r\n(SQL, R, Python, Shiny, statistics, ML) and nobody understands it all\r\n(especially me). We decided that this would happen when people feel\r\nthey’re ready and would take place with one or more designated people\r\nwho understand the code, and not every fortnight with everybody.\r\nOne interesting thing that came out of this discussion was my\r\nrealisation that we need to deepen and broaden the skills of the team.\r\nSeveral of us are writing code that nobody has the expertise to review.\r\nI’ve long been obsessed with truck factor but I\r\nhadn’t considered it from the position of validating code before. It’s\r\nentirely my failing as a manager and it’s something else to think about\r\nwhen we’re recruiting next (it’s also relevant for training and\r\ndevelopment, but that’s another post).\r\nJournal club\r\nSomething else that we spontaneously did as part of the code reviews\r\nthat weren’t really code reviews was what we’re loosely calling “journal\r\nclub”- basically one of “I know something that you need to know, I’m\r\ngoing to teach you” or “we all need to get better at x- let’s learn\r\ntogether”. We’re going to start with me: “Everything your data\r\nscientists wanted to know about managing servers and deploying in the\r\ncloud- but were afraid to ask” which will end up on GitHub\r\nsomewhere.\r\nTeam time\r\nNot sure about the name for this one! The last thing that everybody\r\nseemed to want was something else we were already doing. It’s basically\r\njust the ability to just bring anything you like and talk about it.\r\nWe’ve had all sorts of things. “This code is fine, it’s just horribly\r\nslow”, “I can’t figure out what is going to give the best experience for\r\nour users”, “I’ve done this analysis but it’s kind of shallow- what else\r\ncan I do?”. I don’t think this is anything particular, it doesn’t have a\r\nproper name, it isn’t particularly for one purpose, it’s more just the\r\nteam’s way of saying “We help and support each other and we carve out\r\none hour every two weeks so team members can bring a problem and talk it\r\nthrough with us”.\r\nWrap up\r\nThat’s where we’re at now, once we’ve agreed this as the way forward\r\n(or agreed something else, obviously) then we’ll give it a go. As is\r\nprobably clear, our Friday sessions and code review are just one part of\r\ntrying to have a team that works well together and I’m totally new at\r\nthis so if anybody out there has anything to add I’d be super grateful\r\nto hear it.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-07-30-pair-programming-code-review-journal-club-and-team-time/img/the-geographical-institutions-p84.jpg",
    "last_modified": "2022-04-01T21:10:46+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-22-age-bands-methodology/",
    "title": "Age bands methodology",
    "description": "A blog compiling all the age bands methodology that can be used for comparing analysis populations against.",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Lextuga007": {}
        }
      }
    ],
    "date": "2021-06-22",
    "categories": [
      "Resources"
    ],
    "contents": "\r\nAge Bands\r\nOn the face of it applying age bands to data analysis is simple, you need to consider the start and end ages and group up the rest into reasonable groups. However, if you want to use the data to compare to other sources of aggregate data it needs to be grouped in a similar way.\r\nFriends and Family questionnaires\r\nTrusts can control some of the data they collect for the Friends and Family questionnaires and Nottinghamshire Healthcare NHS Foundation Trust have used a grouped age question to anonymise responses from patients. Our feedback can be analysed through the shiny app with code here.\r\nSUCE (Service User and Carers Experience) survey:\r\nUnder 12 12-17 18-25 26-39 40-64 65-79 80+ years Refused Missing\r\nSUCE (Service User and Carers Experience) survey - under 12 years specific\r\nUnder 6 6 to 8 9 to 11 2 to 17 18+ years\r\nNational Workforce Dataset\r\nThe National Workforce Dataset (ESR values) use ages grouped like from the Age profile projection model which uses:\r\n<20 20-24 25-29 30-34 35-39 40-44 45-49 50-54 55-59 60-64 65-69 70+\r\nOffice of National Statistics (ONS) - population projections\r\nAlso see: https://cdu-data-science-team.github.io/team-blog/posts/2021-06-22-population-projections-websites/\r\nAge bands from population projections are:\r\n0-4 5-9 10-14 15-19 20-24 25-29 30-34 35-39 40-44 45-49 50-54 55-59 60-64 65-69 70-74 75-79 80-84 85-89 90 and over\r\nOffice of National Statistics (ONS) - survey best practice\r\nThere are various suggested age bands for surveys which are listed on this Wikimedia page.\r\nNHS Staff Survey\r\nInformation on the NHS Staff Survey has changed in how it was published but the latest website (as of June 2021) has the dashboard with age bands as:\r\n16-20 21-30 31-40 41-50 51-65 66+\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-22-age-bands-methodology/voyageurs-anciens-et-modernes.jpg",
    "last_modified": "2022-04-01T20:16:11+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-22-population-projections-websites/",
    "title": "Population Projections",
    "description": "Links for population projection data.",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Lextuga007": {}
        }
      }
    ],
    "date": "2021-06-22",
    "categories": [
      "Resources"
    ],
    "contents": "\r\nPopulation projections\r\nThe ONS run the census once every 10 years in England and are estimated between censuses. These are used for resource allocation and planning as well as providing denominator population groups for some analyses.\r\nOffice of National Statistics (ONS) Mid Year Estimates\r\nThe mid year estimates are available here.\r\nThe Analysis Tool ins an interactive tool that creates a population pyramid in excel.\r\nBy CCG\r\nMid-year (30 June) estimates of the usual resident population for clinical commissioning groups (CCGs) in England: link\r\nPopulation figures over a 25-year period, by five-year age groups and sex for clinical commissioning groups (CCGs) in England. 2018-based estimates are the latest principal projection: link\r\nSpecific Population Projections\r\nPOPPI and PANSI\r\nProjecting Older People Population Information and Projecting Adult Needs and Service Information\r\nThere are two specific websites designed to help explore the possible impact that demography and certain conditions on populations either aged 65 and over: POPPI or populations aged 18 to 64: PANSI.\r\n\r\nOriginally developed for the Department of Health, this system provides population data by age band, gender, ethnic group, and tenure, for English local authorities.\r\n\r\nThese require an account to access but only one account is required to access both site and is free to register.\r\nProjections are by Region and cover age groups but also gender, ethnic groups, health (including mental health) and learning disability.\r\nJournal articles\r\nLancet Public Health\r\nForecasting the care needs of the older population in England over the next 20 years estimates from the Population Ageing and Care Simulation (PACSim) modelling study. Aug 2018)\r\nModelling the growing need for social care in older people\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-22-population-projections-websites/img/allgemeine-erdbeschreibung.jpg",
    "last_modified": "2022-04-01T20:16:11+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-10-making-a-presentation-repo-github-template/",
    "title": "Making presentation slides into a GitHub repository template",
    "description": "One of a series of posts relating to creating presentation templates using {xaringan}, GitHub and R Studio.",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Lextuga007": {}
        }
      }
    ],
    "date": "2021-06-10",
    "categories": [
      "Open source",
      "GitHub",
      "Presentations"
    ],
    "contents": "\r\nCreating a template GitHub repository\r\nAfter creating {xaringan} presentations slides for the CDU Data Science Team using the branding from Nottinghamshire Healthcare NHS Foundation Trust, I wanted to share the files used as a template. There are quite a few used in {xaringan} slides because it relies upon CSS and images to give the ‘professional’ look similar to PowerPoint.\r\nOne of the new features in GitHub is to make your repository a template:\r\n\r\nAnd the nice thing about this, is when someone selects the template button:\r\n\r\nIt means they can fork the repository (get a copy) but all of the commit history is removed giving the next person - if they want - a clean repository to work from. People still have the option to clone the repository with the history by following the usual cloning:\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-10-making-a-presentation-repo-github-template/img/grundzüge-der-mathematischen-geographie-und-der-landkartenprojection.jpg",
    "last_modified": "2022-04-01T20:16:11+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-10-making-an-rstudio-presentation-template/",
    "title": "Making an RStudio presentation template",
    "description": "One of a series of posts relating to creating presentation templates using {xaringan}, GitHub and R Studio.",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Lextuga007": {}
        }
      }
    ],
    "date": "2021-06-10",
    "categories": [
      "Open source",
      "RStudio",
      "Presentations"
    ],
    "contents": "\r\nCreating a template through R Studio\r\nDid you know it’s possible to set up your own templates into RStudio? I didn’t until I started looking into the presentation templates for the CDU Data Science Team. The benefit of this is that I no longer have to set up clone repositories each time which is awkward too if you want to keep all presentations in one repository like we do for the team’s presentations.\r\nA few in the team suggested setting these up as a package but {xaringan} slides come with many supporting files (images and CSS), and it wasn’t too obvious how to do this. Luckily a number of people have done it and one such person tweeted about it so I added the details to the repository’s issues for reference.\r\nThis led me to @DrMowinckels’s package {uiothemes} which is particularly useful as she uses this package for various templates and themes and so, following the layout for adding the xaringan slides I added the following folders to our own package {nottshcMethods} following our process of:\r\ncreate a branch with the issue number like 1-slide-templates\r\nmake the changes\r\nset up a pull request to the development branch\r\nFile structure\r\nI added the file structure:\r\nnottshcMethods/inst/rmarkdown/templates/Nottshc/skeleton\r\nand in the Nottshc folder I added a file called template.yaml containing:\r\nname: Nottshc Presentation template\r\ndescription: >\r\n   Standard xaringan Nottshc template for presentations\r\ncreate_dir: TRUE\r\nThe create_dir is particularly important as this is asking if a new folder directory should be created or not. As this is set to TRUE all the files and folder structure in the folder skeleton will be copied. Note that in {uiothemes} all xaringan files are within one folder but I prefer my files to be in subfolders so {nottshcMethods} has the subfolders css and img.\r\nname is what will appear in the RStudio templates later so this needs to be clear and concise.\r\nIn the folder skeleton the important file is the skeleton.Rmd which is the template RMarkdown file for the slides.\r\nAs this is within a package, to run the package Ctrl+Shift+B will build the package on your computer. When that’s run the template will appear in File/New File/R Markdown…/From Template\r\n\r\nAddendum and a plea to blog things like this\r\nI wrote this out a number of weeks before publishing and I’m so glad I did as I immediately forgot everything I did. It was only when I needed to do a presentation and went to my templates in RStudio that I realised how great this is and how I couldn’t remember how I had set it all up. Luckily I had also started this blog in order to share with others and so the moral of the story is, when you write a blog you are sharing your current knowledge with others, but also your future self.\r\nBe kind to your future self, share your thoughts and your technical wins, even if they seem small to you today they may be huge tomorrow.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-10-making-an-rstudio-presentation-template/img/bibliothek-geographischer-handbücher-107.jpg",
    "last_modified": "2022-04-01T20:16:11+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-14-index-of-multiple-deprivation/",
    "title": "Index of Multiple Deprivation",
    "description": "The measure of relative deprivation in small areas in England called lower-layer super output areas",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Letxuga007": {}
        }
      }
    ],
    "date": "2021-05-14",
    "categories": [
      "Resources"
    ],
    "contents": "\r\nIndex of Multiple Deprivation (2019)\r\nIMD is very useful for categorising the area a person lives in for deprivation. This information/links and code only cover England and deprivation is scored in relation to all the areas within England using the IMD (2019). This means that they have been ordered by the deprivation score and then ranked (IMDRank).\r\nThe common decile used is between 1 and 10 with 10 being the least deprived.\r\nData warehouses\r\nThe data is often held in 3 tables:\r\nthe postcodes of the data held (for example patients when in the healthcare sector)\r\na lookup postcode table (like a directory of postcodes) from\r\nhttps://digital.nhs.uk/services/organisation-data-service/data-downloads/ods-postcode-files\r\nthe IMD data from\r\nOlder version: http://www.gov.uk/government/statistics/english-indices-of-deprivation-2015 https://www.gov.uk/government/statistics/english-indices-of-deprivation-2019\r\nSelect File 7 for the dataset.\r\nOther data is available from this dataset including IDAOPI which relates to only older people.\r\nNote that the column headers change, in 2015 it was LADistrictCode2013 and in 2019 it is LADistrictCode2019. Also LADistrictName2013 has become LADistrictName2019.\r\nWatch for…\r\n‘Unknown’ in the data warehouse\r\nData warehouse tables may include a row for ‘unknown’ (https://www.sqlchick.com/entries/2011/5/16/usage-of-unknown-member-rows-in-a-data-warehouse.html) and this may create a valid join across tables: unknown is entered as a postcode that links to the postcode table that in turn returns data from the IMD table. This may result in a value being returned from the IMD table that isn’t valid like 0 which looks like it should be an IMD decile score, for example, but which is not.\r\nPostcode spaces\r\nAlso, postcode lengths vary according to how many spaces there are between the two parts. In the UK the postcode format can be 3 parts and then 3 or 4 then 3:\r\nNG1 1AA NG26 1AA\r\nJoining datasets is always better when the space between the postcode parts is removed. In SQL this can be:\r\nREPLACE(postcode, ’ ‘,’’)\r\nin R it can be\r\n\r\n\r\npostcode <- \"NG16 1AA\"\r\ngsub(\" \",\"\",postcode)\r\n\r\n\r\n\r\nPartial postcodes\r\nPartial postcodes will not give a sufficiently reliable IMD score.\r\nExample join code (SQL)\r\nTo get the IMD score the LSOA (Lower Super Output Area) code is required which is taken from the full postcode.\r\nThis code will not run and is dependent on the naming conventions of the SQL server. The column names of LSOA11 and LSOAcode2011 will have come from the data sources. PostCode_space will have been added to the table by the data warehouse administrator(s).\r\n\r\nSELECT Top 100 imd.*\r\nFROM DIM_AI.PatientData AS p\r\nLEFT JOIN DIM_AI.PostCodes AS pc ON p.PostCode = pc.PostCode_space                                              \r\nLEFT JOIN DIM_AI.IMD AS i ON pc.PC.LSOA11 = i.LSOAcode2011\r\nWHERE p.PostCode LIKE 'NG%'\r\n\r\nCreating quintiles\r\nSome publicly available data is in quintiles for IMD to remove small identifiable numbers.\r\nTo replicate that in local data the equation: floor((IMDDecile-1)/2) + 1 can be applied.\r\nQuintiles in SQL\r\n\r\nSELECT DISTINCT IMDDecile,\r\nFLOOR((IMDDecile-1)/2) + 1 AS IMDQuintile\r\nFROM DIM_AI.IMD\r\nORDER BY IMDDecile\r\n\r\nQuintiles in R\r\nThe following example Pubicly available data will not run download if this Rmarkdown script is run as the eval has been set to FALSE. Either change this to TRUE, remove eval=FALSE altogether or copy the code to another R script to run.\r\n\r\n\r\n# IMD by Ethnicity by Region 2007-2013\r\n# Latest: https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/livebirths/adhocs/006134birthsbyethnicitysexregionandimdquintilebyfinancialyear2007to2013\r\ndownload.file(\"https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/livebirths/adhocs/006134birthsbyethnicitysexregionandimdquintilebyfinancialyear2007to2013\",\r\n              destfile = \"regionbirthsbyethnicityIMD20072013.xls\",\r\n              method = \"wininet\", #use \"curl\" for OS X / Linux, \"wininet\" for Windows\r\n              mode = \"wb\") #wb means \"write binary\"\r\n\r\n\r\n\r\nApplying the formula:\r\n\r\n\r\nlibrary(tidyverse)\r\n# Generate a dataset\r\ndf <- structure(list(IMDDecile = c(0L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, \r\n8L, 9L, 10L, NA)), row.names = c(NA, -12L), class = c(\"tbl_df\", \r\n\"tbl\", \"data.frame\"))\r\n# Make the 0 generated into NA\r\ndf_quintile <- df %>% \r\n  replace_na(list(IMDDecile = 0)) %>% \r\n  mutate(IMDQuintile = floor((IMDDecile-1)/2) + 1,\r\n         IMDQuintile = as.character(IMDQuintile)\r\n         )\r\n\r\n\r\n\r\nCreating local IMDs\r\nIn Nottingham/Nottinghamshire the differences between the LSOA areas is diminished when ranked against England as a whole, but when ranked locally, the variation is much more pronounced. Consequently, for the majority of our analysis the Trust approach is to use Nottinghamshire quintiles of deprivation / IMD. There will be some instances where national quintiles (or deciles) will be required – namely any external analysis – however, these will be the exception.\r\nFor note, when using local quintiles anyone with a non-local postcode will come back unmatched as well as those people with proxy postcodes used often to denote homeless status (ZZ…).\r\nIMD in SQL\r\nTo create the local rankings in SQL the data needs to be restricted to the appropriate area, for example when joining to the Postcodes and restricting and then a windows partition applied to the data ROW_NUMBER() OVER(ORDER BY IMDRank) to create a new ranking score and NTILE(10) OVER (ORDER BY IMDRank) to create new deciles.\r\nTo replicate this in dplyr the code would be:\r\n\r\n\r\n # Data for Nottingham and Nottinghamshire taken from Postcodes and IMD. The original files are very large to download. \r\n# Note that there are columns in the Postcode data sample that don't exist in the source file. \r\n# These have been added locally but may be of use to others, such as CountyName and LocalAuthority_Name. # These are specifically added as Nottingham Local Authority is a Unitary Authority and appears in a different column to Nottinghamshire County's District Councils \r\nload(\"data/sampleDataNottingham.RData\")\r\nlibrary(dplyr)\r\n# Note on the join the two column names are different so are listed in the by = and they need to be in the correct order so the column from imdNottingham appears first.\r\nlocalRanking <- imdNottingham %>% \r\n  inner_join(postcodeData %>% \r\n               select(LSOA11) %>% \r\n               group_by(LSOA11) %>% \r\n               slice(1), by = c(\"LSOAcode2011\" = \"LSOA11\")) %>% \r\n  mutate(Notts_rank = row_number(IMDRank),\r\n         Notts_decile = ntile(IMDRank, 10)) \r\nlocalRanking %>% \r\n  select(LSOAcode2011:IMDDecile) %>% \r\n  head(5) %>% \r\n  knitr::kable(format = \"html\")\r\n\r\n\r\n\r\nLSOAcode2011\r\n\r\n\r\nLSOAname2011\r\n\r\n\r\nLADistrictCode2019\r\n\r\n\r\nLADistrictName2019\r\n\r\n\r\nIMDScore\r\n\r\n\r\nIMDRank\r\n\r\n\r\nIMDDecile\r\n\r\n\r\nE01013812\r\n\r\n\r\nNottingham 018C\r\n\r\n\r\nE06000018\r\n\r\n\r\nNottingham\r\n\r\n\r\n58.744\r\n\r\n\r\n1042\r\n\r\n\r\n1\r\n\r\n\r\nE01013814\r\n\r\n\r\nNottingham 022B\r\n\r\n\r\nE06000018\r\n\r\n\r\nNottingham\r\n\r\n\r\n42.893\r\n\r\n\r\n3499\r\n\r\n\r\n2\r\n\r\n\r\nE01013810\r\n\r\n\r\nNottingham 018A\r\n\r\n\r\nE06000018\r\n\r\n\r\nNottingham\r\n\r\n\r\n52.690\r\n\r\n\r\n1767\r\n\r\n\r\n1\r\n\r\n\r\nE01013811\r\n\r\n\r\nNottingham 018B\r\n\r\n\r\nE06000018\r\n\r\n\r\nNottingham\r\n\r\n\r\n53.234\r\n\r\n\r\n1688\r\n\r\n\r\n1\r\n\r\n\r\nE01013815\r\n\r\n\r\nNottingham 022C\r\n\r\n\r\nE06000018\r\n\r\n\r\nNottingham\r\n\r\n\r\n41.721\r\n\r\n\r\n3805\r\n\r\n\r\n2\r\n\r\n\r\nOther useful links\r\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/464430/English_Index_of_Multiple_Deprivation_2015_-_Guidance.pdf\r\nhttps://fingertips.phe.org.uk/search/imd\r\nhttp://dclgapps.communities.gov.uk/imd/idmap.html\r\nTechnical report for 2019: https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/833951/IoD2019_Technical_Report.pdf\r\nReferencing IMD in a paper or research\r\nFrom a journal check to see how IMD is referenced in published papers, this was from a 2016 BMJ article that cites the Index in the references as:\r\nDepartment for Communities and Local Government. English indices of deprivation 2015. 2015. https://www.gov.uk/government/statistics/ english-indices-of-deprivation-2015\r\n(Taken from https://bmjopen.bmj.com/content/bmjopen/6/11/e012750.full.pdf)\r\nAnother paper from 2016 cites as:\r\nDepartment for Communities and Local Government. English Indices of Deprivation 2015. Available online: http://www.gov.uk/government/statistics/english-indices-of-deprivation-2015 (accessed on 27 April 2016).\r\n(taken from https://www.mdpi.com/1660-4601/13/8/750)\r\nLooking at the Government page that lists the full text the library assistant said: “I would reference it from the Ministry of Housing, Communities and Local Government which would be more up to date for 2019 and with online references you should always put the date you accessed it. So I would suggest amending to the following:”\r\nMinistry of Housing, Communities and Local Government. English Indices of Deprivation 2015. 2015. https://www.gov.uk/government/statistics/english-indices-of-deprivation-2015 (Accessed 4 June 2019)\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-14-index-of-multiple-deprivation/img/grundzüge-der-mathematischen-geographie-und-der-landkartenprojection-93.jpg",
    "last_modified": "2022-04-01T20:16:11+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-14-mapping/",
    "title": "Mapping",
    "description": "Mapping using public health tools",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Letxuga007": {}
        }
      }
    ],
    "date": "2021-05-14",
    "categories": [
      "Resources"
    ],
    "contents": "\r\nMapping\r\nPublic Health Tools\r\nAs Public Health is based within Local Authorities many of their boundaries are related to government rather than health boundaries. Smaller areas related to GPs will be included but data becomes patchy at Trust boundary level. For example, Nottinghamshire Healthcare NHS Foundation Trust covers Nottinghamshire, Nottingham and some areas outside of these boundaries.\r\nLocal Health\r\nPublic Health use the following to overlay data such as life expectancy over IMD scores. It is possible to upload data to this site but this has not been approved by IG.\r\nShape\r\nThis requires creating an account but is freely available to NHS staff.\r\nThis has Trust locations already in the account and data can be overlayed. Drive time and public transport within so many minutes is particularly useful.\r\nCentroid mapping\r\nSometimes you have a set of addresses but no way of mapping them. The Office for National Statistics’ Open Geography Portal provides the centroids for all UK postcodes.\r\nUsing R to get centroid information: https://www.trafforddatalab.io/recipes/gis/postcodes.html#\r\nOr weighted by LSOA area: https://geoportal.statistics.gov.uk/datasets/lower-layer-super-output-areas-december-2011-population-weighted-centroids\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-14-mapping/img/half-hour-library-of-travel-nature-and-science-for-young-readers.jpg",
    "last_modified": "2022-04-01T20:16:11+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-27-working-in-the-open/",
    "title": "Working in the open",
    "description": "What does it mean to work in the open? What is open source? What problems can we solve if we share more openly?",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Letxuga007": {}
        }
      }
    ],
    "date": "2021-02-27",
    "categories": [
      "Open source"
    ],
    "contents": "\r\nWhat is working out in the open?\r\n“Working in the open” isn’t a technical term; it’s just my simple way of explaining a concept without having to mention all the tools that are available to do this.\r\n\r\n\r\n\r\nAs you can see from @ChrisBeeley’s tweet, he refers to “writing stuff in the open and making it reusable”; we use slightly different language but it covers the same principles.\r\nBeing open in the Public Sector\r\nI’ve always found that analysts working in the NHS and Local Authorities are always happy to share their methodologies, approaches to work and even code and how we shared this was often dictated by who we know and the tools we have to hand, like Excel or Word. If a change was made to the original I’d never know about it and, vice versa, if I improved the code I wouldn’t have an easy way to share back what I’d done.\r\nWhilst analysts were happy to share code I still built up a code repository for myself and for many years I recycled my own code. I often refer back to projects where I know I’ve written a particular bit of code that is useful and I’d rarely wrote out complete chunks of code that ran independently of project data. Working openly changes how you approach code because sharing projects that don’t work too well without some changes isn’t all that useful to others.\r\nWorking with an intention to be open makes you aware of public scrutiny and so, inevitably, you may take a bit longer to make code tidy, write a few more explanatory comments and ensure that code does what you think it should. What’s nice about doing this is that although the openness of work is intended for someone else’s benefit, often that person is still you.\r\nRepeatedly solving the same problem\r\nMany of the tasks that analysts and data scientists in the public sector are tasked with are the same. National Returns and benchmarking submissions are common and are completed by many trusts using slightly different approaches but, ultimately, leading to the same data output. This results in a constant cycle of problem solving where the solution, if not shared publicly, means others have to do the very same discovery work.\r\nIn a completely different context, it would be like finding a chemical compound, not sharing that knowledge and other people working hard in other labs to repeat the discovery. By constantly working in this ‘discovery’ phase we never further our collective knowledge by refining the techniques, analysing the results and, hopefully, using the “compound” to make a difference.\r\nPublishing code means that anyone in my team, my trust, the NHS, even the world can see what I have already discovered. In sharing to the world audience, I know I have something I myself can use. Of course, it takes time to write these things out but once it’s written it never needs to be rediscovered again - but it can be improved upon.\r\nHow do the CDU data science work in the open?\r\nThe CDU data science team have a strong desire to work openly and we have created a GitHub account to share code and knowledge like the pages on IMD1 and mapping.\r\nWe are also involved with the NHS-R Community, facilitating training, presenting webinars and talks as well as hosting the annual Hacktoberfest which was virtual last year. We had originally set up the Hacktoberfest to just be our team, setting aside one day to contribute to projects and practice using GitHub. Pretty quickly after agreeing this would be a good idea we extended this to the NHS-R Community as we felt that there really wasn’t any need for it to be restricted to just our team. We had a few people come in and out of the MS Teams meeting through the day and, like many things in the NHS-R Community, it was very supportive and informal.\r\nBuiding up skills\r\nWorking on other people’s projects in a Hackathon may seem, on the face of it, “non-essential” work, but it’s invaluable as it not only opens up connections with others who can help with your projects, but you invariably see useful code you can then use. Reviewing code is one thing, but to really understand a piece of code, debug or solve a problem you often have to break it apart and build it back up. In doing so you learn how it is constructed programmatically and how the other person/people have approached a problem. Doing this with others’ scripts has made me a better coder and none of the effort has been wasted.\r\nA recent example of valuable “non-essential” work for me was helping someone in the NHS-R Community Slack group who had an issue with their RMarkdown and getting a plotly chart to appear in the eventual html output, although it would appear when each chunk was run. I took the code and moved each chunk into a template RMarkdown to see if it ran, section by section. In doing so I located the problem but I also saw a new bit of code2\r\n\r\n\r\ncode_folding: \"hide\"\r\n\r\n\r\n\r\nwhich I’d never seen that before. Now I could have equally have learned about this from reading about RMarkdown but it will forever stick in my memory as it was in the context of solving a problem. We helped each other and now I’m sharing that learning in this blog - working out in the open.\r\nThe full YAML for reference\r\n---\r\ntitle: \"Test\"\r\ndate: \"25/02/2021\"\r\noutput: \r\n  html_document:\r\n    code_folding: \"hide\"\r\n    toc: true\r\n    toc_float: true\r\n    toc_collapsed: false\r\n---\r\nIf you want to read more about how our journey is going with working out in the open keep in touch by following us on Twitter and in these blogs.\r\n\r\nIndices of Multiple Deprivation↩︎\r\nThe problem was results=‘hide’ being in the knitr::opts_chunk$set() code which affects the output. I’m still learning how these codes work so didn’t spot that at first, so I learned more about RMarkdown by debugging.↩︎\r\n",
    "preview": "posts/2021-02-27-working-in-the-open/img/letters-from-high-latitudes.jpg",
    "last_modified": "2022-04-01T20:16:11+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-20-a-new-github-release-and-future-projects/",
    "title": "A new GitHub release and future projects",
    "description": "We have a new project out and would like to tell you about some more of our future work.",
    "author": [
      {
        "name": "Chris Beeley",
        "url": {}
      }
    ],
    "date": "2021-01-20",
    "categories": [
      "Open source"
    ],
    "contents": "\r\nIf you’ve read the about section of this blog then you’ll know that our team believes in (and practises!) open source data science. We strive to put as much code and (sometimes synthetic) data out as possible, with an open source licence (MIT, usually), and where we can we try to make our code reasonably easy to re-use (although this is not always simple). We have just pushed out a prototype version of an application and this seemed like a timely moment to talk about the application, and what else we have coming up on the open source side of things. It’s worth saying that some of our team members work really, really hard doing lots of stuff that is very difficult to share so although you might not see as much of them on the GitHub they’re doing sterling work for the Trust and the team is dependent on their expertise for all of our work, whether it’s open source or not.\r\nText mining application\r\nWe already have a blog post about this work and we have come to the point where we have produced a release version (0.1.0) for the dashboard which summarises the acccuracy of the models and helps to show the kinds of decisions that it’s making. Please read the blog post for details of this work but our ambition in brief is to produce a text mining algorithm for patient experience that can be used in any NHS organisation in the country. The actual algorithm work (which is in Python mainly) has not yet stabilised to a release version just yet but is available on GitHub.\r\nWe will be shipping another dashboard that helps trusts to visualise their feedback as part of the project. We’re currently working on that but we’re not quite ready to share it yet, keep an eye on our Twitter and blog for more details. We have a lot more to come in the way of working with staff and patient experience data, too, it’s not just this work, so please feel free to follow along with the code once it’s all open and maybe even send us a pull request 😉.\r\nForecasting of patient numbers\r\nWe’re also involved in a Health Foundation funded project which looks at predicting numbers of certain types of patients in the hospital. It takes the form of a dashboard which can predict the numbers of patients likely to fall into particular categories in the next 1-10 days based on previous data of this kind. The model has complex seasonality (although currently it achieves better results if you constrain it to results since April because of COVID) and a TBATS model produced the best results. The code is MIT open source and could be easily adapted to predict lots of different univariate series. There are lots of other people involved in this project and written materials from them are forthcoming, I will add them to this blog post once they are available. Our role was just to help with the forecasting and write the dashboard, lots of other work has gone into it.\r\nForecasting pharmacy dispensing\r\nThis is another Health Foundation funded forecasting project which attempts to predict the amount of many different medications which will be dispensed from a pharmacy in order to better manage stock levels. Again the code for our bit is open source MIT, although it is very early days for this project so there will be much more to come. There is much more to this project than just the code, and I will update this blog post with more details once more of the outputs are ready.\r\nA Shiny interface to EndomineR\r\nThis work relates to another Health Foundation funded project but it was funded by NHS-R. EndomineR is a clinical text mining system which works with endoscopic reports and helps to collate and analyse data from free text reports automatically. This work replicates an existing Shiny interface but uses the {golem} package to rewrite the code within modules and to make the application run as an R package. This will make the code easier to maintain, update, and generalise to other clinical settings. This project is currently in active development but it should be ready for a first release in February some time, the code again available open source on GitHub.\r\n{golem}, gitflow, and production data science\r\nWe are all still learning but we are trying to use good methods to make sure that our code is robust and easy to deploy, and to help us collaborate with each other. To this end we:\r\nUse the {golem} package for a lot of our Shiny work, and modularise our Shiny code\r\nUse RStudio Connect (I have written some stuff about this on my own blog)\r\nUse gitflow\r\nHave regular (two weekly) code review sessions\r\nI’m really interested in understanding how to get better at working together in the open and tools to help code easy to deploy and generalise. If you’re interested in that too, especially if you work in the NHS (some of the hurdles are the same size and height everywhere in the NHS 😆) then please get in touch.\r\nWe have lots of stuff planned including better analysis of HoNOS data, more to come on staff and patient experience, methods for summarising clinical outcomes and health inequalities, and other stuff from team members where I’m not quite sure how close they are to launch. Please watch out for regular updates if you would like to see more stuff from us, on our Twitter and on this blog.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-20-a-new-github-release-and-future-projects/img/admiralty-manual-for-the-deviations-of-the-compass.jpg",
    "last_modified": "2022-04-01T20:16:11+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-13-data-clinics-in-nottinghamshire-healthcare/",
    "title": "Data clinics in Nottinghamshire Healthcare",
    "description": "We have been working with teams to help them with their data problems. This post describes some of the clinics and what has come about as a result of this work.",
    "author": [
      {
        "name": "Lori Edwards Suárez",
        "url": {
          "https://twitter.com/Lori_E_S_": {}
        }
      }
    ],
    "date": "2021-01-13",
    "categories": [
      "Data clinics"
    ],
    "contents": "\r\nOur team recently piloted data clinics within the Trust in order to:\r\nImprove data quality and completeness\r\nImprove the means by which staff collect and record the essential information, making it more efficient and freeing them up to spend more time with their patients.\r\nThis was achieved by going “back to basics” with the people who collect and input data, the key principles are ensuring they know why they are collecting the data, making sure the data collection system works well for the teams and that the data can be used by both the data collectors and analysts. A variety of avenues were considered, such as reducing excess data collection and reducing duplication which make data gathering more laborious and tedious for clinicians.\r\nWhen data collection is difficult for clinicians it often results in the data not being filled properly, correcting this increases the accuracy and completeness of the dataset. Structuring clinical records and decreasing their reliaance on free text input is also beneficial for data analysis but is also often faster and easier for clinical staff. The clinics are a collaborative venture with the clinical team and others such as analysts and system admin, the type of staff varies depending on the needs of each teams. My role was to facilitate the conversations using skills learnt from working closely with the clinical teams to learn to “translate” between clinical language and data/IT language. Subtle differences in expression between the two groups often lead to misunderstandings which could stifle progress (more examples). The key element was that the problem was generated by the team themselves. This ensures that the clinic is focused on solving their difficulties which should help them to improve their own systems rather than forcing a change on them.\r\nAs a test run we had two teams go through the process:\r\nA forensic mental health team which wanted to move away from using Excel to collect their data\r\nA community mental health team which wanted to collect some extra information to better understand the impact of their team without adding too much to their workload\r\nThe forensic team was a new service which had a lot of data requested of them and they wished to improve their data collection and assess its quality. The Team Leader had used team-specific forms in RiO (the clinical database which they use) previously and was interested in seeing if it was possible here. However, they were having a tough time explaining to the managers who were not familiar with such a system how to approve it and get it built into the system. The spreadsheet was found to have a lot of duplication and data being requested that was not necessarily attainable by the team. We looked together at what the purpose was and changed some of the data from free text to a more structured pick list from the valid values for that piece of data. We also had to explain to the managers that this change was not going to affect their reporting adversely. The patient record system was able to provide the team with what they needed and to automate some aspects to reduce workload for clinicians. Some outcomes measures already existed but others were not yet available on the system, an Excel sheet was made to collect them (an improvement over a folder in the corner of the room) with a reduction in demand for clinicians with simple automation of score summations. The team are thrilled that they can collect the data necessary for reporting and understanding their service in a more intuitive way, project managers are content they are getting the same information and more data is readily available for service improvement. Reports are being built which give the clinicians easy access to data which allows them to engage better and feel ownership of the data.\r\nThe community mental health team wanted to collect some more information to improve their ability to understand their outcomes. They needed to be able to distinguish between the cohorts of patients that were being referred. This ended up having a simple solution that had not been known to the clinical team – adding in more specific referral reasons. The patient cohort was clear and defined and could be determined at referral. They wanted some more information on one of the cohorts to understand the group further and to see how specific patients within the group progressed. To gather this data, a short form on the electronic patient record was created which takes one minute to fill in but adds a wealth of information. The team also got to play with the form before it went live to gain familiarity and to help them feel ownership of it. They also wanted to be able to predict when referrals may come in. As we got to know the pathways that brought patients into the service, we learned that we had information about patientes in the previous stage of the pathway. So, we managed to collect some information to understand the time between the previous stage and the referral. This means we can see when there is an uptick in people passing through the previous stage and predict a spike in referrals for the team to prepare for.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-13-data-clinics-in-nottinghamshire-healthcare/img/grundzüge-der-mathematischen-geographie-und-der-landkartenprojection-106.jpg",
    "last_modified": "2022-04-01T20:16:10+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-14-classification-of-patient-feedback/",
    "title": "Classification of patient feedback",
    "description": "An NHSE funded project to devise an application to automatically tag the content of patient feedback",
    "author": [
      {
        "name": "Andreas Soteriades",
        "url": {}
      }
    ],
    "date": "2020-11-14",
    "categories": [
      "Patient feedback"
    ],
    "contents": "\r\nConsider the following problem. A NHS trust is devoted to improving their services to provide patients with the best possible experience. The trust sets up a Patient Feedback system in order to identify key areas of improvement. Specialized trust staff (the “coders”) read the feedback and decide what it is about. For example, if the feedback is “The doctor carefully listened to me and clearly explained me the possible solutions”, then the coders can safely conclude that the feedback is about communication.\r\nBut what happens when thousands and thousands of patient feedback records are populating the trust’s database every few days? Can the coders keep up with tagging such a high volume of records? After all, unless they read all of it, they cannot tag it!\r\nWe need to find a clever way to get some weight off the coders’ shoulders!\r\nHere in Nottinghamshire Healthcare NHS Foundation Trust, we (the Data Science team) have opted for a Machine Learning approach to help coders tag the incoming patient feedback. In particular, we are developing Text Classification algorithms that “read” the feedback and decide what it is about.\r\nFirst things first: what are Machine Learning and Text Classification?\r\nMachine Learning is a wider concept, but here we will talk about the so-called supervised Machine Learning. Say a child is playing with a hole cube:\r\nPhoto of a child’s sorting toy with the sorting shapes stacked to the side of the sorting boxBy trying to pass different shapes through different holes, the child follows a process of “training” or “learning”, through which they learn to identify the right shape for each hole. Once they have been “trained”, they can easily predict what shape is the right one for a specific hole, on this or any other hole cube.\r\nThe process that the child has just followed is very similar to Machine Learning: see the child as an algorithm, the shapes as a dataset, and the holes as tags and you have a supervised Machine Learning problem. In other words, in supervised Machine Learning the algorithm “learns from” or “is trained on” the dataset, and thus becomes able to predict what tag corresponds to each record.\r\nSo when we have patient feedback data that have already been tagged by our coders, we can train an algorithm to assign the most appropriate tag to each feedback record, based on the content of the feedback text. As fresh, untagged feedback populates the trust’s database, the algorithm is then able to predict the most appropriate tag for it. In other words, the algorithm learns to automatically classify the text according its content, which is what Text Classification is about: a form of supervised Machine Learning that is about predicting the appropriate tag for the given text.\r\nHow can Text Classification improve NHS services?\r\nIncrease tagging speed. As mentioned earlier, the idea is to have the algorithm automatically tag feedback that the coders simply do not have time to read and tag themselves. To begin with, it will make the process of tagging much more efficient.\r\nNarrow down searches for NHS staff. If a member of staff (e.g. manager, doctor, nurse) wishes to focus on improve patient experience that has to do with, e.g. communication, they will want to read some or all of the incoming feedback about it. The algorithm will crunch the incoming feedback, decide which records are about communication, and feed them back to the member of staff.\r\nAre there any cons?\r\nAlgorithms make errors. For example, an algorithm may incorrectly classify feedback about smoking as being about communication. This is to be expected as no algorithm can ever be 100% accurate. What is key then is to make the algorithm as accurate as possible for the task at hand. This is an area where we focus on on a daily basis.\r\nDespite some inaccuracies, Machine Learning will still offer the great advantage of narrowing down NHS staff searches almost exclusively to the feedback of interest. If a manager has 100 feedback records of which only 20 are potentially relevant, and the algorithm predicts that 30 are potentially relevant (because it will make a few mistakes), this would still be a 70% reduction in the number of feedback records to be read by the manager!\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-14-classification-of-patient-feedback/img/grundzüge-der-mathematischen-geographie-und-der-landkartenprojection-96.jpg",
    "last_modified": "2022-04-01T20:16:10+01:00",
    "input_file": {}
  }
]
