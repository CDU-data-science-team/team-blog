[
  {
    "path": "posts/2023-09-01-UX-data-science/",
    "title": "UX and data science",
    "description": "Trialing UX techniques to evaluate data science products",
    "author": [
      {
        "name": "YiWen Hon",
        "url": {
          "https://www.linkedin.com/in/yiwen-h/": {}
        }
      }
    ],
    "date": "2023-09-01",
    "categories": [
      "Articles",
      "Patient Experience",
      "UX"
    ],
    "contents": "\r\nAs we approach the last stages of the Patient Experience Qualitative Data Categorisation project phase 2, we are making our final improvements to our two data science outputs, experiencesdashboard and pxtextmining. Part of this involves thinking about user experience (UX) and how we can improve this for our end users.\r\nWhat is UX?\r\nUX is a growing field with many definitions; Wikipedia describes it as being “how a user interacts with and experiences a product, system or service.” The products that we design as data scientists always have an end user, and it is important that a successful project ensures that these end user needs are met, so that customers are satisfied and actually continue to use the products.\r\nIt’s important to note that user experience is not the same as user requirements or project aims, and that often what users think they want might not be the same as what they actually need for a successful experience. The quote (dubiously) attributed to Henry Ford, “If I had asked people what they wanted, they would have said faster horses,” exemplifies this.\r\nFurthermore, UX is separate from accessibility. Ensuring that products are able to be used by everyone, regardless of their disabilities, should be a given. Accessibility guidelines for digital products produced by UK public sector bodies are available, and will not be further discussed here.\r\nPeople with UX expertise are few and far between in the NHS. Without being able to call on their assistance, it’s up to us to do our best in this area with the resources available to us.\r\nUX techniques for data science\r\nSomething that’s become clear in my brief sojourn into the scary world of UX is that there is no one size fits all approach - it all depends on your product and who it is for. Just as the data science products that we build come in all different shapes and sizes, and are for all different types of audiences, the UX methods used to evaluate and improve them should also be different.\r\nWe may try the following approaches initially:\r\nThe jobs to be done approach: Breaks down user needs into high-level goals customers want to accomplish using your product. We could then assess how well the product is able to meet each of the tasks. It’s also helpful to think about the bigger picture behind the product - what is the major underlying customer need? The focus should be more on what users are trying to accomplish, rather than how.\r\nClick testing / heatmaps: This could be as simple as using logs to identify where users click the most, or which areas of the product are used the most/least. We could also do things like track load times for completing specific tasks.\r\nUser observation: Sitting with users and observing them as they interact with the product, whilst bearing in mind the ‘jobs to be done’ and ‘heatmap’ concepts above, could provide useful insights for improving product usability.\r\nHow can data scientists get involved with UX?\r\nUX is a field that has seen a lot of growth in the past decade or so, and as such there is an overwhelming amount of information online. It can be hard to know where to begin, or what the ‘right’ method is.\r\nInitially, I made use of my network, asking around on NHS-R Slack, Gov Data Science Slack, and LinkedIn. Natasha S. den Dekker was kind enough to provide some words of advice; her thoughts have heavily influenced this blogpost. You may have someone in your network who could provide some guidance relating to your specific project.\r\nAs mentioned previously, there is not one perfect and foolproof UX methodology that’s going to solve all our problems and work for all our projects. We’re complete beginners in this field, without any professional experience. But it’s still better to start somewhere and make mistakes than not to do anything at all. We’ll try some of the methods above and evaluate how well they work, and learn from our experiences for the next project. After all, practice makes perfect, and UX will be a valuable skill to have in the data scientist’s toolbox.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-09-01-UX-data-science/img/bl_014813921.jpg",
    "last_modified": "2023-09-01T10:34:11+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-01-09-patient-feedback-project-update/",
    "title": "Patient feedback text mining project update",
    "description": "The second phase of the pxtextmining and experiencesdashboard projects has begun! What have we achieved so far, and what are our plans?",
    "author": [
      {
        "name": "YiWen Hon",
        "url": {
          "https://github.com/yiwen-h/": {}
        }
      }
    ],
    "date": "2023-01-09",
    "categories": [
      "Packages",
      "Open Source"
    ],
    "contents": "\r\nThe second phase of the pxtextmining / experiencesdashboard project has just begun, after a year-long hiatus. This NHS England funded project aims to enable NHS Trusts to make better use of their qualitative NHS Friends and Family Test data, through the creation of an open source product featuring text classification and data visualisation. This blog post will outline what we’ve been up to so far and our next steps.\r\nFirst steps\r\nPicking up someone else’s code isn’t always easy, and it’s taken time for Oluwasegun (working on the R/Shiny-based experiencesdashboard) and myself (working on the Python-based pxtextmining element) to feel confident in making changes to the packages. Much of my early work on pxtextmining has involved package management, such as switching to poetry for dependency management and mkdocs for documentation. These changes should improve the usability and accessibility of the package - for example, the number of dependencies has been reduced from 51 to 18, and we are now using pyproject.toml instead of setup.py/requirements.txt, in compliance with PEP 518.\r\nThe second phase of the project also involves a brand new dataset, which is currently being labelled by qualitative researchers from NHS England’s Insight and Feedback Team. This dataset utilises a new framework of labels which is derived from the data; there are more categories/targets than in phase 1, so we already know this will be an interesting challenge. The dataset should also be richer as we have new partner organisations contributing - whilst in phase 1 the data was derived from community and acute trusts, we also have an ambulance trust this time round. Work on this is still underway and the process of manually labelling the data is quite labour-intensive, but we are hoping to have some initial data to work with in the next few weeks.\r\nWhat happens next?\r\nSome features we’re looking to implement in pxtextmining, when we have the new data, are the implementation of active learning / human in the loop approaches to the machine learning pipeline, and the addition of multilabel classification functionality.\r\nOluwasegun, will be working on enabling users of experiencesdashboard to edit the data that they have uploaded, and improving the general usability and functionality of the frontend.\r\nWe will also improve the way the two packages work alongside each other by implementing an API in pxtextmining which can be called by the Shiny frontend, keeping the two packages separate but interconnected. At the moment, experiencesdashboard installs an R port of the pxtextmining package, so there is some duplication.\r\nCommunication around this project will also be improved, as we will be setting up a new documentation site that discusses the project as a whole for a non-technical audience. This project site will link out to the existing documentation pages for each project, which are more technically oriented.\r\nWe’ve got lots to do over the next few months! Any comments, questions or suggestions are very welcome.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-01-09-patient-feedback-project-update/img/014880349.jpg",
    "last_modified": "2023-09-01T10:10:40+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-09-when-things-go-wrong-in-github/",
    "title": "When things go wrong in GitHub",
    "description": "Avoiding mistakes and what to do when accidents happen in GitHub",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/letxuga007": {},
          "https://fosstodon.org/web/@Letxuga007": {}
        }
      }
    ],
    "date": "2022-11-09",
    "categories": [
      "Open source",
      "GitHub",
      "Security"
    ],
    "contents": "\r\nConcerns around GitHub\r\nAny NHS organisation should be rightly concerned about how securely data is managed by analysts and data scientists, however, the worries often overlook that the dangers for Git/GitHub are the same as for using email, Microsoft Office documents like Excel and even Social Media which a lot of Trusts allow access to as part of people’s work. As an example, sending out emails to a group of patients should always be in the blind copy, never in the To or CC but it has been known to occur. Having asked our own IT how we could avoid this particular breach the response was that there wasn’t a technological solution and we’d still have to rely upon people not revealing patients’ emails by accident.\r\nWhilst the focus of concerns is around the very public nature of GitHub, the practices to mitigate against breaches should apply to all code, regardless of the use of these versioning tools as analysts/data scientists share their code with others using other methods, like email.\r\nA good rule to avoid potential breaches is to never ever have hard coded sensitive information in your code. Sensitive information includes:\r\nconnection strings to SQL servers\r\nPersonal Access Token codes\r\npatient information, even pseudonymised information, in code (for example using code like filter(patientid == '12345') or in SQL WHERE patientid = '12345')\r\nStandard Operating Procedures\r\nRead more detail on our Standard Operating Procedure on using Git/GitHub which has links to technical explanations and all posts can be found with the tags Open Source and GitHub.\r\nWhen things go wrong\r\nBreaches can occur and if there are measures in place to ensure code is checked, hopefully, any breach will be spotted and can be removed. This works very well if you never commit to the main branch and pull requests are checked by one person. If something has been committed by accident, that second pair of eyes looking over the code should help to pick it up and the branch can be deleted from GitHub.\r\nRemoving a file with sensitive data on main takes a bit more work than just deleting a branch as changes will have also been made in the GitHub commit history. Changing history means that collaborators will be affected as their repository will no longer match. Also, the solution offered by GitHub using the BFG Repo-Cleaner requires software installation so might need to be part of any IG/IT assurance of risk mitigation plans.\r\nIf this isn’t available to you it is still possible to purge a file and its commit history but as there are many different scenarios, whether something has been committed, pushed, being merged and so on, and a really comprehensive site on how to manage undoing, fixing or removing commits depending on the scenario is a good reference but no longer appears to be updated. This original reference has been extended though into a TiddlyWiki and continues to be maintained.\r\nDisaster planning\r\nLastly, something that the team hasn’t done yet but we should consider strongly is practicing a disaster recovery plan. Whilst writing down what can and should be done in an incident is helpful to some extent, words cannot reduce the strong emotions and panic that naturally occur during such events and which in turn hinder logical thinking. Taking time to practice dealing with mistakes we can both refine the plans and also build up our resilience if the situation were ever to occur.\r\nOther suggestions\r\ngitea self hosted Git service\r\nHaving just moved to Fosstodon (part of the Mastadon social network) I asked if anyone had any other resources to fix GitHub sensitive data mistakes and a suggestion was given to use a local gitea repo and then when ready commit to GitHub with a pull request. This means files can be double checked and also can reduce the commit history, the example being:\r\n\r\nOriginal Repo: ~5000+ Pull Requests/Commits\r\nCopied Repo: ~5 Pull Requests\r\n\r\nGovernment Analysis Function\r\nAnother link shared was for handling data breaches in the Quality Assurance Code for Analysis and Research. Steps not mentioned in this blog is to move an affected public repository to private if something has been committed as well as changing any passwords/keys that may have been revealed.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-09-when-things-go-wrong-in-github/img/grundzüge-der-mathematischen-Geographie-p78.jpg",
    "last_modified": "2023-09-01T10:10:40+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-21-data-scientist-band-7-job/",
    "title": "Data Scientist job",
    "description": "An accessible version of the Data Scientist band 7 job in the team",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {}
      }
    ],
    "date": "2022-07-21",
    "categories": [
      "Jobs"
    ],
    "contents": "\r\nBackground\r\nDue to formatting issues this is an accessible (we hope but please\r\nsay if not!) version of the band 7 role in the data science team. It’s\r\nalso available for future reference after the post advert closes.\r\nApply here\r\nThe Nottinghamshire Healthcare NHS Foundation Trust TRACS\r\nsystem and NHS\r\nJobs\r\nJob Advert\r\nMain area: Data Science\r\nGrade: Band 7\r\nContract: Fixed term: 12 months\r\nHours: Full time - 37.5 hours per week\r\nJob ref: 186-1252-22-CS\r\nSite: Duncan Macmillan House\r\nTown: Nottingham\r\nSalary: £40,057 - £45,839 per annum (pro rata for part time)\r\nSalary period: Yearly\r\nClosing: 14/08/2022 23:59\r\nInterview date: 06/09/2022\r\nJob overview\r\nWe’re building the future of text analytics for patient experience\r\ndata at Nottinghamshire Healthcare NHS Trust, and open sourcing it so\r\neveryone can use it. This post supports a team working on an NHSE funded\r\nproject to build and deploy text analytics for patient experience data\r\nfor several pilot trusts, with a view to recruiting more as the work\r\nmatures.\r\nWe need your skills in Python, text mining, machine learning, deep\r\nlearning, and everything else that’s needed to make reliable, accurate,\r\nperformant models that can read and theme text data. We use R and Shiny\r\non the frontend and Python and MySQL in the back, and you can see the\r\ncode here\r\nhttps://github.com/CDU-data-science-team/pxtextmining https://github.com/CDU-data-science-team/experiencesdashboard\r\nThe team are fully hybrid and this post can be offered on a remote\r\nbasis.\r\nMain duties of the job\r\nAssisting with the deployment of the text mining system in the cloud\r\nfor access by partner organisations Development of text mining\r\nalgorithms\r\nCarrying out, feeding back, and acting on the results of user\r\ntesting\r\nAnalysing and suggesting ways to improve the algorithms and\r\ndashboard implementation\r\nHelping team members and the wider Trust and partner organisations\r\nto develop skills and understanding in the development, deployment, use,\r\nand interpretation of text mining algorithms in a patient experience\r\ncontext\r\nAssist with the development and testing of user documentation for\r\nthe Shiny application\r\nDetailed job\r\ndescription and main responsibilities\r\nWe are a small and friendly team of data scientists with expertise in\r\nRAP/ data pipelines, forecasting, statistics, and Shiny. Our team\r\nbelieves in using open source data science to make the world better and\r\nyou will see on our GitHub that we share as much of our work as we\r\ncan.\r\nTwo of the team are senior fellows in NHS-R and if you join this team\r\nyou will be part of a large and thriving national community of data\r\nscientists using R and Python in healthcare.\r\nThe job will involve writing and deploying text mining and machine\r\nlearning algorithms as part of a team building models for use anywhere\r\nNHS organisations collect patient experience data.\r\nFor more details please refer to the job description\r\nPerson specification\r\nPhysical requirements\r\nEssential criteria\r\nAble to sit for long periods of time at a desk in front of a computer\r\nworkstation and has advanced keyboard skills.\r\nQualifications\r\nEssential criteria\r\nEducated to degree level or equivalent experience\r\nContinuing Professional Development\r\nDesirable criteria\r\nPost graduate qualification or equivalent relevant experience\r\nHow identified\r\nApplication\r\nExperience\r\nEssential criteria\r\nExperience of writing and maintaining code, especially R or Python,\r\nfor data analysis\r\nExperience of statistical analysis or machine learning\r\nExperience of preparing reports on data\r\nDesirable criteria\r\nExperience designing and programming dashboards (e.g. using Shiny or\r\nDash)\r\nExperience with reproducible reporting\r\nExperience preparing public health and epidemiological focused analyses\r\nand reports\r\nHow identified\r\nApplication/Interview/Test\r\nKnowledge\r\nEssential criteria\r\nAdvanced knowledge of statistics or machine learning\r\nAbility to extract information from databases/datasets to help\r\nunderstand a process or solve a problem and make informed\r\nrecommendations based on detailed data insight\r\nDesirable criteria\r\nKnowledge of Linux terminal and server maintenance\r\nKnowledge of public health and epidemiological approaches\r\nHow identified\r\nApplication/Interview\r\nSkills\r\nEssential criteria\r\nExcellent numerical/statistical/ skills\r\nExcellent skills with R/Python or similar for data analysis\r\nAbility to identify and fix bugs in code\r\nStrong team working skills\r\nAbility to manage own workload\r\nAbility to build positive relations within and outside the organisation,\r\nwith health professionals and managers\r\nExcellent written and oral communication skills\r\nDesirable criteria\r\nAbility to write documentation and deliver training\r\nHow identified\r\nApplication/Interview/Test\r\nJob Description\r\nRole Purpose:\r\nA data scientist is required to assist with the day to day work\r\nwithin a data science team at Nottinghamshire Healthcare. The work of\r\nthe team is quite varied and includes statistical analysis, data\r\nengineering, and producing reports and dashboards that help clinical and\r\nmanagerial staff understand their data.\r\nThe team also carries out work that is funded by national bodies and\r\nthe postholder will be required to assist in the management of data to\r\nsupport these projects. The team has a strong belief in the power of\r\nopen source to accelerate learning in the NHS and the team and\r\npostholder will be able to open source some of their code as well as\r\ncontributing to relevant open source projects.\r\nRole Context:\r\nThis post is based within the Clinical Development Unit of\r\nNottinghamshire Healthcare NHS Foundation Trust. The Trust provides\r\nintegrated healthcare services, including mental health, intellectual\r\ndisability and physical health. The Trust employs over 9000 staff\r\nprovide these services in a variety of settings, ranging from the\r\ncommunity through to acute wards, as well as secure settings.\r\nThe Clinical Development Unit is a specialist analytic unit set up\r\nwithin Nottinghamshire Healthcare NHS Trust with a remit to analyse a\r\nrange of Trust and regional data using public health, epidemiological,\r\nstatistical, and machine learning methods, and to use open source\r\nprogramming approaches to produce robust and reproducible reports. It\r\nalso contributes to and supports the vital work of the Trust’s main\r\nApplied Informatics team.\r\nKey Accountabilities\r\nWorking Practice\r\nProduce analyses of data using appropriate statistical techniques,\r\nincluding public health and epidemiological focused analyses.\r\nPerformance measures: Production of analyses\r\nTo provide high quality support for analysis and intelligence\r\ninterpretation. Offering clear explanations in relation to the complex\r\nstatistical methodologies used. To understand and interpret results of\r\nanalyses to suggest what conclusions can be drawn.\r\nPerformance measures: Production of reports with associated\r\nmethodology\r\nDeploy reports and dashboards to assist with ongoing project work\r\nwithin the department.\r\nPerformance measures: Deployment of reports and dashboards\r\nHelping team members and the wider Trust and partner organisations to\r\ndevelop skills and understanding in the development, deployment, use,\r\nand interpretation of data.\r\nPerformance measures: Evidence of work with Trust and partner\r\norganisations to promote best practice in the data science methods\r\nWriting and conducting unit tests for code as well as testing and\r\nfixing bugs.\r\nPerformance measures: Production of technical scripts with notes\r\nTo communicate effectively to non-technical internal/external\r\ncontacts in both written and verbal form.\r\nPerformance measures: Appropriate documentation produced\r\nAnalysing and suggesting ways to improve the work of the department\r\nwith respect to data engineering, reporting, or productionisation of\r\nanalytic work, including the use of public health and epidemiological\r\nbased methods.\r\nPerformance measures: Improvements in work implemented\r\nTo develop, maintain and utilise a range of NHS datasets working\r\nclosely with other analytic teams and data scientists.\r\nPerformance measures: Production of dataset(s) for multiple uses\r\nDimensions\r\nThe postholder will produce analyses, reports, and dashboards\r\nrelating to data captured across the Trust as well as (where\r\nappropriate) within the integrated care system (particularly in regard\r\nto public health and population health management-based approaches).\r\nThey will find and help to bid for work funded regionally and\r\nnationally. They will contribute to open source repositories and promote\r\ngood practice with data science regionally and nationally.\r\nSafeguarding\r\nAll employees are responsible for taking all reasonable measures to\r\nensure that the risks of harm to children and vulnerable adults are\r\nminimised. They should take all appropriate actions to address concerns,\r\nworking to agreed local policies and procedures including the guidance\r\non Safeguarding, in partnership with other relevant agencies. This\r\nincludes accessing appropriate training, advice and support.\r\nDisclosure and Barring\r\nServices\r\nWhere this post relates to the types of work, activity, employment or\r\nprofession as set out in The Exceptions Order made under the\r\nRehabilitation of Offender Act 1974; the post will be subject to a DBS\r\nDisclosure check at the point of recruitment and thereafter, as the\r\nTrust determines appropriate. The level of the check will be determined\r\nby the type of activities undertaken and the level of contact the post\r\nholder will have with children and/or adults in receipt of health\r\nservices.\r\nInfection Control\r\nAll employees of Nottinghamshire Healthcare NHS Foundation Trust have\r\nan individual responsibility to have knowledge of and employ the basic\r\nprinciples of infection prevention and control practice. All employees\r\nmust comply with Infection Prevention and control mandatory training\r\nrequirements specific to their role.\r\nEquality & Diversity\r\nAll staff should be able to demonstrate an understanding of and\r\ncommitment to Equality, Diversity and Inclusion as identified within the\r\nTrust’s Equality and Diversity Policy and associated Equality, Diversity\r\nand Human Rights legislation.\r\nSustainability\r\nIt is the responsibility of all staff to minimise the Trust’s\r\nenvironmental impact wherever possible. This will include recycling,\r\nswitching off lights, computers, monitors and equipment when not in use.\r\nHelping to reduce paper waste by minimising printing/copying and\r\nreducing water usage, reporting faults and heating/cooling concerns\r\npromptly and minimising travel. Where the role includes the ordering and\r\nuse of supplies or equipment the post holder will consider the\r\nenvironmental impact of purchases.\r\nData Quality Statement\r\nAll staff of Nottinghamshire Healthcare NHS Foundation Trust have a\r\nresponsibility for data quality; improved data quality leads to better\r\ndecision-making across the Trust. The more high-quality data, the more\r\nconfidence the organisation has in decisions. Good data decreases risk\r\nand can result in consistent improvements in results. Employees within\r\ndata roles have a responsibility for inputting high quality data\r\n(accurate, valid, timely, complete) and for ensuring that high quality\r\ndata is maintained.\r\nCommunication\r\nManaging issues arising from within the Trust and dealing with\r\nexplaining complex ideas, occasionally of a sensitive/conflicting\r\nnature\r\nLiaise and negotiate with internal and external customers to meet\r\ninformation requirements in an efficient and cost-effective way\r\nUse tact, diplomacy and clarity of communication to aid in the\r\ndissemination of complex information to internal and external customers\r\nat all levels\r\nUse appropriate influencing skills when providing advice and support\r\nfor internal and external customers in the use and interpretation of\r\ndata, particularly where such data conflicts or does not support initial\r\nassumptions or expectations\r\nUse statistical and modelling techniques appropriate to the audience\r\nto present information and concisely communicate findings\r\nPresent, interpret and explain complicated statistical and analytical\r\ntheories/models to large groups at meetings and presentations\r\nDemonstrate developed tools and interactive models to future\r\nusers\r\nCommunicate effectively with staff across the organisation,\r\nespecially with those in the clinical development unit, performance, and\r\nthe data warehouse team, as well as data scientists nationally\r\nAbility to write documentation and deliver training\r\nRepresent the Trust’s core values in all communication\r\nKnowledge, Training and\r\nExperience\r\nExperience of writing and maintaining code, especially SQL and\r\nR/Python, for data analysis\r\nExperience of creating and maintaining SQL tables and views for use for\r\nmultiple purposes\r\nTo understand and advise on data quality issues and appropriate data\r\nvalidation techniques.\r\nTo develop, maintain, and collaborate on a complex code-base using\r\nrelevant programming languages and tools, such as Git\r\nTo design or contribute to scripts or programmes being developed to\r\nautomate the creation of analysis and reports.\r\nExperience of summarising and presenting complex analyses to varied\r\naudiences.\r\nKnowledge and a strong interest in analytics and identifying key\r\nmessages in analytics\r\nEducated to Master’s level in a computing/ quantitative subject or\r\nequivalent experience\r\nAnalytical and Judgement\r\nSkills\r\nAdvanced quantitative analysis skills, statistics and/or machine\r\nlearning\r\nExcellent skills with SQL and R/Python or statistical programming\r\nenvironment\r\nAbility to create solutions to problems\r\nAbility to identify and fix bugs in code\r\nAbility to critique and improve existing processes\r\nAbility to review assumptions and apply judgement\r\nExcellent analytical and numeric skills\r\n#Planning and Organisational Skills\r\nPlanning and organisation of tasks for self and occasionally others\r\nwhich includes making short term adjustments to plans to reflect changes\r\nin workload e.g. urgent requests for problems solving, software fixes\r\n(workarounds and permanent fixes) or assistance requests from others in\r\nthe team, key users and stakeholders.\r\nResponsible for own workload, within the tasks as agreed with the\r\nsenior data scientist. Expected to work with very little supervision and\r\nto be able to provide progress reports, reprioritise according to\r\ncompeting demands, and to complete work within the timelines agreed\r\nbetween the customer and the team.\r\nPhysical Skills\r\nAdvanced keyboard skills There is a requirement to travel to\r\ndifferent locations within the Trust\r\nResponsibility for Patient/Client Care\r\nIncidental contact with patients.\r\nEnsure appropriate Information Governance/Data Protection guidelines are\r\napplied to ensure patient confidentiality\r\nResponsibility for\r\nPolicy/Service Development\r\nImplements policies in own work area whilst proposing changes to\r\nreporting processes, training programmes, and system designs which may\r\nhave an impact on the Trust and other stakeholders\r\nContribute to recommendations on changes to data science practices to\r\nline manager and assist in implementing Responsibility for Financial and\r\nPhysical Resources\r\nResponsible for the proper and safe use of IT equipment by users\r\nResponsibility for HR\r\nThe post holder will be required to deliver specialist training or\r\nprovide skills transfer to others within the Team or to key users and\r\nstakeholders when implementing new systems\r\nTraining may be either formal or informal using different methods\r\nThe post holder may be required to support any future recruitment\r\nprocess by being involved in short listing and interview processes in\r\nthe recruitment and selection of new staff\r\nResponsibility for\r\nInformation Resources\r\nThe principal data scientist role will provide analysis of user\r\nneeds, designing of systems, development of code, testing and\r\nmaintenance of systems.\r\nThe post holder will be required to use appropriate software of choice\r\nto develop or create reporting mechanisms.\r\nThe post holder will be expected to adhere to Information Governance\r\nprocedures and any partnership agreements and to raise any issues\r\nappropriately.\r\nResponsibility for\r\nResearch and Development\r\nKeep up to date with best practices in the technical field in order\r\nto enable the department to improve the way in which it implements\r\nsystems, leading to quicker development, easier maintainability and\r\nhigher quality. Spreading the results of these investigations throughout\r\nthe department.\r\nResearching new technologies, white papers and leading industry concepts\r\nand processes in order to support continuous improvement\r\nActively undertaking R&D activities into specialised technical areas\r\nas directed by the senior data scientist. Mentoring other staff in the\r\nresults of these investigations. Contributing to recommendations as to\r\nthe future technical direction of the department as a result of the\r\nR&D activities. Influencing key decision makers on these technical\r\nmatters.\r\nFreedom to Act\r\nWorks independently, within the direction set by the department’s\r\nbroad technical strategy. Has a high level of disciplined\r\nself-management and ownership of outcomes.\r\nAs a specialist in this area will act independently and take action when\r\ndealing with uncertain situations or optimising opportunities.\r\nRecommending appropriate change and solutions whilst proactively\r\nengaging with the wider organisation to identify areas for improvement\r\nand taking the necessary action to implement the changes. Has\r\nresponsibility for the implementation of own development work and\r\noccasionally supports other team members in achieving their goals. Makes\r\nindependent technical decisions on a regular basis and implements those\r\ndecisions without reference to line management.\r\nPhysical Effort\r\nThere is a frequent requirement for sitting or standing in a\r\nrestricted position for a substantial proportion of the working time\r\nMental Effort\r\nFrequent concentration required, occasional prolonged when dealing\r\nwith complex coding requirements, checking function and capability using\r\nvarious techniques\r\nEmotional Effort\r\nAbility to work under pressure\r\nThe post holder may be required to manage expectation of users, when\r\ndealing with conflicting interests and may need to deliver unwelcome\r\nnews to a customer if expectations cannot be achieved including the\r\nexplanation of issues and delays to systems and services to customers\r\nand users.\r\nWorking Conditions\r\nUse of VDU more or less continuously\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-21-data-scientist-band-7-job/img/Flowers_of_Paradise_R_ F_Hallward.jpg",
    "last_modified": "2023-09-01T10:10:40+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-21-shiny-developer-band-7-job/",
    "title": "Shiny Developer job",
    "description": "An accessible version of the Shiny Developer band 7 job in the team",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {}
      }
    ],
    "date": "2022-07-21",
    "categories": [
      "Jobs"
    ],
    "contents": "\r\nBackground\r\nDue to formatting issues this is an accessible (we hope but please\r\nsay if not!) version of the band 7 role in the data science team. It’s\r\nalso available for future reference after the post advert closes.\r\nApply here\r\nThe Nottinghamshire Healthcare NHS Foundation Trust TRACS\r\nsystem and NHS\r\nJobs\r\nJob Advert\r\nMain area: Data Science\r\nGrade: Band 7\r\nContract: Fixed term: 12 months\r\nHours: Full time - 37.5 hours per week\r\nJob ref: 186-1253-22-CS\r\nSite: Duncan Macmillan House\r\nTown: Nottingham\r\nSalary: £40,057 - £45,839 per annum (pro rata for part time)\r\nSalary period: Yearly\r\nClosing: 14/08/2022 23:59\r\nJob overview\r\nWe’re building the future of text analytics for patient experience\r\ndata at Nottinghamshire Healthcare NHS Trust, and open sourcing it so\r\neveryone can use it. This post supports a team working on an NHSE funded\r\nproject to build and deploy text analytics for patient experience data\r\nfor several pilot trusts, with a view to recruiting more as the work\r\nmatures.\r\nWe need your skills in Python, text mining, machine learning, deep\r\nlearning, and everything else that’s needed to make reliable, accurate,\r\nperformant models that can read and theme text data. We use R and Shiny\r\non the frontend and Python and MySQL in the back, and you can see the\r\ncode here\r\nhttps://github.com/CDU-data-science-team/pxtextmining https://github.com/CDU-data-science-team/experiencesdashboard\r\nThe team are fully hybrid and this post can be offered on a remote\r\nbasis.\r\nMain duties of the job\r\nProducing a frontend/ dashboard implementation of the text mining\r\nsystem for use by partner trusts\r\nAssisting with the deployment of the text mining system in the cloud\r\nfor access by partner organisations\r\nCarrying out, feeding back, and acting on the results of user\r\ntesting\r\nAnalysing and suggesting ways to improve the algorithms and\r\ndashboard implementation\r\nHelping team members and the wider Trust to develop skills and\r\nunderstanding in the development, deployment, use, and interpretation of\r\ninteractive dashboards produced with Shiny\r\nAssist with the development and testing of user documentation for\r\nthe Shiny application\r\nDetailed job\r\ndescription and main responsibilities\r\nWe are a small and friendly team of data scientists with expertise in\r\nRAP/ data pipelines, forecasting, statistics, and Shiny. Our team\r\nbelieves in using open source data science to make the world better and\r\nyou will see on our GitHub that we share as much of our work as we\r\ncan.\r\nTwo of the team are senior fellows in NHS-R and if you join this team\r\nyou will be part of a large and thriving national community of data\r\nscientists using R and Python in healthcare.\r\nThe job will involve writing and deploying text mining and machine\r\nlearning algorithms as part of a team building models for use anywhere\r\nNHS organisations collect patient experience data.\r\nFor more details please refer to the job description\r\nPerson specification\r\nValues\r\nEssential criteria\r\nAll colleagues are expected to demonstrate at interview that they act\r\nin line with Nottinghamshire Healthcare NHS Foundation Trust Values:\r\nTrust, Honesty, Respect, Compassion and Teamwork\r\nPhysical requirements\r\nEssential criteria\r\nAble to sit for long periods of time at a desk in front of a computer\r\nworkstation and has advanced keyboard skills.\r\nQualifications\r\nEssential criteria\r\nEducated to degree level or equivalent experience Continuing\r\nProfessional Development\r\nDesirable criteria\r\nPost graduate qualification or equivalent relevant experience\r\nHow identified\r\nApplication\r\nExperience\r\nEssential criteria\r\nExperience of writing and maintaining code, especially R for data\r\nanalysis/ web application development\r\nExperience of web application programming for data science, especially\r\nR/ Shiny but also e.g. JavaScript, React\r\nExperience of interpreting and analysing complex information and\r\npresenting it in a simplified manner\r\nDesirable criteria\r\nExperience of designing and delivering training to small groups\r\nExperience with setup and maintenance of Linux server\r\nHow identified\r\nApplication/Interview/Test\r\nKnowledge\r\nEssential criteria\r\nKnowledge of web application programming paradigms, particularly for\r\ndata science (especially Shiny but also e.g. JavaScript, node.js,\r\nReact)\r\nDesirable criteria\r\nKnowledge of Linux terminal and server maintenance\r\nHow identified\r\nApplication/Interview/Test\r\nSkills\r\nEssential criteria\r\nExcellent numerical/statistical/ skills\r\nExcellent skills with R/Python or similar for data analysis and\r\ninterface design (e.g. Shiny)\r\nAbility to identify and fix bugs in code\r\nStrong team working skills\r\nAbility to manage own workload\r\nAbility to build positive relations within and outside the organisation,\r\nwith health professionals and managers\r\nAbility to concentrate when carrying out detailed analysis over\r\nsustained periods\r\nExcellent written and oral communication skills\r\nDesirable criteria\r\nAbility to write documentation and deliver training\r\nHow identified\r\nApplication/Interview/Test\r\nJob Description\r\nRole Purpose:\r\nA shiny developer is required to continue a programme of work funded\r\nby NHS England, initially for one year but with a view to continuing\r\nfunding into subsequent years. The purpose of the work is to refine a\r\ntext classification and reporting system that has been developed that\r\nprocesses patient experience data and tags it by theme and sentiment.\r\nThis work will take modern machine learning approaches such as zero\r\nshot, weak supervision, and human in the loop learning and couple them\r\nwith an intuitive user interface in order to allow completely new novel\r\ndatasets and theme taxonomies to be accurately predicted by an ML model\r\nwith minimal labelled data. The interface will also provide users with\r\nuseful and easy to use summaries of their data.\r\nThe work will be open sourced and provided to the whole NHS system\r\nfree of charge and this is a key deliverable of the project. This work\r\nhas the potential to greatly increase the amount of insight generated by\r\npatient feedback across all services in the NHS, and thereby improve the\r\nresponsiveness of services to feedback.\r\nRole Context:\r\nThis post is based within the Clinical Development Unit of\r\nNottinghamshire Healthcare NHS Foundation Trust. The Trust provides\r\nintegrated healthcare services, including mental health, intellectual\r\ndisability and physical health. The Trust employs over 9000 staff\r\nprovide these services in a variety of settings, ranging from the\r\ncommunity through to acute wards, as well as secure settings.\r\nThe Clinical Development Unit is a specialist analytic unit set up\r\nwithin Nottinghamshire Healthcare NHS Trust with a remit to analyse a\r\nrange of Trust and regional data using public health, epidemiological,\r\nstatistical, and machine learning methods, and to use open source\r\nprogramming approaches to produce robust and reproducible reports. It\r\nalso contributes to and supports the vital work of the Trust’s main\r\nApplied Informatics team.\r\nThe post is offered on a 12 month contract.\r\nTrust values\r\nAll colleagues are expected to demonstrate at interview and\r\nthroughout employment that they act in line with Nottinghamshire\r\nHealthcare NHS Foundation Trust Values: Trust, Honesty, Respect,\r\nCompassion and Teamwork\r\nKey Accountabilities\r\nWorking Practice\r\nAssisting with the deployment of the text mining system in the cloud\r\nfor access by partner organisations\r\nPerformance measures: Deployment of software to agreed\r\nspecification\r\nCarrying out, feeding back, and acting on the results of user\r\ntesting\r\nAnalysing and suggesting ways to improve the algorithms and dashboard\r\nimplementation\r\nPerformance measures: Evidence of user testing and development work\r\nbased on user testing\r\nWriting and conducting unit tests for code as well as testing and\r\nfixing bugs\r\nPerformance measures: Robust and tested code\r\nHelping team members and the wider Trust and partner organisations to\r\ndevelop skills and understanding in the development, deployment, use,\r\nand interpretation of text mining algorithms in a patient experience\r\ncontext\r\nPerformance measures: Evidence of work with Trust and partner\r\norganisations to promote best practice in the use of text mining\r\nalgorithms in a patient experience context\r\nHelping team members and the wider Trust to develop skills and\r\nunderstanding in the development, deployment, use, and interpretation of\r\ninteractive dashboards produced with Shiny\r\nPerformance measures: Evidence of work with Trust and partner\r\norganisations to promote best practice in the use of dashboards in a\r\npatient experience context\r\nAssist with the development and testing of user documentation for the\r\nShiny application\r\nPerformance measures: Appropriate documentation produced\r\nDimensions\r\nThe postholder will produce dashboards to interact with machine\r\nlearning models and to report on patient experience data for\r\nNottinghamshire Healthcare NHS Foundation Trust and at least five other\r\ntrusts (subject to the trusts’ capacity to engage). The code will be\r\nopen sourced, enabling collaboration where appropriate with other\r\nindividuals across the NHS.\r\nSafeguarding\r\nAll employees are responsible for taking all reasonable measures to\r\nensure that the risks of harm to children and vulnerable adults are\r\nminimised. They should take all appropriate actions to address concerns,\r\nworking to agreed local policies and procedures including the guidance\r\non Safeguarding, in partnership with other relevant agencies. This\r\nincludes accessing appropriate training, advice and support.\r\nDisclosure and Barring\r\nServices\r\nWhere this post relates to the types of work, activity, employment or\r\nprofession as set out in The Exceptions Order made under the\r\nRehabilitation of Offender Act 1974; the post will be subject to a DBS\r\nDisclosure check at the point of recruitment and thereafter, as the\r\nTrust determines appropriate. The level of the check will be determined\r\nby the type of activities undertaken and the level of contact the post\r\nholder will have with children and/or adults in receipt of health\r\nservices.\r\nInfection Control\r\nAll employees of Nottinghamshire Healthcare NHS Foundation Trust have\r\nan individual responsibility to have knowledge of and employ the basic\r\nprinciples of infection prevention and control practice. All employees\r\nmust comply with Infection Prevention and control mandatory training\r\nrequirements specific to their role.\r\nEquality & Diversity\r\nAll staff should be able to demonstrate an understanding of and\r\ncommitment to Equality, Diversity and Inclusion as identified within the\r\nTrust’s Equality and Diversity Policy and associated Equality, Diversity\r\nand Human Rights legislation.\r\nSustainability\r\nIt is the responsibility of all staff to minimise the Trust’s\r\nenvironmental impact wherever possible. This will include recycling,\r\nswitching off lights, computers, monitors and equipment when not in use.\r\nHelping to reduce paper waste by minimising printing/copying and\r\nreducing water usage, reporting faults and heating/cooling concerns\r\npromptly and minimising travel. Where the role includes the ordering and\r\nuse of supplies or equipment the post holder will consider the\r\nenvironmental impact of purchases.\r\nData Quality Statement\r\nAll staff of Nottinghamshire Healthcare NHS Foundation Trust have a\r\nresponsibility for data quality; improved data quality leads to better\r\ndecision-making across the Trust. The more high-quality data, the more\r\nconfidence the organisation has in decisions. Good data decreases risk\r\nand can result in consistent improvements in results. Employees within\r\ndata roles have a responsibility for inputting high quality data\r\n(accurate, valid, timely, complete) and for ensuring that high quality\r\ndata is maintained.\r\nCommunication\r\nLiaise and negotiate with internal and external customers to meet\r\ninformation requirements in an efficient and cost-effective way.\r\nUse tact, diplomacy and clarity of communication to aid in the\r\ndissemination of complex information to internal and external customers\r\nat all levels.\r\nUse appropriate influencing skills when providing advice and support\r\nfor internal and external customers in the use and interpretation of\r\ndata, particularly where such data conflicts or does not support initial\r\nassumptions or expectations.\r\nUse design principles and interactive programming paradigms\r\nappropriate to the audience to present information and concisely\r\ncommunicate findings.\r\nPresent, interpret and explain complicated statistical and analytical\r\ntheories/models to large groups at meetings.\r\nDemonstrate developed tools and interactive models to future\r\nusers.\r\nProvide training for end users in each site in the use and\r\ninterpretation of model results.\r\nCommunicate effectively with CDU, Involvement, NHSE, and partner\r\norganisations\r\nAbility to write documentation and deliver training.\r\nWork collaboratively with key members of CDU, Involvement, NHSE, and\r\npartner organisations, in a way which respects different perspectives\r\nand makes best use of the varied expertise.\r\nRepresent the Trust’s core values in all communication.\r\nKnowledge, Training and\r\nExperience\r\nExperience of writing and maintaining code, especially R/ Shiny, for\r\ninterface design.\r\nExperience of creating and managing statistical work programmes / data\r\nscience pipelines.\r\nExperience of programming web applications for data science.\r\nExperience of summarising and presenting complex analyses to varied\r\naudiences.\r\nExperience of interpreting and analysing complex information and\r\npresenting it in a simplified manner.\r\nKnowledge and a strong interest in analytics and identifying key\r\nmessages in analytics.\r\nEducated to Master’s level in a computing/ quantitative subject or\r\nequivalent experience.\r\nAnalytical and Judgement\r\nSkills\r\nExcellent skills with R/ Shiny and other web application/ database\r\nframeworks (e.g., React).\r\nAbility to create solutions to problems.\r\nAbility to identify and fix bugs in code.\r\nAbility to critique and improve existing processes.\r\nAbility to review assumptions and apply judgement.\r\nExcellent analytical and numeric skills.\r\nPlanning and Organisational\r\nSkills\r\nPlanning and organisation of tasks for self and occasionally others\r\nwhich includes making short term adjustments to plans to reflect changes\r\nin workload e.g. urgent requests for problems solving, software fixes\r\n(workarounds and permanent fixes) or assistance requests from others in\r\nthe team, key users and stakeholders.\r\nResponsible for own workload, within the tasks as agreed with the\r\nsenior data scientist. Expected to work with very little supervision and\r\nto be able to provide progress reports, reprioritise according to\r\ncompeting demands, and to complete work within the timelines agreed\r\nbetween the customer and the team.\r\nPhysical Skills\r\nAdvanced keyboard skills.\r\nThere is a requirement to travel to different locations within the\r\nTrust and collaborating sites in the Midlands.\r\nResponsibility for\r\nPatient/Client Care\r\nIncidental contact with patients.\r\nEnsure appropriate Information Governance/Data Protection guidelines are\r\napplied to ensure patient confidentiality.\r\nResponsibility for\r\nPolicy/Service Development\r\nImplements policies in own work area whilst proposing changes to\r\nreporting processes, training programmes, and system designs which may\r\nhave an impact on the Trust and the pilot sites.\r\nContribute to recommendations on changes to data science practices to\r\nline manager and assist in implementing.\r\nResponsibility\r\nfor Financial and Physical Resources\r\nResponsible for the proper and safe use of IT equipment by users.\r\nResponsibility for HR\r\nThe post holder will be required to deliver specialist training or\r\nprovide skills transfer to others within the Team or to key users and\r\nstakeholders when implementing new systems.\r\nTraining may be either formal or informal using different methods.\r\nThe post holder may be required to support any future recruitment\r\nprocess by being involved in short listing and interview processes in\r\nthe recruitment and selection of new staff.\r\nResponsibility for\r\nInformation Resources\r\nThe senior data scientist role will provide analysis of user needs,\r\ndesigning of systems, development of code, testing and maintenance of\r\nsystems.\r\nThe post holder will be required to use appropriate software of choice\r\nto develop or create reporting mechanisms.\r\nThe post holder will be partially responsible for the safe and\r\nappropriate storage, use and publication of patient feedback data for\r\nthe Trust and partner pilot sites, and is expected to adhere to\r\nInformation Governance procedures and partnership agreements.\r\nResponsibility for\r\nResearch and Development\r\nKeep up to date with best practices in the technical field in order\r\nto enable the department to improve the way in which it implements\r\nsystems, leading to quicker development, easier maintainability and\r\nhigher quality. Spreading the results of these investigations throughout\r\nthe department.\r\nResearching new technologies, white papers and leading industry concepts\r\nand processes in order to support continuous improvement.\r\nActively undertaking R&D activities into specialised technical areas\r\nas directed by the principal data scientist. Mentoring other staff in\r\nthe results of these investigations. Contributing to recommendations as\r\nto the future technical direction of the department as a result of the\r\nR&D activities. Influencing key decision makers on these technical\r\nmatters.\r\nFreedom to Act\r\nWorks independently, within the direction set by the department’s\r\nbroad technical strategy. Has a high level of disciplined\r\nself-management and ownership of outcomes.\r\nAs a specialist in this area will act independently and take action when\r\ndealing with uncertain situations or optimising opportunities.\r\nRecommending appropriate change and solutions whilst proactively\r\nengaging with the wider organisation to identify areas for improvement\r\nand taking the necessary action to implement the changes. Has\r\nresponsibility for the implementation of own development work and\r\noccasionally supports other team members in achieving their goals. Makes\r\nindependent technical decisions on a regular basis and implements those\r\ndecisions without reference to line management.\r\nPhysical Effort\r\nThere is a frequent requirement for sitting or standing in a\r\nrestricted position for a substantial proportion of the working time\r\nMental Effort\r\nFrequent concentration required, occasional prolonged when dealing\r\nwith complex coding requirements, checking function and capability using\r\nvarious techniques\r\nEmotional Effort\r\nAbility to work under pressure\r\nThe post holder may be required to manage expectation of users, when\r\ndealing with conflicting interests and may need to deliver unwelcome\r\nnews to a customer if expectations cannot be achieved including the\r\nexplanation of issues and delays to systems and services to customers\r\nand users.\r\nWorking Conditions\r\nUse of VDU more or less continuously\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-21-shiny-developer-band-7-job/img/Le_Marechal_Ferrant_opéra_comique_en_deux_actes.jpg",
    "last_modified": "2023-09-01T10:10:40+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-05-extending-issues-in-github/",
    "title": "Extending issues in GitHub",
    "description": "Details on features used in GitHub for creating issue templates and labels, particularly the use of `wontfix`.",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/letxuga007": {}
        }
      }
    ],
    "date": "2022-07-05",
    "categories": [
      "GitHub"
    ],
    "contents": "\r\nUsing templates\r\nIn Settings (the last tab along the top of a GitHub\r\nrepository which starts Code… Issues… Pull Requests) there is now\r\nsection in the General for issue templates. Selecting the\r\nGreen button for Set up templates takes you to a drop down\r\nselection box saying Add template and select.\r\nThere are 3 templates available and all can be customised, bug\r\nreport, feature request and custom template.\r\nIn the internal package the team uses we have:\r\nBug report\r\nFeature request\r\nRefactor\r\nSQL server issue template\r\nas the package is the link between the SQL data warehouse and R.\r\nEvery time a template is added or updated this adds a commit to the\r\nhistory.\r\nIssue tags/labels\r\nUsing the issue template you can also add the labels that are\r\nappropriate like bug. Our internal package mainly uses:\r\nbug\r\ndatabase\r\ndocumentation\r\nduplicate\r\nenhancement\r\nquestion\r\nrefactor\r\nand one that is being used more frequently now when the issue is\r\nrelated to the SQL database or another package’s issue\r\nwontfix.\r\nFor example, the package used to access the SQL database retrieves\r\ndata from different databases using {dbplyr} and so if any code attempts\r\nto join the two we get the error:\r\nError in `auto_copy()`:\r\n! `x` and `y` must share the same src.\r\ni set `copy` = TRUE (may be slow).\r\nwhich is related to an issue\r\nthat was closed in 2018 from {bigrquery}. This is referenced in the\r\nissue but with the label wontfix for reference. In fact,\r\ntwo issues have been created now!\r\nReferences to other packages\r\nIf you reference another issue from someone else’s repository it will\r\nappear in their GitHub issue with the description\r\nmentioned this issue....\r\nIf it’s from a restricted private package only you will see the\r\nreference.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-05-extending-issues-in-github/img/a_text-book_of_mineralogy.jpg",
    "last_modified": "2023-09-01T10:10:40+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-24-statistical-process-control-r-packages/",
    "title": "Statistical Process Control (SPC) R Packages",
    "description": "A quick introduction to the R packages available and how they are used by the team",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/letxuga007": {}
        }
      }
    ],
    "date": "2022-06-24",
    "categories": [
      "Statistical Process Control",
      "Resources"
    ],
    "contents": "\r\nStatistical Process Control (SPC)\r\nSPCs are very popular in analysis in the NHS and we are lucky now to have many resources available to understand, produce and explain SPC charts. In the NHS the resources used for SPC are promoted through an initiative called the Making Data Count hosted by NHS England and Improvement who have a FuturesNHS Workspace of the same name and have templates in SQL and Excel.\r\nR packages\r\nqicharts2\r\nUntil very recently the main package used by the team has been {qicharts2}, available through CRAN, which has a really good vignette to get started. The package include many types of SPC including charts for rare events. The package creator, Jacob Anhøj, has also run workshops and given talks (1:45:23) at the NHS-R conferences.\r\nNote that the run chart in qicharts2 uses an additional rule that Jacob Anhøj has written about in statistical journals and has blogged about more generally for the NHS-R Community.\r\nNHSRplotthedots\r\nIn response to the work around Making Data Count, many analysts in the NHS were using R to produce SPC charts but wanted to change the colours/add logos and use the NHS England/Improvement rules. Through the NHS-R Community the analysts were able to work together in collaboration to produce the {NHSRplotthedots} package which is now on CRAN.\r\ncharter packages\r\nJohn Mackintosh, an NHS data analyst/BI developer in Scotland, has created a series of charter packages that he uses in his work and has shared on GitHub and CRAN:\r\nruncharter (github repository) CRAN\r\ncusumcharter (github repository) CRAN\r\nspccharter\r\nThese are particuarly good packages to use for analysing multiple charts at once.\r\nFunnel Plots\r\nAnother form of SPC which is very useful and often appears in Public Health analysis is the funnel plot. Public Health England (now UKHSA) have a few resources on how touse and interpret funnel plots with a spreadsheet template and in R, the package {FunnelPlotR} has been created by Chris Mainey and can now be found on the NHS-R Community GitHub and is available on CRAN.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-06-24-statistical-process-control-r-packages/img/Two_Centuries_of_Soho_p311.jpg",
    "last_modified": "2023-09-01T10:10:40+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-17-caseload-over-time/",
    "title": "Caseload over time",
    "description": "How to count open referrals in a given period of time.",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/letxuga007": {}
        }
      }
    ],
    "date": "2022-05-17",
    "categories": [
      "Analysis"
    ],
    "contents": "\r\nFinding open referrals now is reasonably easy, it will be to look for where there is no date/time in discharges. However, if we need the open referrals last month, some of those referrals may be been closed since then.\r\nA really nice graphic that illustrates the logic required to consider referrals (or other things) in a given period of time is this:\r\nScreenshot of a 6 patient scenarios graphically shown and explained in text belowThis is an image taken from the AphA website but has since been moved sadly and details of the original authors lost. If you are the original owner or know who did this please do get in touch with an issue or email.\r\nTo explain the diagram:\r\nPatient one had an open referral that was closed before the time we are counting.\r\nPatient two had a open referral that closed in the time period.\r\nPatient three had an open and closed referral within the time period.\r\nPatient four started with a referral in the time period but ended afterwards.\r\nPatient five started and ended after the time period.\r\nPatient six started before and ended after the time period.\r\nOpen referrals to count are therefore include Patient 2, Patient 3, Patient 4 and Patient 6.\r\nUsing SQL\r\nThere are three ways of doing this and because we use sk (synthetic or sythnesised keys) in our SQL tables for speed we join to a specific date look up table.\r\nWHERE clause\r\nOne line in the where clause (special thanks to SQLBarney for this code):\r\n\r\n\r\ndeclare @StartDate int = '20180401'\r\ndeclare @EndDate int = '20190401'\r\n\r\n--DischargeDate_sk = 0 won't be needed if the table doesn't use synthetic dates\r\n\r\nSELECT Team, COUNT(DISTINCT patient) AS OpenReferrals\r\nFROM SQLTable\r\nWHERE ((DischargeDate_sk >= @StartDate OR DischargeDate_sk < = 0) AND ReferralDate_sk <= @EndDate)\r\nGROUP BY Team\r\n\r\nExplicitly detail within the WHERE clause\r\nJust to show exactly to replicate the patients included:\r\n\r\n\r\ndeclare @StartDate int = '20180401'\r\ndeclare @EndDate int = '20190401'\r\n\r\nSELECT Team, COUNT(DISTINCT patient) AS OpenReferrals\r\nFROM SQLTable\r\nWHERE ((ReferralDate_sk < @StartDate AND (DischargeDate_sk > = @EndDate OR DischargeDate_sk = 0)) --Started before period and ended after period or still open (Patient 6)\r\n\r\nOR (ReferralDate_sk < = @EndDate AND DischargeDate_sk > = @StartDate) --Started and ended in period (Patient 3) and (Patient 2)\r\n\r\nOR (ReferralDate_sk > = @StartDate AND (DischargeDate_sk > = @EndDate OR r.DischargeDate_sk = 0)))--Started in period and ended after period or still open (Patient 4)\r\nGROUP BY Team\r\n\r\nCreating a line for each day\r\nThis is particularly useful if you want to have individual days, for example, of open referrals that can then be aggregated up to other time units like months or years. It can produce very large datasets though depending on the time period being expanded. Counts of patients need to be on distinct ids as they will be repeated over time so:\r\nDay 1\r\nPatient A\r\nPatient B\r\nDay 2\r\nPatient B\r\nDay 3\r\nPatient C\r\nIf I want the count for the full three days the counts need to be 3 patients, but without a distinct 4 will be returned.\r\n\r\n\r\nSELECT Team, COUNT(DISTINCT patient) AS OpenReferrals\r\nFROM SQLTable\r\nINNER JOIN DateTable AS dt1 ON ReferralDate_sk = dt1.date_sk ---Referral Date--\r\nINNER JOIN DateTable AS dt2 ON DischargeDate_sk = dt2.date_sk ---Discharge Date--\r\n\r\n--The sk dates need to be joined to the date tables to return a true date for the inner join\r\n\r\nINNER JOIN DateTable AS d3 ON dt1.date <= d3.date AND dt2.date > = d3.date\r\n\r\nWHERE d3.fin_year_name = '2018/19'\r\nGROUP BY Team\r\n\r\nOne day’s referrals\r\nLooking for 1 day’s referrals can be a shorter line of code without the need for parameters:\r\n\r\n\r\nSELECT Team, COUNT(DISTINCT patient) AS OpenReferrals\r\nFROM SQLTable\r\nINNER JOIN DateTable AS dt1 ON ReferralDate_sk = dt1.date_sk ---Referral Date--\r\nINNER JOIN DateTable AS dt2 ON DischargeDate_sk = dt2.date_sk ---Discharge Date--\r\n\r\nWHERE dt1.date <= '31 Mar 2019' AND (dt2.date > = '31 mar 2019' OR DischargeDate_sk = 0)\r\n\r\nGROUP BY Team\r\n\r\nUsing R\r\nThis can be recreated in R using dplyr but note that the functions used can be slow for many rows. It can be hard to filter the dates, particularly in Mental Health, as some referrals are open over many years and we need to include those patients 2 and 6 - who had an open referral start before the time period.\r\n\r\n\r\ncaseload <- referrals %>%\r\n  # Ensure dates are in date format, sometimes they get changed when imported \r\n  #from SQL, particularly date format (not so much datetime)\r\n  \r\n  # Using lubridate::today() where a discharge date is not entered (still open) \r\n  #makes this easier to work with dates in later code\r\n  dplyr::mutate(\r\n    referral_date = as.Date(referrals_referral_datetime),\r\n    discharge_date = as.Date(referrals_discharge_datetime),\r\n    discharge_date = case_when(is.na(discharge_date) ~ lubridate::today(),\r\n                               TRUE ~ discharge_date)) %>%\r\n  dplyr:: select(patient,\r\n                 referral,\r\n                 team,\r\n                 referral_date,\r\n                 discharge_date\r\n  ) %>%\r\n  dplyr::group_by(patient,\r\n                  referral,\r\n                  team) %>%\r\n  # Pivot to tidy data (long)\r\n  tidyr::pivot_longer(cols = ends_with(\"Date\"),\r\n                      names_to = \"caseload\",\r\n                      values_to = \"dates\") %>% \r\n  dplyr::ungroup() # good practice to ungroup as can affect counts later\r\n\r\n# complete() fills in the date sequence so 20200101 to 20200103 will now have a \r\n#line for dates 01, 02 and 03\r\nfill_dates <- caseload %>%\r\n  tidyr::complete(dates = seq.Date(min(dates), max(dates), by = \"day\")) \r\n\r\n# counting distinct patients\r\ncaseload_counts <- fill_dates %>%\r\n  # although the dataset is now each day has a row, this is a count by month\r\n  dplyr::mutate(month = lubridate::floor_date(dates, unit = \"month\")) %>% \r\n  dplyr::group_by(team,\r\n                  month) %>%\r\n  summarise(count_distinct_patients = n_distinct(client_id))\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-17-caseload-over-time/img/a_text_book_of_mineralogy.jpg",
    "last_modified": "2023-09-01T10:10:40+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-03-creating-a-branch-and-using-issues/",
    "title": "Creating a branch and using issues",
    "description": "Steps to take when creating branches following the GitHub-Flow method with particular relation to how to tie these in with issues.",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/letxuga007": {}
        }
      }
    ],
    "date": "2022-05-03",
    "categories": [
      "Open source",
      "GitHub"
    ],
    "contents": "\r\nIn the GitHub\r\nSOP the blog refers to the GitHub\r\nflow which is self explanatory around how to create and manage\r\nbranches but within the team we have a few steps related to Issues that\r\nare not included in the flow.\r\nWhy issues are useful\r\nIssues are a searchable source of information in GitHub and are used\r\nin the team to highlight problems but also act as conversations. As a\r\nbranch is a direct copy of another branch, usually main,\r\nthe README should not have any reference to the issue that prompted the\r\nbranch creation.\r\nThe issue needs to be detailed enough that another person, who has no\r\nprior knowledge of the repository, can understand it. This may require\r\nreferences to other systems, full details of data locations (server,\r\nschema, table in SQL for example) and avoid acronyms where possible.\r\nNaming branches\r\nIn all repositories (except for blogs) every branch should correspond\r\nto an issue and have a title format of number from the issue with a\r\nshort version of the title. For example, 40-blog-distill\r\nwhen the issue is Blog about the distill template.\r\nIssues can, and should, be linked to pull requests but until the pull\r\nrequest is made there is no other link to an issue from a branch other\r\nthan a reference in the name.\r\nBlogs sites are an exception\r\nWhere an issue exists, like in the example Blog about the distill template\r\nthis should have a correspondingly named branch, however, more often\r\nthan not blogs and posts are created separate to “issues”. In this\r\ncircumstance it is ok to use branch names like the one used for this\r\nblog creating-branch-issues.\r\nClosing an issue\r\nAll issues should be closed either by linking to pull request or\r\nclosed with an explanation as to why it can be closed. Sometimes issues\r\nare raised that don’t have enough explanations or don’t become work,\r\neven then, a closing message is needed for future reference.\r\nCreate branches from GitHub\r\nIt is possible to create a branch from an Issue whilst in GitHub,\r\nthis option appears in the right hand menu under\r\nDevelopment which is also where other pull requests can be\r\nlinked to issue (both closed and open).\r\nIf you use this code will appear to be used locally on the computer\r\nin the terminal, copy and right click in the RStudio terminal to\r\npaste.\r\nNote that if you use a development branch that the new\r\nbranch will be from main which you may not necessary\r\nwant.\r\nSummary\r\nAlways create an issue (unless it’s a blog site)\r\nAlways create a corresponding branch using the issue number and\r\nshort name\r\nLink issues to pull requests\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-03-creating-a-branch-and-using-issues/img/A_Treatise_on_Crystallography.jpg",
    "last_modified": "2023-09-01T10:10:40+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-13-keeping-git-branches-tidy/",
    "title": "Keeping Git branches tidy",
    "description": "Following the GitHub SOP tidying branches needs to happen both on GitHub and locally and this details some of the ways to do that.",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/letxuga007": {}
        }
      }
    ],
    "date": "2022-04-13",
    "categories": [
      "Open source",
      "GitHub"
    ],
    "contents": "\r\nIn the GitHub\r\nSOP we detailed how\r\n\r\nBranches should be deleted as soon as possible\r\n\r\nand this is good practice both locally and on GitHub. This short post\r\ndetails the ways to do this.\r\nDelete after a merge\r\nDeleting on GitHub is very easy and doesn’t require typing to the\r\ncommand line (although that is possible too). After merging a button\r\nwill appear in purple saying Delete branch but it’s also\r\npossible to set this as the default for the repository. In this case the\r\npurple button will say Restore branch.\r\nGo to the Settings for the repository and then scroll\r\ndown to the Pull Requests section. Select the\r\nAutomatically delete head branches.\r\nMore information on this can be found on GitHub\r\nhelp pages\r\nDeleting stale branches in\r\nGitHub\r\nEven if a pull request hasn’t occurred it’s possible to use GitHub to\r\ntidy the branches\r\nScreenshot highlighting where to find the\r\nbranches link on GitHubIt’s possible to delete branches, particularly “Stale” ones using the\r\nbin icon to the far right of the branch you want to delete.\r\nRestoring a deleted branch\r\nNote that the branch will appear with a Restore button\r\nwhen first deleted but this disappears after the page is refreshed or\r\nsome time has passed.\r\nLocal branches\r\nOnce a branch is deleted remotely that doesn’t automatically mean the\r\nlocal branches have gone. Even after a pull request to another branch\r\n(like main) so to tidy up locally it’s advisable to use the\r\nTerminal if you want do to do this in RStudio and type:\r\ngit branch -d <name-of-branch>\r\nRemoving stale connections\r\nIf a branch is deleted remotely it will still appear in the local git\r\nbranches\r\nScreenshot of branch drop down menu from\r\nRStudio with the remote branches highlightedTo remove these stale connections type in the Terminal\r\ngit remote prune origin\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-13-keeping-git-branches-tidy/img/Admiralty_Manual_for_the_Deviation_of_the_Compass.jpg",
    "last_modified": "2023-09-01T10:10:40+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-12-settings-in-ssms/",
    "title": "Settings in SSMS",
    "description": "Some of the options that may be useful in SSMS",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/letxuga007": {}
        }
      }
    ],
    "date": "2022-04-12",
    "categories": [
      "SQL",
      "Accessibility"
    ],
    "contents": "\r\nIn my team I tend to still use a lot of SSMS for SQL coding. Having\r\njust got a new computer I quickly realised that the settings I’d made to\r\nmake the layout a bit easier on my eyes had been lost but thankfully I’d\r\nnoted some of the options which I’m going to share here for future me or\r\nanyone else interested in the settings in SSMS. In terms of IDE I,\r\npersonally, find R Studio much better to set up for accessibility and\r\nappreciate their regular updates around this area. However, whilst I use\r\nR Studio (and specifically RMarkdown) for SQL workflows I don’t always\r\nget the speed or error notifications like SSMS depending on the packages\r\nI use.\r\nChanging the back screen\r\ncolour\r\nThis was a tip shared with me as a background that can help some\r\npeople who are dyslexic. I’m not dyslexic but much prefer this colour as\r\nit offsets the glare of the default white background. There aren’t set\r\noptions in SSMS so you have to set the colour manually and the options\r\ncan be found in Tools/Options/Font and colors.\r\nScreenshot of the menu from\r\nTools/Options/Font and colors with the background colour already set to\r\nyellowThe default yellow in theItem background is too bright\r\nfor me so I change it in Custom... setting the\r\nfollowing:\r\nHue: 35\r\nSat 205\r\nLum 194\r\nThe change will be implemented after the program is shut down and\r\nrestarted.\r\nChanging position of the\r\nstatus bar\r\nThe status bar defaults to the bottom of the screen and says which\r\nserver you are accessing, the database and it’s also the information\r\nwhere how long a query took to run and how many rows are returned. I\r\ndidn’t like the position being at the bottom so I move this to the top\r\nin  Tools/Options/Text Editor/Editor Tab changing the\r\nStatus bar location to Top.\r\n\r\nThe change will be implemented after the program is shut down and\r\nrestarted.\r\nChanging the colour of\r\nthe status bar\r\nThis is really useful if you have access to other databases that\r\nshare names and so can help with navigation. For example, I use a\r\nserver, a dev(elopment) server and a staging server with similar names.\r\nIt means that if I query the wrong one I’ll get strange results.\r\nChanging the colour can help when I need the server and dev open at the\r\nsame time and could get muddled.\r\nThis needs to be done when first connecting to the server when you\r\nselect the Connect to database. After choosing the server you\r\nwill want the colour bar to change for select the button called\r\nOptions >> that follows Connect,\r\nCancel and Help.\r\nIn the menu that appears you can tick Use custom color:\r\nand then Select to go to the colour’s menu.\r\nScreenshot of the menu for the SQL Server\r\nwith a custom colour already selectedAdding lines to the SQL\r\nscript\r\nWhen SQL gives errors it often refers to the line of code the error\r\nrelates to. One trick is to double click on the red text in the query\r\nresults and it will take you to the line (this doesn’t always work well\r\nfor CTEs Common Table Expressions and will just take you to the top line\r\nof the CTE).\r\nLine numbers on the script is not a default setting and have to be\r\nturned on, again in Tools/Options/Text\r\neditor/Transact-SQL/General:\r\nScreenshot of the menu with the line\r\nnumbers tickedThe change will be implemented after the program is shut down and\r\nrestarted.\r\nQuery results include the\r\nheader\r\nOften when I copy out the results from SSMS I also need the column\r\nheaders and this may not be default. Got to Tools/Options/Query\r\nResults/SQL Server/Results to Grid and select\r\nInclude column headers when copying or saving the results.\r\nAny other tips?\r\nThere are probably plenty of other settings that are useful to know\r\nor sites that list these out and we are always keen to learn more so\r\nplease do get in touch with creating an issue\r\nor emailing the\r\nteam.\r\nHat Tip (and further\r\nresources)\r\nThe following was shared when this was tweeted\r\nby the team account\r\nand is a wonderful resource for many SQL techniques. The presentation\r\nincludes much of what is listed here because Barney Lawrence had shared many\r\nof these with Zoë Turner\r\nwhen he worked in Nottinghamshire Healthcare NHS Foundation Trust. It’s\r\nwonderful that these techniques are being shared publicly through\r\nvarious media.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-12-settings-in-ssms/img/notes_on_the_treatment_of_gold_ore.jpg",
    "last_modified": "2023-09-01T10:10:40+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-01-using-usethis-to-set-up-gitignore/",
    "title": "Using {usethis} to set up .gitignore",
    "description": "How to ensure that certain files cannot be accidentally committed to GitHub (or any other version controlled area).",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/letxuga007": {}
        }
      }
    ],
    "date": "2022-04-01",
    "categories": [
      "Open source",
      "GitHub"
    ],
    "contents": "\r\nUse .gitignore\r\n.gitignore is used to prevent committing certain files or folders to\r\nGitHub and can be set both locally in a project and globally.\r\nUsing the {usethis} package a lot of the set up of these files can be\r\ndone from the RStudio Console command line.\r\nSet up\r\nThese slides\r\nfrom Forwards are for package development but are fantastic to follow to\r\nget your computer set up with GitHub.\r\nCheck the Git connections are set up with:\r\n\r\n\r\nusethis::git_sitrep()\r\n\r\n\r\n\r\nIf Vaccinated: FALSE which it is most likely to be if\r\nthis is the first time {usethis} was used type:\r\n\r\n\r\nusethis::git_vaccinate()\r\n\r\n\r\n\r\nThis adds .DS_Store, .Rproj.user,\r\n.Rdata, .Rhistory, and\r\n.httr-oauth to the global .gitignore which can\r\nbe checked with:\r\n\r\n\r\nusethis::edit_git_ignore(scope = \"user\")\r\n\r\n\r\n\r\nThis open file can be altered directly and saved.\r\nChanging project .gitignore\r\nTo view and edit directly:\r\n\r\n\r\nusethis::edit_git_ignore(scope = \"project\")\r\n\r\n\r\n\r\nOr use the following to add individual files and folders like\r\nsecrets/:\r\n\r\n\r\nusethis::use_git_ignore(\"secrets/\")\r\n\r\n\r\n\r\nPotential issues with\r\nnetworks/VPNs\r\nIf git_vaccinate has and error starting with\r\n\r\nlibgit2::git_config_set_string\r\n\r\nand then a reference to a lock to a private network drive (usually\r\nwhen off the VPN/Network) then type:\r\n\r\n\r\nusethis::edit_git_config()\r\n\r\n\r\n\r\nand copy the following code, with <> changed as\r\nappropriate:\r\n[user]\r\n    email = <Email>\r\n    name = <GitHub Account name>\r\n[core]\r\n    excludesFile = C:/Users/<own folder>/.gitignore\r\n[init]\r\n    defaultBranch = main\r\n\r\nOverwriting a global\r\n.gitignore\r\nThis was a question that came out of a team Code Review and we found\r\nthe following blog\r\nfrom Scott Radcliff with the answer that yes, it is possible to\r\noverwrite a global setting if this is ever required.\r\nNot tested but examples using the ! to exempt:\r\nfor single files !data.csv\r\nfor a group of files !*.csv\r\nfor a subfolder !secrets/not-secret/ from Stackoverflow\r\noverride all rules in global !* from Stackoverflow\r\n.rda and .RData files\r\nWe also discussed .rda and .Rdata files\r\nwhich are save R data files. gitvaccinate() adds the\r\n.Rdata but doesn’t mention .rda files so be\r\naware of that.\r\nThe difference in the types of files are that .Rdata can\r\nstore single or multiple R objects but .rda can only save\r\nsingle.\r\nIt might be worthy of a blog/team review on its own but there are\r\nissues with saving multiple R objects to one place if they are updated\r\nregularly. It’s very easy to not re-save/update an R object and affect\r\nthe overall data workflow and so I, particularly as I’ve got into this\r\nvery muddle, only save one R object per .Rdata file.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-01-using-usethis-to-set-up-gitignore/img/grundzüge_der_mathematischen_geographie_und_der_landkartenprojection.jpg",
    "last_modified": "2023-09-01T10:10:40+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-21-github-sop/",
    "title": "GitHub Standard Operating Procedure",
    "description": "How we use GitHub in the CDU data science team.",
    "author": [
      {
        "name": "Chris Beeley",
        "url": {
          "https://twitter.com/chrisbeeley": {}
        }
      }
    ],
    "date": "2022-03-21",
    "categories": [
      "Open source",
      "GitHub",
      "Teamwork"
    ],
    "contents": "\r\nIntroduction\r\nWe’ve been using GitHub in the CDU data science team since 8th May 2020 (I think- that’s the oldest commit I could find). Team members past and present have used git and GitHub to varying degrees, often solo. Data science is a team sport, without question, but like any team sport it requires proper coordination between participants to be effective. Git and GitHub are very powerful tools, but they are hard to use well even when you’re collaborating just with yourself, and working well with others is another set of skills in itself. We’ve all made a lot of mistakes and learned a lot in the last two years, and this post is designed to codify some of this learning into a Standard Operating Procedure (SOP) for the benefit of team members past and present. And of course, we share it for the general benefit of others who might learn something from our experiences, and also in the hope that those with more/ different skills and knowledge might be able to help us refine it further (get in touch through the usual channels ☺️).\r\nStandard operating procedure\r\nBranches\r\nWe adopted gitflow fairly early on as the team expanded and it worked very well for our purposes. I now see that there is a health warning on the linked post and gitflow is deprecated elsewhere in favour of something like GitHub flow. I’m not going to go into the differences here because it is not the focus of this post but there are some important differences and I can feel another blog post coming on about different models of using git and GitHub once I’ve discussed the matter with team members.\r\nThe essential feature of gitflow as we’re using it is that there are three types of branches -\r\nmain branch, which is the working code,\r\ndevelopment, which is a kind of “staging” branch where code can be worked on and\r\ntested and feature branches which either fix bugs or add new features\r\nThe main idea is that feature code is merged into the development branch and the development branch is periodically merged into the main branch. Each time you merge to main you do a point release with semantic versioning.\r\nWe’ve all found that it works pretty well but it is overkill for simpler repos. The development branch is not necessary, and you can merge straight to main from a feature branch for simpler things, especially where it is mainly one person writing, maintaining, and using the code. For important stuff that we all use though, particularly {nottshcData} the development branch is really useful because it means that we can all send PRs to a branch without worrying about untested code getting out into the wild.\r\nWith all that said how have we found it best to use it in practice? A few points:\r\nMerge to development and main as frequently as possible. Especially from my point of view, as the team manager, I spend a lot of time on everyone’s repos, and if everything is tucked away in branches I can’t see it. * Documentation should go straight to main, there is no need to use branches at all, or if a branch is used it can be continuously merged to main (in case anyone else is writing documentation)\r\nBranches should be deleted as soon as possible, for the same reason, especially if they include commits that are not on main. If they are ahead of main that implies that they could have useful stuff on, or be a failed experiment that needs binning. If you’re off sick and we’re running your code we don’t want to be wondering which it is\r\nCollaboration\r\nThere should be one person in charge of each repo. That person is responsible for making sure that the main branch is clean, bug free, and properly documented. If a team member spots something on someone else’s repo that either doesn’t work or doesn’t look right they should either file an issue (if it’s something that might need discussing) or make a pull request (PR) - if it’s a simple fix to formatting or documentation - I send lots of PRs like this. It is the responsibility of the lead for the repo to check the PR and to merge it in in a timely fashion, request changes to it, or reject it.\r\nHaving a PR accepted is a privilege, not a right, so team members who want to improve a repo need to send a clean PR with a decent explanatory note in it. We might discuss over Slack or in person but in my opinion it’s better if that discussion happens on GitHub where possible because it means everyone, all team members and the open source community at large, can see it. I’m subscribed to all of the repos on our GitHub and I quite often spot areas for improvement or niggles by reading these discussions and it’s my job to understand the workflow and to suggest improvements.\r\nIssues\r\nBug reports and feature requests should all be filed as issues, again to increase transparency of ongoing work (within and without the team). Issues should be clearly worded with enough detail and should wherever possible include a reprex. We were bad with issues in the early days, jotting notes to each other that made a lot of sense at the time but within a month or so they were incomprehensible even to the author. Like code commenting and documentation a little work now can save a lot of confusion in the long run.\r\nReprexes\r\nI really can’t say enough about the reprex. The beauty of a reprex for me is that at least half the time making the reprex solves the problem. Boiling the problem down to its essential elements improves your understanding of the problem to such a degree that it either disappears completely or you can PR the bug fix yourself. And when it doesn’t it’s so valuable to have it as the person receiving the issue. If you don’t include a reprex, basically, don’t expect a fix, unless there’s a really good reason why you can’t include one.\r\nData security\r\nData security is obviously really important when you’re using GitHub, even if it’s a private repo. The simplest way to keep data security is don’t keep data in a git controlled folder. If you don’t do that, you will never accidentally push data to a repo. It’s impossible. A lot of our code just gets stuff straight from the server and processes it all in place, so there really is no need a lot of the time to save data anywhere.\r\nIf you absolutely must save data, for example if you have long running or complex data operations to complete, just make sure that you use .gitignore appropriately. My usual strategy is to have a folder called secret, and I add secret/ to .gitignore. Do that and check what you’re committing and pushing.\r\nIf the worst happens, and I should say the worst has never happened to us and I don’t think it ever will, you need to destroy all the commits with the data in.\r\nData security is a very important area and in my very inexpert opinion using GitHub doesn’t make data any less secure. Using GitHub means that you are always thinking about what’s in your data and it reduces sloppy practices like storing credentials in code (which you should NEVER do, even if you’re not using git- store them in environment variables. I read about so many information breaches from the NHS and they seem to be largely people emailing stuff about without concentrating. It’s complacency, they’re so used to everything being secure they forget what they’re doing.\r\nA note on packages\r\nThis is not strictly about GitHub, but it fits in the general area of collaboration and so I wanted to include it. We have adopted R packages almost wholesale in the team (another blog post that needs to be written), but there is one minor problem that they introduce that I wanted to flag up here.\r\nPeople get into the habit of building the package as they go, and then they use the built package for their analysis. This is fine as far as it goes, but you should never deploy or share anything based on code that you built yourself. Before you share or deploy you need to merge to main, switch to a new session and install the code from GitHub. This ensures that the code that you are running is the same code we all have access to. We’ve all spent far too long debugging code that only exists on one person’s laptop, and if you’re off sick and we redeploy your stuff you want to make sure we deploy what you actually want, because we can’t access the built stuff on your laptop.\r\nEdit\r\nTo add to this there is a specific post on package workflow for development which has much of what is written here and acts as a reminder of all the steps to cover.\r\nHelpful resources\r\nNHSX draft guidance\r\nGDS guidance\r\nHealth foundation guide to sharing code\r\nA collection of links I curate\r\nBest practice for managing credentials\r\nGitFlow\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-03-21-github-sop/preview_pic.jpg",
    "last_modified": "2023-09-01T10:10:40+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-04-open-source-within-the-NHS-benefits-and-possible-pitfalls/",
    "title": "Open source within the NHS - benefits and possible pitfalls",
    "description": "What does open source mean for the average staff member in the NHS? What benefits does it bring to patients?",
    "author": [
      {
        "name": "Oluwasegun Apejoye",
        "url": {
          "https://uk.linkedin.com/in/oluwasegun-michael-apejoye": {}
        }
      }
    ],
    "date": "2022-03-04",
    "categories": [
      "Open source"
    ],
    "contents": "\r\nIntroduction\r\nHealthcare needs to prioritise clinical safety and data security and\r\nthere have been concerns in the past about using and producing open\r\nsource within the NHS. In a simple form, working in the open means\r\nmaking the source codes, that powers our software or application, freely\r\navailable and downloadable from public platforms such as GitHub. This\r\nenables collaboration across the board to maximize benefits for the\r\ndelivery of health and social care services.\r\nThe term “open source” has been in use for a long time, and\r\nit is increasingly gaining traction in the NHS. And, we the clinical developmental\r\nunit – data science team at NottsHC are part\r\nof the open-source campaign within the NHS. We are dedicated to\r\nworking in the open and to using open source tools for our analysis.\r\nAs part of the effort to push for open source, the NSHX has announced\r\nthat NHS Open\r\nSource Policy will officially become available by this summer.\r\nAccording to the draft document, the policy “aims to provide a single\r\nposition and source of guidance for anyone developing open software for\r\nor with the NHS in England”. The announcement about the policy has been\r\nbroadly embraced by the NHS-R community (an open community of R\r\nusers).\r\nBut what does open source translate to for the average staff (with\r\nlimited technical skills) in the NHS? What benefit does this bring to\r\npatients?\r\nA member of our team already did some justice to this subject matter\r\nin her post working-in-the-open,\r\nwhere she shared some of her personal experiences and did some justice\r\nto the benefit of open source. However, with this post, I hope to share\r\nthe general benefit and touch on some more issues about working with\r\nopen source.\r\nBenefits\r\nGreater collaboration: open source enables technical (IT\r\nexperts, developers, analysts) and non-technical (clinicians, nurses,\r\nGPs) staff to input into projects and collaborate easily. It facilitates\r\neasy sharing of software, codes, analytical products etc, and it opens\r\ndoors for reproducibility. With open source, a small idea can easily\r\nbecome a proof of concept which can grow to become a project that ends\r\nup improving care delivery.\r\nCost saving: using an already published code to serve some\r\naspects of our task will lead to reduced duplication of effort, a\r\nreduction in staff time committed to projects and improved staff\r\nefficiency, a faster roll-out of cost-saving solutions and it enables\r\nteams to pursue the best approaches, not just those available locally.\r\nAlso, the awareness of public scrutiny will ensure teams make their\r\nprojects fit for purpose. In addition, embracing open-source tools means\r\nthe NHS will benefit from savings from licensing fees as the cost of\r\nopen-source tools tends to be lower and they also provide greater\r\ncommitment/contract flexibility compared to proprietary tools which\r\ncurrently lock-in the organisation to the supplier.\r\nAvailability of cutting-edge tools: All open-source projects\r\nbenefit from contributions from a wide variety of users. A good example\r\nis the Python and R programming languages, which are widely used tools\r\nfor a wide range of cool stuff. These two projects have benefited from\r\ncontributions that now make them a go-to tool for different tasks.\r\nCommunity effort: The open-source community runs on the\r\nphilosophy of universally shared knowledge because it benefits from a\r\nlarge and growing network of developers and users that are continually\r\ncontributing to projects. With an open-source approach to project\r\ndevelopment, teams from other organizations can take our project, make\r\nchanges to it and make the changes available to the community (including\r\nthe original author) to benefit from the additional feature or\r\nimprovement. This accessibility allows for constant developments and\r\nimprovements to the software.\r\nSafety and Security: Does open source mean we sacrifice\r\npatients’ privacy or lose control of projects? No. The organisation\r\nauthoring the project maintains control of the data at all times and\r\nensures it remains private and confidential, and can permit or restrict\r\nreuse of code using different software licences.\r\nAdditional income stream: The authoring team of an open\r\nsource project can offer to provide subscription services, support and\r\nmaintenance to interested users and therefore generate passive income\r\nfor their trust.\r\nWhat to look out for\r\nIt’s a no-brainer that embracing open source will bring several\r\nbenefits to teams within the healthcare system. However, there are still\r\nsome pitfalls to look out for. Below are some of the ones I have\r\nidentified:\r\nImportance of protecting data: Sharing open source in\r\nhealthcare settings requires safeguards to protect data, and unless\r\nsuitable synthetic data can be produced it can be hard to allow others\r\nto run code that depends on data.\r\nDocumentation and support: Sometimes documentation and\r\nsupport for open source is inferior to proprietary alternatives and this\r\ncan lead to delays and extra work for teams.\r\nLack of sustainability: it is possible to use packages (set\r\nof codes) developed by another team in your project. However, there is a\r\nrisk of the package being abandoned without prior notice by the team\r\nwhich can create problems with incompatibility and require rewriting of\r\nexisting code.\r\nTo mitigate the above issues, there is a need for robust policies to\r\nhelp analysts, developers and other users follow a consistent approach\r\nto open source. And I believe that is where the NHS Open Source\r\nPolicy proves useful.\r\nConclusion\r\nOpen source has many benefits to offer if embraced by all teams (both\r\ntechnical and non-technical) within the healthcare system. Despite these\r\nbenefits, there is a need for robust open source policies to ensure all\r\nteams work in a consistent way that will maximize the benefits for the\r\nquality of care delivered to patients, and the efficiency of the health\r\ncare systems without compromising patient safety and data\r\ngovernance.\r\nIf you want to read more about our open source journey, then keep in\r\ntouch by following us on Twitter and GitHub.\r\nFurther reading\r\nNHSX open source\r\npolicy\r\nNHS\r\ntechnology: Being open to open source\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-03-04-open-source-within-the-NHS-benefits-and-possible-pitfalls/img/image.jpg",
    "last_modified": "2023-09-01T10:10:40+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-17-package-workflow/",
    "title": "Package Workflow",
    "description": "From a team time session discussing the workflow to contributing to a \n(currently) private package",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Letxuga007": {}
        }
      }
    ],
    "date": "2021-12-17",
    "categories": [
      "Packages",
      "Workflow",
      "Teamwork"
    ],
    "contents": "\r\nUpdate\r\nThis post was edited in November 2022 with learnings that the team had since the post was originally written in 2021. Rather than writing a new post sections have been added with Edit.\r\nMany other posts have been added with more technical detail and are tagged in the blog with Github and Open source.\r\nTeam time\r\nOur team has regular team times but which don’t always focus on code. Quite often we use the opportunity to talk through processes and approaches to analysis and these notes reflect a recent code review where were discussed some of our learning on package development and workflow using GitHub for our (currently) private repository that takes data from the SQL data warehouses and makes it tidy following on from Milan Wiedemann’s nottshcverse.\r\nThese are some of the agreed points and discussions we had about some of the processes that have developed organically through working collaboratively. This process is very flexible and may change over time depending on our own developing skills or from others’ input (we are always keen to know how to improve our workflows!) so this blog may be changed or superseded in the future.\r\nCurrently we are using a basic workflow of:\r\nissue -> create a branch -> branch to development -> development to main -> package installed on people’s machines and on the R Studio Connect server\r\nEdit\r\nOnly one person should be responsible for the package and accepting pull requests. If several people can contribute it is now possible in GitHub to add specific branch protection rules (found in Settings in the repo) like requiring a pull request before merging. Always have a fresh look at the files changed in GitHub, even if you are working on the package alone. These can be found in the Pull request under Files changed:\r\nScreenshot of the tabs in a GitHub Pull request with Files changed (41) highlightedStart with an issue\r\nIssues are not just for bugs and should also be used for features and questions. The benefits of issues are that they are highly searchable and can show thinking on subjects from something that may be a question becoming a feature. If people are following the repository they will be notified of any issues. You can also @ specific people in the issue itself which is good if you are the owner of the issue but want input from someone.\r\nScreenshot of GitHub repository with the Watch button in the top right highlightedIssues often have one thing detailed but can have many parts to them in a task list:\r\nScreenshot of an issue with tasks listed, some tickedThe tasks can be ticked in the viewing mode without having to go to the code. To get the task list box type - [ ] and when it is ticked it becomes - [X]. Note that this is sensitive to spaces so any extra spaces will just show the code on the view mode.\r\nWe agreed in our team time that all issues, even questions, should be closed with a good summary of why it’s closed. For example, we had an issue discussing whether it could be a feature to combine functions that were reliant on each other to save typing them out in analysis. This led to a discussion which resulted in no changes to the code (i.e. no branches) but the issue was closed with a detail on why no work was started on it. This is useful for knowledge sharing with other colleagues, and maybe even the future you, if the suggestion were ever to come up again\r\nCreate a branch\r\nFrom an issue a branch will need to be created and however you do this, either through R Studio or the terminal, ensure that the name starts with the number from the issue. The following is an image from the terminal in R Studio which shows all the branches in the project from the command git branch.\r\n$ git branch\r\n  101-test-colm-names\r\n  101-test-column-names\r\n  194-restrict-rio-demog\r\n  232-calc-ips\r\n  233-update-readme\r\n  243-add-ulysses\r\n* 262-calc-team\r\n  change_calc_tot\r\n  connections\r\n  development\r\nThe active branch has a star next to it or appears in a different colour depending on the terminal and settings being used.\r\nOne of the downsides of not linking issues to branches is that any information on the work being done (difficulties or thoughts on the work) could be somewhere like a README but that is version controlled in code. Issues allow a form of version control but more in a sense of a conversation.\r\nEdit\r\nAfter working with GitHub for a while we’ve agreed to try to keep branches to a minimum as it can get very confusing having a lot of redundant branches.\r\nUpdating the branches\r\nSome work/features take a long time to complete, and in the meantime the development and/or main branches could have changed. This can be a good thing but can also mean that you may be repeating some work that has already been done, for example, formatting some code so that it is within 80 characters which is what our team has agreed to do for code scripts.\r\nTo update the local branch from another there are two git commands that can be used, either fetch or pull.\r\n$ git pull origin main does the fetch and merges. It only shows the changes if there is a conflict and is ok to use if you trust that the code works on the branch you are pulling from.\r\n$ git fetch origin main will not necessarily look like it’s done anything as the files will not be updated or changed, but it does update all the metadata on the changes. fetch is really good to use if you only want to take some of the changes, but not all.\r\nFurther information on how to use git fetch https://www.atlassian.com/git/tutorials/syncing/git-fetch.\r\nDevelopment branch and different versions\r\nWhen working collaboratively with someone on code it can be very easy to have a different version of the package so we’ve agreed to always build from the main branch. We do this directly from GitHub to ensure we are all building from the same code:\r\nEdit\r\n# Example code published on a README to help people load from main:\r\n# install.packages(\"remotes\")\r\nremotes::install_github(\"CDU-data-science-team/nottshcPackage\")\r\nHowever, when working locally from development or from another branch we suggest using {devtools} and loading the package to the session using Ctrl + Shift + l.\r\nIf the workflow was to always Build, Ctrl + Shift + b or from GitHub:\r\n# Example code published on a README to help people load from development:\r\n# install.packages(\"remotes\")\r\nremotes::install_github(\"CDU-data-science-team/nottshcPackage@development\")\r\nit may cause issues with analysis as it will be the package all sessions use. It very easy to build from an experimental branch but then forget, do some analysis using the package and not realise that the package is different to main, which in turn is different to the team’s version and the R Studio Connect version.\r\nWe discovered that, as it says above, development can move on rapidly from main so it’s good to ensure versioning is carried from development to main. This helps identify what version people are working from. As an example, main may be v0.15.0 in its stable form but development is v.0.17.0. Ideally the move to main will be as rapid as possible but can be disruptive to analytical end users so needs to be done with everyone’s knowledge. To do this we try to meet before any merge from development to main to go through the changes. This helps the end users of the package know what to expect and if anything has changed in the back end (like refactoring code) the end users are aware some changes have been made if things don’t work as expected.\r\nTalking through changes is also a really good sense check and that all documentation has been done.\r\nVersioning packages\r\nEdit\r\nUsing {usethis} for version and NEWS.md\r\nThe {usethis} package can be used for updating documentation on the version in the DESCRIPTION.R file and the NEWS.md file. {usethis} can be used to set up a new NEWS.md file:\r\nusethis::use_news_md()\r\n\r\nand this is a great place to write about the package changes. These are now mostly about end user function additions or bugs, rather than internal changes like refactoring as the latter can cause confusion to users, particularly when the effects are minimal to their analysis.\r\nVersioning is best used on development and the types used are: major, minor, patch and dev. For the most part the team’s packages have had minor versions with a few patches.\r\nGitHub releases\r\nOnce on main it is good practice to use the Releases in GitHub (on the right hand side of the main page) which often have the same information in them as the NEWS.md but is a good way to bundle up all the files together for any bug fixes. The team needed this once where a bug had been introduced into main but had been working on a previous version - however, a few versions back. Being able to reinstall the more stable, much older, version helped with analysis that was required urgently.\r\nTagging commits in git\r\nIn team time we agreed that releases should only be on main and to keep a track on version in development we will instead use tags through Git.\r\nWe did a test tag in the Terminal: git tag -a v0.9.1 -m \"Testing (and there is a still a bug)\" and checked in R Studio, in the Commit History, to see if the tag was listed as this will be a good way to check people’s versions of the package when working collaboratively.\r\nFurther information on tagging commits in git: https://www.atlassian.com/git/tutorials/inspecting-a-repository/git-tag\r\nFinding tagged commits\r\nTo check for tags, in R Studio, in the Git panel (top right as default) and the history icon of a clock.\r\nScreenshot of the top right panel icons with the clock highlighted for HistoryIt’s also possible to get to the same screen by selecting the Commit button and then the history tab.\r\nScreenshot of the Commit window with the tab history hightlightedIn the commit history pane the tagged commit has a slightly different colour as well as being its own commit.\r\nScreenshot with the tag version highlightedEdit\r\nThe team hasn’t needed to use a Git tag yet but it’s a good “bookmark” within the commit history to show when certain changes have been made.\r\nWords of caution with commits\r\nWhy you should write informative commits\r\nAs we were discussing how good messages in commits can help located older versions of code, we realised that the link to the issue or the branch is lost from the commits once merged to another branch. Consequently, commits like “Fixed” won’t necessarily make sense when removed from the context like it being on a branch called 207-code10-error. There is no reference in “Fixed” as to what was changed or what the error/problem was. It’s therefore advisable to get into the habit of writing informative commits as if you were writing them to someone else even if, at the moment, you may be the only one working on the project.\r\nWhen to commit\r\nWe had a divergence in the team about whether to commit chunks of working code or smaller steps. This was really down to personal preference but which were both based in differences in approaches to locating historic code changes. The ‘lots of commits’ approach can be a lot of text to go through to find when the code last ‘worked’. However, many individual commits for particular actions can make it easier to pull out one commit that needs removing rather than re-work whole chunks.\r\nOne thing we did all agree on was that the first commit is usually a large chunk of code and is often committed as --First commit or something similar. We also all had used the Amend previous commit in R Studio.\r\nScreenshot of the commit pan from R Studio with the Amend Previous Commit highlightedTicking this button takes you to the last commit which can be changed and it also means that you can add new files or changed files to the commit that you may have missed. Changing the last commit doesn’t work so well though if the commits have already been pushed as that will put the two last commits out of synch and that requires it’s own blog in how to rectify!\r\nEdit\r\nGitHub now offers several options for merges: merge, rebase or squash and so if your practice is to have many commits (also known as atomic commits) on a feature branch when this is merged to development you can use squash to reduce it to once commit. Just be careful of these options between development and main though as a squash to main will change the commit history and break the connection between the two branches.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-17-package-workflow/img/system-of-mineralogy-comprehending-oryctognosie-p687.jpg",
    "last_modified": "2023-09-01T10:10:40+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-18-personal-access-tokens/",
    "title": "Personal Access Tokens",
    "description": "Connecting to the GitHub to install packages from a private GitHub repository requires security \nset ups and this blog details how to do it (and how not to do it).",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Letxuga007": {}
        }
      }
    ],
    "date": "2021-12-17",
    "categories": [
      "Packages",
      "Security",
      "GitHub",
      "Open source"
    ],
    "contents": "\r\nInstallation\r\nBecause some of the packages we are using are (currently) private repositories it is not possible to build using just {remotes} as a Personal Access Token, specifically a GITHUB_PAT, is needed. The {usethis} package suggests using the local Git credential store but this wasn’t compatible with some of our team’s set ups so later in this blog are details on how to do it the “inadvisable” way to be used at your own discretion.\r\nCheck you have a token\r\nThis part is the same for whatever method you use on your own computer as this section details how to get the GITHUB_PAT set up. To check for existing GITHUB_PAT use code:\r\nSys.getenv(\"GITHUB_PAT\")\r\nIf nothing has been set up by you on GitHub this may return an empty string ““.\r\nSet up a token\r\nFirstly, go to https://github.com/settings/tokens which takes you to your own personal GitHub account settings where you will need to Generate new token. Give this a suitable name like Repo access and tick the repo group for full control of private repositories.\r\nRemember to copy the code that is generated as this cannot be viewed again\r\nFrom the Git credential store\r\nPackages required\r\n\r\n\r\ninstall.packages(\"usethis\")\r\ninstall.packages(\"gitcreds\")\r\ninstall.packages(\"remotes\")\r\n\r\n\r\n{usethis} suggests using the local Git credential store quite strongly!\r\n\r\nIf you have previously set your GitHub PAT in .Renviron, stop doing that.\r\n\r\nSet up\r\n\r\n\r\n\r\nGitHub will open and, if you are not already logged in, you will need to enter your (GitHub) password.\r\nThe page for GitHub tokens management is https://github.com/settings/tokens and you can do the same as the code\r\nin this screen by selecting Generate new token. Give this a suitable name like Package installation and tick the repo group for full control of private repositories.\r\nRemember to copy the code that is generated as this cannot be viewed again\r\nNext set up a GITHUB_PAT in RStudio:\r\ngitcreds::gitcreds_set()\r\nYou will then be prompted to enter the copied token (no quotes are required). If you’ve already got a token entered the following message will show:\r\n-> Your current credentials for 'https://github.com':\r\n\r\n  protocol: https\r\n  host    : github.com\r\n  username: PersonalAccessToken\r\n  password: <-- hidden -->\r\n\r\n-> What would you like to do? \r\n\r\n1: Keep these credentials\r\n2: Replace these credentials\r\n3: See the password / token\r\n\r\nSelection: ```\r\nEnter an item from the menu, or 0 to exit\r\nSelection: \r\nReference: https://usethis.r-lib.org/articles/articles/git-credentials.html\r\nInstalling\r\nThe {usethis} documentation suggests using {pak} but this may not work on network drives/VPNs (there are a few issues that have been opened and quickly closed referring to these as potential problems) and so in the meantime\r\ncombining {remotes} with {gitcreds} is a workaround:\r\n# install.packages(\"remotes\")\r\nremotes::install_github(\"<name of GitHub account>/<name of repository>\", \r\n    auth_token = gitcreds::gitcreds_get(use_cache = FALSE)$password)\r\nThe absolutely not recommended method\r\nPackages required\r\n\r\n\r\ninstall.packages(\"usethis\")\r\ninstall.packages(\"remotes\")\r\n\r\n\r\nIf you are sure this is what you want to do then type in RStudio:\r\nusethis::edit_r_environ()\r\nIn the file that opens, type into it (where YOUR-PAT is the generated code you’ve copied) and save the file:\r\nGITHUB_PAT=YOUR-PAT\r\nRestart R and run Sys.getenv(\"GITHUB_PAT\") to check that the \"\" has changed.\r\nReference: https://www.jumpingrivers.com/t/2019-user-git/03-githubpat.html#6\r\nInstalling\r\nNow, for as long as the token exists, the following code will install the package:\r\n# install.packages(\"remotes\")\r\nremotes::install_github(\"<name of GitHub account>/<name of repository>\", auth_token = Sys.getenv(\"GITHUB_PAT\"))\r\nBuilding package\r\nIt is preferable to build from the repository rather than locally as this should be what everyone in the team has access to. To build and test any new functions use Ctrl+Shift+L from the {devtools} package to load locally for the session.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-18-personal-access-tokens/img/british-mineralogy-or-coloured-figures-p132.jpg",
    "last_modified": "2023-09-01T10:10:40+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-06-nottshcverse/",
    "title": "Development of open tools for analysing healthcare data in R",
    "description": "***TLTR: (Too Long To Read)*** \nOur goal was to make it easier to work with healthcare data in a reproducible and collaborative way.\nWe wrote lots of R functions for recurring data manipulations and analytical tasks that magically translate into SQL code and communicate with large databases.\nAll our functions are grouped into R packages because this made it easier for us to: *(i)* write good documentation of our code and analytical tasks, *(ii)* easily distribute updates across all team members, *(iii)* formally test our code, and *(iv)* integrate common data manipulations (or analyses) into interactive dashboards in a modular way.",
    "author": [
      {
        "name": "Milan Wiedemann",
        "url": {}
      }
    ],
    "date": "2021-09-29",
    "categories": [
      "Open source",
      "Teamwork",
      "Packages"
    ],
    "contents": "\r\n\r\nContents\r\nChallenges and\r\nsolutions\r\nOverview of the\r\n{nottshcverse}\r\nSimplified working example\r\nCreate example data\r\nCreate SQLite\r\nconnection\r\nExample analysis\r\nWriting a function\r\nExample SQL code\r\nExample results\r\n\r\nWatch out now\r\nSummary\r\nRelated work and resources\r\n\r\n\r\n\r\n\r\nOur team works with routinely collected NHS patient data. Currently\r\nwe focus on understanding better how patients are using the service,\r\nchanges in clinical outcome measures, and analysis of patient\r\nexperiences. The main questions that guide our work are ‘What works\r\nfor whom and how does it work?’, ‘What doesn’t work?’, and\r\n‘How can we integrate patient experiences into our analyses?’.\r\nWe developed a set of different tools, the\r\n{nottshcverse}, to help us look at these\r\nquestions by automating recurring and time-consuming tasks so that we\r\ncan spend more time thinking the clinical questions.\r\nChallenges and solutions\r\nReal data is messy but we should do our best to tidy the mess, where\r\npossible in an automated way. There are many challenges when working\r\nwith healthcare data, here is only a small selection of those that I\r\nthink underlies most analyses. Figure\r\n1 shows our solutions to these\r\nchallenges.\r\n\r\n\r\n\r\nFigure 1: Main goals that guided the\r\ndevelopment of the {nottshcverse} packages.\r\n\r\n\r\n\r\n🤔 It’s really hard to get the clinical data that is needed in a\r\nreproducible way that is consistent across different people who work on\r\nthe same (or related) analysis. This is particularly true because most\r\nof the data is stored on SQL servers and only starts to make sense after\r\njoining multiple different datasets. 🤓 We wrote functions that\r\nmade it very easy and secure to (i) connect to and\r\n(ii) query data from different databases that we are working\r\nwith. This way we could get the data we needed for our analyses using\r\nvery few lines of code in R.\r\n\r\n\r\n# First, create connection to databases\r\nconn_s1 <- connect_sql(server = \"DB-one\")\r\nconn_iapt <- connect_sql(server = \"DB-two\")\r\n\r\n# Now we can use the connection to get contacts data from SystmOne ...\r\ndb_contacts_s1 <- get_s1_contacts(from = \"2020-01-01\", \r\n                                  to = \"2020-12-31\", \r\n                                  conn = conn_s1)\r\n\r\n# ... and IAPTus databases\r\ndb_contacts_iapt <- get_iapt_contacts(from = \"2020-01-01\", \r\n                                      to = \"2020-12-31\", \r\n                                      conn = conn_iapt)\r\n\r\n# Note that the objects 'contacts_s1' and 'contacts_iapt' are just pointing to \r\n# the databases (I like to use the prefix db) and not actually downloaded to the\r\n# environment on your computer.\r\n\r\n\r\n\r\n🤔 Most of the time, the data is not in the format that is needed\r\nfor further analyses. There may be specific data manipulations that are\r\nneeded to make sense of the data and analyse it properly. Also,\r\ndifferent databases might be set up in ways that it is hard to merge\r\ndata. 🤓 We wrote function that tidy the raw data from the\r\ndatabases so that it is more consistent across databases and easier to\r\nanalyse. All of this still happens within the server so that our\r\ncomputers don’t have to do all of this work.\r\n\r\n\r\n# Each get_*_data() function comes with a tidy_*_data(), here tidy_s1_contacts()\r\n# Here I use the connection to the raw (messy) contacts  data that I created above\r\n# and tidy it using the tidy_s1_contacts() function\r\ndb_contacts_s1 <- db_contacts_s1 %>% \r\n  tidy_s1_contacts()\r\n\r\ndb_contacts_iapt <- db_contacts_iapt %>% \r\n  tidy_iapt_contacts()\r\n\r\n\r\n\r\n🤔 The methods of data analyses and visualisations should be\r\nunderstandable, reproducible, and available to other people.\r\nUnfortunately this is not always the case yet because the software tools\r\nthat are used are not script based and often shared in private emails.\r\n🤓 We wrote R functions for common analytical tasks and\r\nvisualisations.\r\n🤔 Code should be really well documented so that it is easy to\r\nunderstand what’s going on. This includes the current version of the\r\ncode as well as all previous versions and changes. This can be done\r\nusing tools like Git and GitHub, but unfortunately most code is\r\ncurrently shared undocumented in private emails. 🤓 We created\r\ndetailed documentations that are easily accessible to everyone who uses\r\nour R packages. Also, because develop our tools on GitHub, every change\r\nto our code is documented.\r\n🤔 Mistakes happen! Sometimes things that you have no control\r\nover can change and break your code that previously worked fine (e.g.,\r\nthe format of the raw data or a functions that someone else wrote).\r\nTherefore, it’s important to continuously test whether the code is still\r\nworking the way it’s supposed to work. 🤓 R packages (or similar\r\nsolutions in other statistical programming languages) are relatively\r\neasy to test, for example using the {testthat} package. We started to\r\nimplement tests into our work so that we can check if changes that we\r\nmake to our code don’t break anything.\r\nOverview of the\r\n{nottshcverse}\r\n\r\n\r\n\r\nFigure 2: Overview of some R packages\r\ndeveloped by the Clinical Development Unit Data Science Team⭐and the\r\nNHS-R Community ❤️\r\n\r\n\r\n\r\n{nottshcData}: Unified framework to query, transform,\r\nand aggregate data from different databases\r\n{nottshcMethods}: Tools for performing common\r\nanalytical tasks (e.g., grouping continuous age into groups)\r\n{honos}, {LSOApop}: Packages designed in\r\ngeneric way to help use and others {nottshcData} work with\r\nspecific questionnaires (e.g., Health of the Nation Outcome Scales,\r\nHoNOS) or open data sets (e.g. LSOA population estimates)\r\n{outcomesdashboard}: Our dashboards use all the\r\npackages mentioned above + special packages developed specifically to\r\nsupport the dashboards with helper functions\r\nSimplified working example\r\nTo illustrate how R can be used to work with databases I’ll use the\r\nfollowing example. Imagine we’re working with a database called\r\nSystmTwo (S2) and need to use two different\r\ntables for our analysis:\r\n[S2].[contacts]: Information about contacts with\r\nclinical teams\r\n[S2].[demographics]: Some demographic information\r\n\r\n\r\n\r\nCreate example data\r\n\r\n\r\n# Set up example contacts table\r\ncontacts_s2 <- tibble(client_id = c(1, 1, 1, 2), \r\n                      contact_id = c(123, 124, 125, 156), \r\n                      referral_id = c(456, 459, 500, 501), \r\n                      referral_date = c(\"2018-04-19\", \"2019-05-23\", \r\n                                        \"2020-06-01\", \"2018-12-11\"),\r\n                      contact_date = c(\"2018-05-19\", \"2019-06-05\", \r\n                                       \"2020-07-08\", \"2019-01-15\"),\r\n                      team_id = c(\"tm1\", \"tm2\", \"tm1\", \"tm1\"), \r\n                      hcp_id = c(\"hcp1\", \"hcp2\", \"hcp1\", \"hcp1\"), \r\n                      contact_type = c(\"phone\", \"f2f\", \"video\", \"phone\"),\r\n                      assessment_id = c(321, 322, 344, NA))\r\n\r\n# Set up example demographics table with 2 patients\r\ndemographics_s2 <- tibble(client_id = c(1, 2), \r\n                          dob = c(\"1988-01-01\", \"1965-01-01\"),\r\n                          dod = c(NA, NA),\r\n                          sex = c(\"f\", \"m\"))\r\n\r\n\r\n\r\nCreate SQLite\r\nconnection\r\n\r\n\r\n# Create connection (conn) to \"local\" database called SystmTwo (s2)\r\nconn_s2 <- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\r\n\r\n# Copy local data frame to conn_s2 database\r\ndb_s2_contacts <- copy_to(conn_s2, contacts_s2)\r\ndb_s2_demographics <- copy_to(conn_s2, demographics_s2)\r\n\r\n\r\n\r\nExample analysis\r\nHere we join the contacts with the demographics information to\r\ncalculate the age at the time a patient has their contact\r\n(age_at_contact).\r\n\r\n\r\n# Calculate age at time of contact\r\ndb_age_at_contacts <- db_s2_contacts %>% \r\n  left_join(db_s2_demographics) %>% \r\n  mutate(age_at_contact = as.Date(contact_date) - as.Date(dob))\r\n\r\n\r\n\r\nWriting a function\r\nWe can also write our own functions and use them in a modular way\r\nwhenever we need them. Here’s a simple example to demonstrate how we can\r\ndo the same calculation as shown above using our own function. In this\r\nexample the function arguments take the variable names for date of birth\r\n(dob) and the contact date (contact_date).\r\n\r\n\r\ncalc_age_at_contact <- function(data, var_dob, var_contact_date) {\r\n  # Add code here to check that arguments are specified correctly\r\n  data %>% \r\n    dplyr::mutate(age_at_contact = as.Date({{var_contact_date}}) - as.Date({{var_dob}}))\r\n  }\r\n\r\n\r\n\r\nExample SQL code\r\nAs mentioned above (Challenges and solutions, Point 1), the object\r\nthat we work with most of the time are just SQL queries and not real\r\ndata stored in your R environment. We can look at the underlying SQL\r\ncode using the dplyr::show_query() function. I don’t really\r\nknow SQL very well myself, but some people who do have created a great\r\npackage that translates R code into SQL code (see the dbplyr\r\npackage for more).\r\n\r\n\r\n# Use dplyr::show_query() function to see underlying SQL code\r\nshow_query(db_age_at_contacts)\r\n\r\n\r\n<SQL>\r\nSELECT `client_id`, `contact_id`, `referral_id`, `referral_date`, `contact_date`, `team_id`, `hcp_id`, `contact_type`, `assessment_id`, `dob`, `dod`, `sex`, CAST(`contact_date` AS DATE) - CAST(`dob` AS DATE) AS `age_at_contact`\r\nFROM (SELECT `LHS`.`client_id` AS `client_id`, `contact_id`, `referral_id`, `referral_date`, `contact_date`, `team_id`, `hcp_id`, `contact_type`, `assessment_id`, `dob`, `dod`, `sex`\r\nFROM `contacts_s2` AS `LHS`\r\nLEFT JOIN `demographics_s2` AS `RHS`\r\nON (`LHS`.`client_id` = `RHS`.`client_id`)\r\n)\r\n\r\nNote that we can also see the SQL code from our own functions.\r\n\r\n\r\ndb_age_at_contacts %>% \r\n  calc_age_at_contact(var_dob = dob, \r\n                      var_contact_date = contact_date) %>% \r\n  show_query()\r\n\r\n\r\n<SQL>\r\nSELECT `client_id`, `contact_id`, `referral_id`, `referral_date`, `contact_date`, `team_id`, `hcp_id`, `contact_type`, `assessment_id`, `dob`, `dod`, `sex`, CAST(`contact_date` AS DATE) - CAST(`dob` AS DATE) AS `age_at_contact`\r\nFROM (SELECT `client_id`, `contact_id`, `referral_id`, `referral_date`, `contact_date`, `team_id`, `hcp_id`, `contact_type`, `assessment_id`, `dob`, `dod`, `sex`, CAST(`contact_date` AS DATE) - CAST(`dob` AS DATE) AS `age_at_contact`\r\nFROM (SELECT `LHS`.`client_id` AS `client_id`, `contact_id`, `referral_id`, `referral_date`, `contact_date`, `team_id`, `hcp_id`, `contact_type`, `assessment_id`, `dob`, `dod`, `sex`\r\nFROM `contacts_s2` AS `LHS`\r\nLEFT JOIN `demographics_s2` AS `RHS`\r\nON (`LHS`.`client_id` = `RHS`.`client_id`)\r\n))\r\n\r\nExample results\r\n\r\n\r\n# Look at results from SQL query shown above\r\ndb_age_at_contacts %>% \r\n  select(client_id, contact_date, dob, age_at_contact)\r\n\r\n\r\n# Source:   lazy query [?? x 4]\r\n# Database: sqlite 3.38.0 [:memory:]\r\n  client_id contact_date dob        age_at_contact\r\n      <dbl> <chr>        <chr>               <int>\r\n1         1 2018-05-19   1988-01-01             30\r\n2         1 2019-06-05   1988-01-01             31\r\n3         1 2020-07-08   1988-01-01             32\r\n4         2 2019-01-15   1965-01-01             54\r\n\r\nOf course this is a VERY simple example. This can get way more\r\ncomplex, think BIG and solve BIG problems. There are many other examples\r\nout there showing how to work with databases in RStudio. I added some\r\nlinks that I found useful at the end of this post.\r\nWatch out now\r\nSo what’s coming next and where can we take this? Is this perfect? I\r\ndon’t know exactly what’s coming next and this is definitely far from\r\nperfect. But it’s the best approach my colleagues and I could come up\r\nwith in the time that we spent working on this. Maybe I’ll improve this\r\none day, maybe someone else will? Until then let’s share ideas and work\r\ntogether to improve healthcare analytics in the NHS. Ohhh, some people\r\nare already working like this 👀 it’s time others join them. I hope\r\nthose who make decisions about the direction of healthcare analytics in\r\nthe NHS will start to understand the problems and opportunities and act\r\nsoon. If not now, when then? We need to move towards a more open and\r\nmodern way of healthcare analytics!\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nSummary\r\nHere’s a short summary in BOLD AND ALL CAPS:\r\nDOCUMENT everything, absolutely everything! Every\r\nfunction and every single change!\r\nAUTOMATE common analytical tasks! Write functions\r\nand packages!\r\nTEST everything! Expect mistakes, there will be\r\n🐛🐛🐛\r\nSHARE as much as we can!\r\nOf course this doesn’t always work. There will always be some messy\r\ndata, inconsistent variable names, undocumented code, and … blah blah\r\nblah.\r\nRelated work and resources\r\n📖 Chris\r\nMainey (2019). SQL Server Database connections in R.\r\n📖 Emily Riederer\r\n(2021). Workflows for querying databases via R - Tricks for modularizing\r\nand refactoring your projects SQL/R interface.\r\n📖 Hadley Wickham,\r\nMaximilian Girlich and Edgar Ruiz (2021). dbplyr: A ‘dplyr’ Back End for\r\nDatabases.\r\n📖 RStudio (2021). Databases using\r\nR from RStudio.\r\n📖 RStudio (2021).\r\nUsing an ODBC driver\r\n📷 Edgar\r\nRuiz (2018). Best practices for working with databases.\r\n🧙 Chris Beeley (2011 ’Til\r\nInfinity). Random bits of related and unrelated statistics, programming,\r\nand healthcare wizardry.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-08-06-nottshcverse/img/Principes-de-Logique-p435.jpg",
    "last_modified": "2023-09-01T10:10:39+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-14-text-mining-pipeline/",
    "title": "A Text Mining Pipeline for NHS Patient Experience Feedback",
    "description": "This blog post is a more technical description of the pipeline that we have built to analyse patient feedback text data from the NHS.",
    "author": [
      {
        "name": "Andreas Soteriades",
        "url": {}
      }
    ],
    "date": "2021-09-14",
    "categories": [
      "Text Mining",
      "Patient Experience"
    ],
    "contents": "\r\nThe pipeline consists of two distinct modules:\r\nA text classification pipeline for classifying patient feedback text into themes like Communication, Environment/facilities, Staff, etc.\r\nA text mining dashboard reporting results from text classification, sentiment analysis, and analysis of word frequencies to surface information about what patients most talk about, what frustrates them, what they most like in the service etc.\r\nFor the scope and more high-level descriptions of the different analyses carried out see this blog post and project description.\r\nIn terms of the nitty gritty, we are particularly excited about having combined super-cool packages in R and Python to build the pipeline! Two highlights of our work are the use of what we consider to be game-changer R packages:\r\n{golem}- “[…] an opinionated framework for building production-grade shiny applications”. Package {golem} automatically provides the structure for the {shiny} skeleton (app & ui) and makes it very easy to build Shiny apps that are modular, strict as to where the business logic goes, documented, tested, shareable, and agnostic to deployment.\r\n{reticulate}- an R interface to Python that opened up for us unique opportunities for using state-of-the-art Python packages for text classification and sentiment analysis directly in R.\r\nFor more details, refer to this presentation where I describe both packages in much enthusiasm!\r\nPipeline overview\r\nLet’s take a look at the whole pipeline:\r\n\r\nThe pipeline consists of an ecosystem of tailor-made packages in R ({experienceAnalysis}, {pxtextmineR}, {pxtextminingdashboard}) and Python (pxtextmining) that we designed to be both fit-for-purpose, but also as generic as possible for use by other NHS trusts or by anyone in general. Let’s break down the pipeline into smaller steps:\r\nThe pipeline reads the text data and sends it to the text classification pipeline, as well as to the dashboard.\r\nThe text classification pipeline uses Scikit-learn- fuelled pxtextmining to tune and train a Machine Learning model. It then writes the results (predictions, performance metrics, classifier performance bar plots, a SAV with the trained text classification pipeline etc.). These are then passed into dedicated modules in the {golem} dashboard that present predictions on unlabelled feedback, as well as tables and plots with performance metrics.\r\nMeanwhile, the text data is also passed into the dashboard for sentiment analysis and other text mining (e.g. TF-IDFs). The dashboard has dedicated modules that use our external packages to perform these analyses. In particular, {experienceAnalysis} makes extensive use of {tidytext}, although it offers functions that conveniently perform automatically a few data preprocessing and manipulation steps that would otherwise need to be done manually before passing them to the {tidytext} functions. On the other hand, {pxtextmineR} has {reticulate}- fuelled functions for doing sentiment analysis with Python packages TextBlob and vaderSentiment.\r\nInternally, the {golem} dashboard also has a series of R scripts containing simple utility functions that are useful for running small tasks (e.g.  sort a character vector) that would otherwise be run inside the modules themselves.\r\nNote that using external packages and utility functions to prepare the data keeps the modules clean from any business logic that would make the dashboard too specific to the dataset used. This is a key advantage of {golem}: we can use the dashboard as a framework for reporting results on any dataset that we would like to use! A simple example is the following: say we want to report averages for a number of categories. This could be mean sepal length for each plant species in the iris data, mean flipper length for each penguin species in the penguin data, and mean miles per gallon for each car engine type in the mtcars data. We can build a {golem} that produces the mean of a variable according to different categories and then pass either iris, penguin or mtcars to get a dashboard for each of these datasets.\r\nAnd this is exactly where the YAML file in {golem} comes in handy. The YAML file acts as a control panel where the user specifies what dataset and which columns from the dataset to use in the business logic. In our case, this means that we can produce a dashboard for our own data or for the patient feedback data of any NHS trust! As deployment with {golem} is pretty straightforward, we are able to host several dashboards on the server, each of which uses a dataset from a different NHS trust.\r\nAmazingly, {golem} ships the whole dashboard as an R package! We call our packaged dashboard pxtextminingdashboard. This package contains open patient experience data in RDA format that can be used to run the app. All you need to do is install pxtextminingdashboard, load it in R and run run_app. The dashboard is also available here.\r\nConclusion\r\nWe have built a truly revolutionary text mining pipeline that can be used for free by any NHS trust and will hopefully help surface business-critical information to guide improvements in healthcare services. We believe that the pipeline is a great example of how one can make the best of both R and Python. Both {golem} and {reticulate} are game-changers- try them out!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-09-14-text-mining-pipeline/img/a-treatise-on-map-projections-p88.jpg",
    "last_modified": "2023-09-01T10:10:39+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-30-pair-programming-code-review-journal-club-and-team-time/",
    "title": "Pair programming, code review, journal club, and team time",
    "description": "What can the CDU data science team do to verify its outputs, disseminate learning, and support individual development in team sessions?",
    "author": [
      {
        "name": "Chris Beeley",
        "url": {}
      }
    ],
    "date": "2021-07-30",
    "categories": [
      "Teamwork"
    ],
    "contents": "\r\nWe’ve been a proper data science team for a year now, there’s six\r\nof us at the moment which is GREAT and we’re starting to review some\r\nof the stuff that we’ve been doing to make sure that it’s serving a\r\npurpose. I’m sort of writing this for our benefit just as a way of\r\nrecording what the plan is (and to discuss the plan via pull request 😎)\r\nbut we work in the open so why not talk about this in the open too.\r\nFairly near the beginning of become A Proper Data Science Team we\r\nkicked off “Code review” sessions which would take place fortnightly and\r\nteam members would bring stuff, taking it in turns to do one each. I\r\nhardly think I can improve on Google’s\r\nguide to code review but I’ll summarise the main points here for\r\nthose who don’t click through. Code review is the process of one or more\r\npeople who have not written the code in question to read it and check\r\nthat it’s okay, before the code is merged into the codebase. Code\r\nreviewers are considering:\r\nDesign\r\nFunctionality\r\nComplexity\r\nTests\r\nGood naming practices (variables, functions)\r\nComments\r\nStyle (judged against a style guide like the tidyverse style guide)\r\nConsistency\r\nDocumentation\r\nIn the linked document Google emphasise that you should read every\r\nline- not scan over things and assume they’re okay. It’s worth\r\nremembering as well that we are doing data science- so we need to be\r\nreviewing statistical and ML methods as well. People (including me) fall\r\ninto the trap of thinking that data science is just programming and they\r\nstop asking themselves hard questions about the methods they are using\r\nand that is very dangerous.\r\nI put code review in quotes advisedly because although we went into\r\nthem thinking we would be doing code review over the weeks and months we\r\nended up doing something totally different. Sometimes we would pretty\r\nmuch do code review. Sometimes we would spend a long time discussing\r\ncode style. Other times we would start off doing code review and end up\r\ndoing an ad hoc lesson in a particular method that the person who came\r\nto code review. Sometimes we would have such a good time doing one of\r\nthese things that we couldn’t fit all the excitement into an hour and\r\nwould call extraordinary code review meetings so we could carry on\r\ndiscussing it and not have to wait two weeks.\r\nIt was pretty anarchic but we were all learning and having fun in a\r\nsupportive environment so I thought we may as well just see where we\r\nended up. After a year I thought it was time to review what we were\r\ndoing around assuring our code so I kicked off a discussion about it\r\nwhich this blog post is part of.\r\nWe have had a pretty wide ranging discussion about it and the first\r\npoint of interest is that everybody wanted to keep the fortnightly\r\nsessions but everybody agreed that, however awesome they were, they\r\nweren’t really “code review”. We boiled down what we feel we need to\r\nfour distinct activities. For the sake of brevity I will summarise them\r\nhere- I could probably do a blog post on each and if anybody would like\r\nto hear more then find me on Twitter, have a look at the about page.\r\nPair programming\r\nPair programming is what it sounds like- programming in pairs. It’s a\r\ngreat way of teaching and helping each other, and solving problems\r\ntogether, but it’s also a way of reviewing code too. The maxim we came\r\nup with today is simple.\r\n\r\nEvery line of code should have been read by two people\r\n\r\nYou can do that in a pair, live, or you can do it by review (or you\r\ncan do both). Interestingly enough the team just spontaneously started\r\ndoing pair programming. Nobody mentioned the word, nobody asked them to.\r\nIt just makes sense to do that so they just started doing it, which I\r\nabsolutely love.\r\nCode review\r\nThis is the other review methodology that came out of the discussion.\r\nI already talked about what Google think this should be so there’s not\r\nmuch to add. One thing we said is that the other thing we need to change\r\nis doing it fortnightly as a group. Proper code review can only be done\r\nby someone who understands the code. We have a LOT of skills in the team\r\n(SQL, R, Python, Shiny, statistics, ML) and nobody understands it all\r\n(especially me). We decided that this would happen when people feel\r\nthey’re ready and would take place with one or more designated people\r\nwho understand the code, and not every fortnight with everybody.\r\nOne interesting thing that came out of this discussion was my\r\nrealisation that we need to deepen and broaden the skills of the team.\r\nSeveral of us are writing code that nobody has the expertise to review.\r\nI’ve long been obsessed with truck factor but I\r\nhadn’t considered it from the position of validating code before. It’s\r\nentirely my failing as a manager and it’s something else to think about\r\nwhen we’re recruiting next (it’s also relevant for training and\r\ndevelopment, but that’s another post).\r\nJournal club\r\nSomething else that we spontaneously did as part of the code reviews\r\nthat weren’t really code reviews was what we’re loosely calling “journal\r\nclub”- basically one of “I know something that you need to know, I’m\r\ngoing to teach you” or “we all need to get better at x- let’s learn\r\ntogether”. We’re going to start with me: “Everything your data\r\nscientists wanted to know about managing servers and deploying in the\r\ncloud- but were afraid to ask” which will end up on GitHub\r\nsomewhere.\r\nTeam time\r\nNot sure about the name for this one! The last thing that everybody\r\nseemed to want was something else we were already doing. It’s basically\r\njust the ability to just bring anything you like and talk about it.\r\nWe’ve had all sorts of things. “This code is fine, it’s just horribly\r\nslow”, “I can’t figure out what is going to give the best experience for\r\nour users”, “I’ve done this analysis but it’s kind of shallow- what else\r\ncan I do?”. I don’t think this is anything particular, it doesn’t have a\r\nproper name, it isn’t particularly for one purpose, it’s more just the\r\nteam’s way of saying “We help and support each other and we carve out\r\none hour every two weeks so team members can bring a problem and talk it\r\nthrough with us”.\r\nWrap up\r\nThat’s where we’re at now, once we’ve agreed this as the way forward\r\n(or agreed something else, obviously) then we’ll give it a go. As is\r\nprobably clear, our Friday sessions and code review are just one part of\r\ntrying to have a team that works well together and I’m totally new at\r\nthis so if anybody out there has anything to add I’d be super grateful\r\nto hear it.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-07-30-pair-programming-code-review-journal-club-and-team-time/img/the-geographical-institutions-p84.jpg",
    "last_modified": "2023-09-01T10:10:39+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-22-age-bands-methodology/",
    "title": "Age bands methodology",
    "description": "A blog compiling all the age bands methodology that can be used for comparing analysis populations against.",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Lextuga007": {}
        }
      }
    ],
    "date": "2021-06-22",
    "categories": [
      "Resources"
    ],
    "contents": "\r\nAge Bands\r\nOn the face of it applying age bands to data analysis is simple, you need to consider the start and end ages and group up the rest into reasonable groups. However, if you want to use the data to compare to other sources of aggregate data it needs to be grouped in a similar way.\r\nFriends and Family questionnaires\r\nTrusts can control some of the data they collect for the Friends and Family questionnaires and Nottinghamshire Healthcare NHS Foundation Trust have used a grouped age question to anonymise responses from patients. Our feedback can be analysed through the shiny app with code here.\r\nSUCE (Service User and Carers Experience) survey:\r\nUnder 12 12-17 18-25 26-39 40-64 65-79 80+ years Refused Missing\r\nSUCE (Service User and Carers Experience) survey - under 12 years specific\r\nUnder 6 6 to 8 9 to 11 2 to 17 18+ years\r\nNational Workforce Dataset\r\nThe National Workforce Dataset (ESR values) use ages grouped like from the Age profile projection model which uses:\r\n<20 20-24 25-29 30-34 35-39 40-44 45-49 50-54 55-59 60-64 65-69 70+\r\nOffice of National Statistics (ONS) - population projections\r\nAlso see: https://cdu-data-science-team.github.io/team-blog/posts/2021-06-22-population-projections-websites/\r\nAge bands from population projections are:\r\n0-4 5-9 10-14 15-19 20-24 25-29 30-34 35-39 40-44 45-49 50-54 55-59 60-64 65-69 70-74 75-79 80-84 85-89 90 and over\r\nOffice of National Statistics (ONS) - survey best practice\r\nThere are various suggested age bands for surveys which are listed on this Wikimedia page.\r\nNHS Staff Survey\r\nInformation on the NHS Staff Survey has changed in how it was published but the latest website (as of June 2021) has the dashboard with age bands as:\r\n16-20 21-30 31-40 41-50 51-65 66+\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-22-age-bands-methodology/voyageurs-anciens-et-modernes.jpg",
    "last_modified": "2023-09-01T10:10:39+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-22-population-projections-websites/",
    "title": "Population Projections",
    "description": "Links for population projection data.",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Lextuga007": {}
        }
      }
    ],
    "date": "2021-06-22",
    "categories": [
      "Resources"
    ],
    "contents": "\r\nPopulation projections\r\nThe ONS run the census once every 10 years in England and are estimated between censuses. These are used for resource allocation and planning as well as providing denominator population groups for some analyses.\r\nOffice of National Statistics (ONS) Mid Year Estimates\r\nThe mid year estimates are available here.\r\nThe Analysis Tool ins an interactive tool that creates a population pyramid in excel.\r\nBy CCG\r\nMid-year (30 June) estimates of the usual resident population for clinical commissioning groups (CCGs) in England: link\r\nPopulation figures over a 25-year period, by five-year age groups and sex for clinical commissioning groups (CCGs) in England. 2018-based estimates are the latest principal projection: link\r\nSpecific Population Projections\r\nPOPPI and PANSI\r\nProjecting Older People Population Information and Projecting Adult Needs and Service Information\r\nThere are two specific websites designed to help explore the possible impact that demography and certain conditions on populations either aged 65 and over: POPPI or populations aged 18 to 64: PANSI.\r\n\r\nOriginally developed for the Department of Health, this system provides population data by age band, gender, ethnic group, and tenure, for English local authorities.\r\n\r\nThese require an account to access but only one account is required to access both site and is free to register.\r\nProjections are by Region and cover age groups but also gender, ethnic groups, health (including mental health) and learning disability.\r\nJournal articles\r\nLancet Public Health\r\nForecasting the care needs of the older population in England over the next 20 years estimates from the Population Ageing and Care Simulation (PACSim) modelling study. Aug 2018)\r\nModelling the growing need for social care in older people\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-22-population-projections-websites/img/allgemeine-erdbeschreibung.jpg",
    "last_modified": "2023-09-01T10:10:39+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-10-making-a-presentation-repo-github-template/",
    "title": "Making presentation slides into a GitHub repository template",
    "description": "One of a series of posts relating to creating presentation templates using {xaringan}, GitHub and R Studio.",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Lextuga007": {}
        }
      }
    ],
    "date": "2021-06-10",
    "categories": [
      "Open source",
      "GitHub",
      "Presentations"
    ],
    "contents": "\r\nCreating a template GitHub repository\r\nAfter creating {xaringan} presentations slides for the CDU Data Science Team using the branding from Nottinghamshire Healthcare NHS Foundation Trust, I wanted to share the files used as a template. There are quite a few used in {xaringan} slides because it relies upon CSS and images to give the ‘professional’ look similar to PowerPoint.\r\nOne of the new features in GitHub is to make your repository a template:\r\n\r\nAnd the nice thing about this, is when someone selects the template button:\r\n\r\nIt means they can fork the repository (get a copy) but all of the commit history is removed giving the next person - if they want - a clean repository to work from. People still have the option to clone the repository with the history by following the usual cloning:\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-10-making-a-presentation-repo-github-template/img/grundzüge-der-mathematischen-geographie-und-der-landkartenprojection.jpg",
    "last_modified": "2023-09-01T10:10:39+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-10-making-an-rstudio-presentation-template/",
    "title": "Making an RStudio presentation template",
    "description": "One of a series of posts relating to creating presentation templates using {xaringan}, GitHub and R Studio.",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Lextuga007": {}
        }
      }
    ],
    "date": "2021-06-10",
    "categories": [
      "Open source",
      "RStudio",
      "Presentations"
    ],
    "contents": "\r\nCreating a template through R Studio\r\nDid you know it’s possible to set up your own templates into RStudio? I didn’t until I started looking into the presentation templates for the CDU Data Science Team. The benefit of this is that I no longer have to set up clone repositories each time which is awkward too if you want to keep all presentations in one repository like we do for the team’s presentations.\r\nA few in the team suggested setting these up as a package but {xaringan} slides come with many supporting files (images and CSS), and it wasn’t too obvious how to do this. Luckily a number of people have done it and one such person tweeted about it so I added the details to the repository’s issues for reference.\r\nThis led me to @DrMowinckels’s package {uiothemes} which is particularly useful as she uses this package for various templates and themes and so, following the layout for adding the xaringan slides I added the following folders to our own package {nottshcMethods} following our process of:\r\ncreate a branch with the issue number like 1-slide-templates\r\nmake the changes\r\nset up a pull request to the development branch\r\nFile structure\r\nI added the file structure:\r\nnottshcMethods/inst/rmarkdown/templates/Nottshc/skeleton\r\nand in the Nottshc folder I added a file called template.yaml containing:\r\nname: Nottshc Presentation template\r\ndescription: >\r\n   Standard xaringan Nottshc template for presentations\r\ncreate_dir: TRUE\r\nThe create_dir is particularly important as this is asking if a new folder directory should be created or not. As this is set to TRUE all the files and folder structure in the folder skeleton will be copied. Note that in {uiothemes} all xaringan files are within one folder but I prefer my files to be in subfolders so {nottshcMethods} has the subfolders css and img.\r\nname is what will appear in the RStudio templates later so this needs to be clear and concise.\r\nIn the folder skeleton the important file is the skeleton.Rmd which is the template RMarkdown file for the slides.\r\nAs this is within a package, to run the package Ctrl+Shift+B will build the package on your computer. When that’s run the template will appear in File/New File/R Markdown…/From Template\r\n\r\nAddendum and a plea to blog things like this\r\nI wrote this out a number of weeks before publishing and I’m so glad I did as I immediately forgot everything I did. It was only when I needed to do a presentation and went to my templates in RStudio that I realised how great this is and how I couldn’t remember how I had set it all up. Luckily I had also started this blog in order to share with others and so the moral of the story is, when you write a blog you are sharing your current knowledge with others, but also your future self.\r\nBe kind to your future self, share your thoughts and your technical wins, even if they seem small to you today they may be huge tomorrow.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-10-making-an-rstudio-presentation-template/img/bibliothek-geographischer-handbücher-107.jpg",
    "last_modified": "2023-09-01T10:10:39+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-14-index-of-multiple-deprivation/",
    "title": "Index of Multiple Deprivation",
    "description": "The measure of relative deprivation in small areas in England called lower-layer super output areas",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Letxuga007": {}
        }
      }
    ],
    "date": "2021-05-14",
    "categories": [
      "Resources"
    ],
    "contents": "\r\nIndex of Multiple Deprivation (2019)\r\nIMD is very useful for categorising the area a person lives in for deprivation. This information/links and code only cover England and deprivation is scored in relation to all the areas within England using the IMD (2019). This means that they have been ordered by the deprivation score and then ranked (IMDRank).\r\nThe common decile used is between 1 and 10 with 10 being the least deprived.\r\nData warehouses\r\nThe data is often held in 3 tables:\r\nthe postcodes of the data held (for example patients when in the healthcare sector)\r\na lookup postcode table (like a directory of postcodes) from\r\nhttps://digital.nhs.uk/services/organisation-data-service/data-downloads/ods-postcode-files\r\nthe IMD data from\r\nOlder version: http://www.gov.uk/government/statistics/english-indices-of-deprivation-2015 https://www.gov.uk/government/statistics/english-indices-of-deprivation-2019\r\nSelect File 7 for the dataset.\r\nOther data is available from this dataset including IDAOPI which relates to only older people.\r\nNote that the column headers change, in 2015 it was LADistrictCode2013 and in 2019 it is LADistrictCode2019. Also LADistrictName2013 has become LADistrictName2019.\r\nWatch for…\r\n‘Unknown’ in the data warehouse\r\nData warehouse tables may include a row for ‘unknown’ (https://www.sqlchick.com/entries/2011/5/16/usage-of-unknown-member-rows-in-a-data-warehouse.html) and this may create a valid join across tables: unknown is entered as a postcode that links to the postcode table that in turn returns data from the IMD table. This may result in a value being returned from the IMD table that isn’t valid like 0 which looks like it should be an IMD decile score, for example, but which is not.\r\nPostcode spaces\r\nAlso, postcode lengths vary according to how many spaces there are between the two parts. In the UK the postcode format can be 3 parts and then 3 or 4 then 3:\r\nNG1 1AA NG26 1AA\r\nJoining datasets is always better when the space between the postcode parts is removed. In SQL this can be:\r\nREPLACE(postcode, ’ ‘,’’)\r\nin R it can be\r\n\r\n\r\npostcode <- \"NG16 1AA\"\r\ngsub(\" \",\"\",postcode)\r\n\r\n\r\n\r\nPartial postcodes\r\nPartial postcodes will not give a sufficiently reliable IMD score.\r\nExample join code (SQL)\r\nTo get the IMD score the LSOA (Lower Super Output Area) code is required which is taken from the full postcode.\r\nThis code will not run and is dependent on the naming conventions of the SQL server. The column names of LSOA11 and LSOAcode2011 will have come from the data sources. PostCode_space will have been added to the table by the data warehouse administrator(s).\r\n\r\nSELECT Top 100 imd.*\r\nFROM DIM_AI.PatientData AS p\r\nLEFT JOIN DIM_AI.PostCodes AS pc ON p.PostCode = pc.PostCode_space                                              \r\nLEFT JOIN DIM_AI.IMD AS i ON pc.PC.LSOA11 = i.LSOAcode2011\r\nWHERE p.PostCode LIKE 'NG%'\r\n\r\nCreating quintiles\r\nSome publicly available data is in quintiles for IMD to remove small identifiable numbers.\r\nTo replicate that in local data the equation: floor((IMDDecile-1)/2) + 1 can be applied.\r\nQuintiles in SQL\r\n\r\nSELECT DISTINCT IMDDecile,\r\nFLOOR((IMDDecile-1)/2) + 1 AS IMDQuintile\r\nFROM DIM_AI.IMD\r\nORDER BY IMDDecile\r\n\r\nQuintiles in R\r\nThe following example Pubicly available data will not run download if this Rmarkdown script is run as the eval has been set to FALSE. Either change this to TRUE, remove eval=FALSE altogether or copy the code to another R script to run.\r\n\r\n\r\n# IMD by Ethnicity by Region 2007-2013\r\n# Latest: https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/livebirths/adhocs/006134birthsbyethnicitysexregionandimdquintilebyfinancialyear2007to2013\r\ndownload.file(\"https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/livebirths/adhocs/006134birthsbyethnicitysexregionandimdquintilebyfinancialyear2007to2013\",\r\n              destfile = \"regionbirthsbyethnicityIMD20072013.xls\",\r\n              method = \"wininet\", #use \"curl\" for OS X / Linux, \"wininet\" for Windows\r\n              mode = \"wb\") #wb means \"write binary\"\r\n\r\n\r\n\r\nApplying the formula:\r\n\r\n\r\nlibrary(tidyverse)\r\n# Generate a dataset\r\ndf <- structure(list(IMDDecile = c(0L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, \r\n8L, 9L, 10L, NA)), row.names = c(NA, -12L), class = c(\"tbl_df\", \r\n\"tbl\", \"data.frame\"))\r\n# Make the 0 generated into NA\r\ndf_quintile <- df %>% \r\n  replace_na(list(IMDDecile = 0)) %>% \r\n  mutate(IMDQuintile = floor((IMDDecile-1)/2) + 1,\r\n         IMDQuintile = as.character(IMDQuintile)\r\n         )\r\n\r\n\r\n\r\nCreating local IMDs\r\nIn Nottingham/Nottinghamshire the differences between the LSOA areas is diminished when ranked against England as a whole, but when ranked locally, the variation is much more pronounced. Consequently, for the majority of our analysis the Trust approach is to use Nottinghamshire quintiles of deprivation / IMD. There will be some instances where national quintiles (or deciles) will be required – namely any external analysis – however, these will be the exception.\r\nFor note, when using local quintiles anyone with a non-local postcode will come back unmatched as well as those people with proxy postcodes used often to denote homeless status (ZZ…).\r\nIMD in SQL\r\nTo create the local rankings in SQL the data needs to be restricted to the appropriate area, for example when joining to the Postcodes and restricting and then a windows partition applied to the data ROW_NUMBER() OVER(ORDER BY IMDRank) to create a new ranking score and NTILE(10) OVER (ORDER BY IMDRank) to create new deciles.\r\nTo replicate this in dplyr the code would be:\r\n\r\n\r\n # Data for Nottingham and Nottinghamshire taken from Postcodes and IMD. The original files are very large to download. \r\n# Note that there are columns in the Postcode data sample that don't exist in the source file. \r\n# These have been added locally but may be of use to others, such as CountyName and LocalAuthority_Name. # These are specifically added as Nottingham Local Authority is a Unitary Authority and appears in a different column to Nottinghamshire County's District Councils \r\nload(\"data/sampleDataNottingham.RData\")\r\nlibrary(dplyr)\r\n# Note on the join the two column names are different so are listed in the by = and they need to be in the correct order so the column from imdNottingham appears first.\r\nlocalRanking <- imdNottingham %>% \r\n  inner_join(postcodeData %>% \r\n               select(LSOA11) %>% \r\n               group_by(LSOA11) %>% \r\n               slice(1), by = c(\"LSOAcode2011\" = \"LSOA11\")) %>% \r\n  mutate(Notts_rank = row_number(IMDRank),\r\n         Notts_decile = ntile(IMDRank, 10)) \r\nlocalRanking %>% \r\n  select(LSOAcode2011:IMDDecile) %>% \r\n  head(5) %>% \r\n  knitr::kable(format = \"html\")\r\n\r\n\r\n\r\nLSOAcode2011\r\n\r\n\r\nLSOAname2011\r\n\r\n\r\nLADistrictCode2019\r\n\r\n\r\nLADistrictName2019\r\n\r\n\r\nIMDScore\r\n\r\n\r\nIMDRank\r\n\r\n\r\nIMDDecile\r\n\r\n\r\nE01013812\r\n\r\n\r\nNottingham 018C\r\n\r\n\r\nE06000018\r\n\r\n\r\nNottingham\r\n\r\n\r\n58.744\r\n\r\n\r\n1042\r\n\r\n\r\n1\r\n\r\n\r\nE01013814\r\n\r\n\r\nNottingham 022B\r\n\r\n\r\nE06000018\r\n\r\n\r\nNottingham\r\n\r\n\r\n42.893\r\n\r\n\r\n3499\r\n\r\n\r\n2\r\n\r\n\r\nE01013810\r\n\r\n\r\nNottingham 018A\r\n\r\n\r\nE06000018\r\n\r\n\r\nNottingham\r\n\r\n\r\n52.690\r\n\r\n\r\n1767\r\n\r\n\r\n1\r\n\r\n\r\nE01013811\r\n\r\n\r\nNottingham 018B\r\n\r\n\r\nE06000018\r\n\r\n\r\nNottingham\r\n\r\n\r\n53.234\r\n\r\n\r\n1688\r\n\r\n\r\n1\r\n\r\n\r\nE01013815\r\n\r\n\r\nNottingham 022C\r\n\r\n\r\nE06000018\r\n\r\n\r\nNottingham\r\n\r\n\r\n41.721\r\n\r\n\r\n3805\r\n\r\n\r\n2\r\n\r\n\r\nOther useful links\r\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/464430/English_Index_of_Multiple_Deprivation_2015_-_Guidance.pdf\r\nhttps://fingertips.phe.org.uk/search/imd\r\nhttp://dclgapps.communities.gov.uk/imd/idmap.html\r\nTechnical report for 2019: https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/833951/IoD2019_Technical_Report.pdf\r\nReferencing IMD in a paper or research\r\nFrom a journal check to see how IMD is referenced in published papers, this was from a 2016 BMJ article that cites the Index in the references as:\r\nDepartment for Communities and Local Government. English indices of deprivation 2015. 2015. https://www.gov.uk/government/statistics/ english-indices-of-deprivation-2015\r\n(Taken from https://bmjopen.bmj.com/content/bmjopen/6/11/e012750.full.pdf)\r\nAnother paper from 2016 cites as:\r\nDepartment for Communities and Local Government. English Indices of Deprivation 2015. Available online: http://www.gov.uk/government/statistics/english-indices-of-deprivation-2015 (accessed on 27 April 2016).\r\n(taken from https://www.mdpi.com/1660-4601/13/8/750)\r\nLooking at the Government page that lists the full text the library assistant said: “I would reference it from the Ministry of Housing, Communities and Local Government which would be more up to date for 2019 and with online references you should always put the date you accessed it. So I would suggest amending to the following:”\r\nMinistry of Housing, Communities and Local Government. English Indices of Deprivation 2015. 2015. https://www.gov.uk/government/statistics/english-indices-of-deprivation-2015 (Accessed 4 June 2019)\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-14-index-of-multiple-deprivation/img/grundzüge-der-mathematischen-geographie-und-der-landkartenprojection-93.jpg",
    "last_modified": "2023-09-01T10:10:39+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-14-mapping/",
    "title": "Mapping",
    "description": "Mapping using public health tools",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Letxuga007": {}
        }
      }
    ],
    "date": "2021-05-14",
    "categories": [
      "Resources"
    ],
    "contents": "\r\nMapping\r\nPublic Health Tools\r\nAs Public Health is based within Local Authorities many of their boundaries are related to government rather than health boundaries. Smaller areas related to GPs will be included but data becomes patchy at Trust boundary level. For example, Nottinghamshire Healthcare NHS Foundation Trust covers Nottinghamshire, Nottingham and some areas outside of these boundaries.\r\nLocal Health\r\nPublic Health use the following to overlay data such as life expectancy over IMD scores. It is possible to upload data to this site but this has not been approved by IG.\r\nShape\r\nThis requires creating an account but is freely available to NHS staff.\r\nThis has Trust locations already in the account and data can be overlayed. Drive time and public transport within so many minutes is particularly useful.\r\nCentroid mapping\r\nSometimes you have a set of addresses but no way of mapping them. The Office for National Statistics’ Open Geography Portal provides the centroids for all UK postcodes.\r\nUsing R to get centroid information: https://www.trafforddatalab.io/recipes/gis/postcodes.html#\r\nOr weighted by LSOA area: https://geoportal.statistics.gov.uk/datasets/lower-layer-super-output-areas-december-2011-population-weighted-centroids\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-14-mapping/img/half-hour-library-of-travel-nature-and-science-for-young-readers.jpg",
    "last_modified": "2023-09-01T10:10:39+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-27-working-in-the-open/",
    "title": "Working in the open",
    "description": "What does it mean to work in the open? What is open source? What problems can we solve if we share more openly?",
    "author": [
      {
        "name": "Zoë Turner",
        "url": {
          "https://twitter.com/Letxuga007": {}
        }
      }
    ],
    "date": "2021-02-27",
    "categories": [
      "Open source"
    ],
    "contents": "\r\nWhat is working out in the open?\r\n“Working in the open” isn’t a technical term; it’s just my simple way of explaining a concept without having to mention all the tools that are available to do this.\r\n\r\n\r\n\r\nAs you can see from @ChrisBeeley’s tweet, he refers to “writing stuff in the open and making it reusable”; we use slightly different language but it covers the same principles.\r\nBeing open in the Public Sector\r\nI’ve always found that analysts working in the NHS and Local Authorities are always happy to share their methodologies, approaches to work and even code and how we shared this was often dictated by who we know and the tools we have to hand, like Excel or Word. If a change was made to the original I’d never know about it and, vice versa, if I improved the code I wouldn’t have an easy way to share back what I’d done.\r\nWhilst analysts were happy to share code I still built up a code repository for myself and for many years I recycled my own code. I often refer back to projects where I know I’ve written a particular bit of code that is useful and I’d rarely wrote out complete chunks of code that ran independently of project data. Working openly changes how you approach code because sharing projects that don’t work too well without some changes isn’t all that useful to others.\r\nWorking with an intention to be open makes you aware of public scrutiny and so, inevitably, you may take a bit longer to make code tidy, write a few more explanatory comments and ensure that code does what you think it should. What’s nice about doing this is that although the openness of work is intended for someone else’s benefit, often that person is still you.\r\nRepeatedly solving the same problem\r\nMany of the tasks that analysts and data scientists in the public sector are tasked with are the same. National Returns and benchmarking submissions are common and are completed by many trusts using slightly different approaches but, ultimately, leading to the same data output. This results in a constant cycle of problem solving where the solution, if not shared publicly, means others have to do the very same discovery work.\r\nIn a completely different context, it would be like finding a chemical compound, not sharing that knowledge and other people working hard in other labs to repeat the discovery. By constantly working in this ‘discovery’ phase we never further our collective knowledge by refining the techniques, analysing the results and, hopefully, using the “compound” to make a difference.\r\nPublishing code means that anyone in my team, my trust, the NHS, even the world can see what I have already discovered. In sharing to the world audience, I know I have something I myself can use. Of course, it takes time to write these things out but once it’s written it never needs to be rediscovered again - but it can be improved upon.\r\nHow do the CDU data science work in the open?\r\nThe CDU data science team have a strong desire to work openly and we have created a GitHub account to share code and knowledge like the pages on IMD1 and mapping.\r\nWe are also involved with the NHS-R Community, facilitating training, presenting webinars and talks as well as hosting the annual Hacktoberfest which was virtual last year. We had originally set up the Hacktoberfest to just be our team, setting aside one day to contribute to projects and practice using GitHub. Pretty quickly after agreeing this would be a good idea we extended this to the NHS-R Community as we felt that there really wasn’t any need for it to be restricted to just our team. We had a few people come in and out of the MS Teams meeting through the day and, like many things in the NHS-R Community, it was very supportive and informal.\r\nBuiding up skills\r\nWorking on other people’s projects in a Hackathon may seem, on the face of it, “non-essential” work, but it’s invaluable as it not only opens up connections with others who can help with your projects, but you invariably see useful code you can then use. Reviewing code is one thing, but to really understand a piece of code, debug or solve a problem you often have to break it apart and build it back up. In doing so you learn how it is constructed programmatically and how the other person/people have approached a problem. Doing this with others’ scripts has made me a better coder and none of the effort has been wasted.\r\nA recent example of valuable “non-essential” work for me was helping someone in the NHS-R Community Slack group who had an issue with their RMarkdown and getting a plotly chart to appear in the eventual html output, although it would appear when each chunk was run. I took the code and moved each chunk into a template RMarkdown to see if it ran, section by section. In doing so I located the problem but I also saw a new bit of code2\r\n\r\n\r\ncode_folding: \"hide\"\r\n\r\n\r\n\r\nwhich I’d never seen that before. Now I could have equally have learned about this from reading about RMarkdown but it will forever stick in my memory as it was in the context of solving a problem. We helped each other and now I’m sharing that learning in this blog - working out in the open.\r\nThe full YAML for reference\r\n---\r\ntitle: \"Test\"\r\ndate: \"25/02/2021\"\r\noutput: \r\n  html_document:\r\n    code_folding: \"hide\"\r\n    toc: true\r\n    toc_float: true\r\n    toc_collapsed: false\r\n---\r\nIf you want to read more about how our journey is going with working out in the open keep in touch by following us on Twitter and in these blogs.\r\n\r\nIndices of Multiple Deprivation↩︎\r\nThe problem was results=‘hide’ being in the knitr::opts_chunk$set() code which affects the output. I’m still learning how these codes work so didn’t spot that at first, so I learned more about RMarkdown by debugging.↩︎\r\n",
    "preview": "posts/2021-02-27-working-in-the-open/img/letters-from-high-latitudes.jpg",
    "last_modified": "2023-09-01T10:10:39+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-20-a-new-github-release-and-future-projects/",
    "title": "A new GitHub release and future projects",
    "description": "We have a new project out and would like to tell you about some more of our future work.",
    "author": [
      {
        "name": "Chris Beeley",
        "url": {}
      }
    ],
    "date": "2021-01-20",
    "categories": [
      "Open source"
    ],
    "contents": "\r\nIf you’ve read the about section of this blog then you’ll know that our team believes in (and practises!) open source data science. We strive to put as much code and (sometimes synthetic) data out as possible, with an open source licence (MIT, usually), and where we can we try to make our code reasonably easy to re-use (although this is not always simple). We have just pushed out a prototype version of an application and this seemed like a timely moment to talk about the application, and what else we have coming up on the open source side of things. It’s worth saying that some of our team members work really, really hard doing lots of stuff that is very difficult to share so although you might not see as much of them on the GitHub they’re doing sterling work for the Trust and the team is dependent on their expertise for all of our work, whether it’s open source or not.\r\nText mining application\r\nWe already have a blog post about this work and we have come to the point where we have produced a release version (0.1.0) for the dashboard which summarises the acccuracy of the models and helps to show the kinds of decisions that it’s making. Please read the blog post for details of this work but our ambition in brief is to produce a text mining algorithm for patient experience that can be used in any NHS organisation in the country. The actual algorithm work (which is in Python mainly) has not yet stabilised to a release version just yet but is available on GitHub.\r\nWe will be shipping another dashboard that helps trusts to visualise their feedback as part of the project. We’re currently working on that but we’re not quite ready to share it yet, keep an eye on our Twitter and blog for more details. We have a lot more to come in the way of working with staff and patient experience data, too, it’s not just this work, so please feel free to follow along with the code once it’s all open and maybe even send us a pull request 😉.\r\nForecasting of patient numbers\r\nWe’re also involved in a Health Foundation funded project which looks at predicting numbers of certain types of patients in the hospital. It takes the form of a dashboard which can predict the numbers of patients likely to fall into particular categories in the next 1-10 days based on previous data of this kind. The model has complex seasonality (although currently it achieves better results if you constrain it to results since April because of COVID) and a TBATS model produced the best results. The code is MIT open source and could be easily adapted to predict lots of different univariate series. There are lots of other people involved in this project and written materials from them are forthcoming, I will add them to this blog post once they are available. Our role was just to help with the forecasting and write the dashboard, lots of other work has gone into it.\r\nForecasting pharmacy dispensing\r\nThis is another Health Foundation funded forecasting project which attempts to predict the amount of many different medications which will be dispensed from a pharmacy in order to better manage stock levels. Again the code for our bit is open source MIT, although it is very early days for this project so there will be much more to come. There is much more to this project than just the code, and I will update this blog post with more details once more of the outputs are ready.\r\nA Shiny interface to EndomineR\r\nThis work relates to another Health Foundation funded project but it was funded by NHS-R. EndomineR is a clinical text mining system which works with endoscopic reports and helps to collate and analyse data from free text reports automatically. This work replicates an existing Shiny interface but uses the {golem} package to rewrite the code within modules and to make the application run as an R package. This will make the code easier to maintain, update, and generalise to other clinical settings. This project is currently in active development but it should be ready for a first release in February some time, the code again available open source on GitHub.\r\n{golem}, gitflow, and production data science\r\nWe are all still learning but we are trying to use good methods to make sure that our code is robust and easy to deploy, and to help us collaborate with each other. To this end we:\r\nUse the {golem} package for a lot of our Shiny work, and modularise our Shiny code\r\nUse RStudio Connect (I have written some stuff about this on my own blog)\r\nUse gitflow\r\nHave regular (two weekly) code review sessions\r\nI’m really interested in understanding how to get better at working together in the open and tools to help code easy to deploy and generalise. If you’re interested in that too, especially if you work in the NHS (some of the hurdles are the same size and height everywhere in the NHS 😆) then please get in touch.\r\nWe have lots of stuff planned including better analysis of HoNOS data, more to come on staff and patient experience, methods for summarising clinical outcomes and health inequalities, and other stuff from team members where I’m not quite sure how close they are to launch. Please watch out for regular updates if you would like to see more stuff from us, on our Twitter and on this blog.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-20-a-new-github-release-and-future-projects/img/admiralty-manual-for-the-deviations-of-the-compass.jpg",
    "last_modified": "2023-09-01T10:10:39+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-13-data-clinics-in-nottinghamshire-healthcare/",
    "title": "Data clinics in Nottinghamshire Healthcare",
    "description": "We have been working with teams to help them with their data problems. This post describes some of the clinics and what has come about as a result of this work.",
    "author": [
      {
        "name": "Lori Edwards Suárez",
        "url": {
          "https://twitter.com/Lori_E_S_": {}
        }
      }
    ],
    "date": "2021-01-13",
    "categories": [
      "Data clinics"
    ],
    "contents": "\r\nOur team recently piloted data clinics within the Trust in order to:\r\nImprove data quality and completeness\r\nImprove the means by which staff collect and record the essential information, making it more efficient and freeing them up to spend more time with their patients.\r\nThis was achieved by going “back to basics” with the people who collect and input data, the key principles are ensuring they know why they are collecting the data, making sure the data collection system works well for the teams and that the data can be used by both the data collectors and analysts. A variety of avenues were considered, such as reducing excess data collection and reducing duplication which make data gathering more laborious and tedious for clinicians.\r\nWhen data collection is difficult for clinicians it often results in the data not being filled properly, correcting this increases the accuracy and completeness of the dataset. Structuring clinical records and decreasing their reliaance on free text input is also beneficial for data analysis but is also often faster and easier for clinical staff. The clinics are a collaborative venture with the clinical team and others such as analysts and system admin, the type of staff varies depending on the needs of each teams. My role was to facilitate the conversations using skills learnt from working closely with the clinical teams to learn to “translate” between clinical language and data/IT language. Subtle differences in expression between the two groups often lead to misunderstandings which could stifle progress (more examples). The key element was that the problem was generated by the team themselves. This ensures that the clinic is focused on solving their difficulties which should help them to improve their own systems rather than forcing a change on them.\r\nAs a test run we had two teams go through the process:\r\nA forensic mental health team which wanted to move away from using Excel to collect their data\r\nA community mental health team which wanted to collect some extra information to better understand the impact of their team without adding too much to their workload\r\nThe forensic team was a new service which had a lot of data requested of them and they wished to improve their data collection and assess its quality. The Team Leader had used team-specific forms in RiO (the clinical database which they use) previously and was interested in seeing if it was possible here. However, they were having a tough time explaining to the managers who were not familiar with such a system how to approve it and get it built into the system. The spreadsheet was found to have a lot of duplication and data being requested that was not necessarily attainable by the team. We looked together at what the purpose was and changed some of the data from free text to a more structured pick list from the valid values for that piece of data. We also had to explain to the managers that this change was not going to affect their reporting adversely. The patient record system was able to provide the team with what they needed and to automate some aspects to reduce workload for clinicians. Some outcomes measures already existed but others were not yet available on the system, an Excel sheet was made to collect them (an improvement over a folder in the corner of the room) with a reduction in demand for clinicians with simple automation of score summations. The team are thrilled that they can collect the data necessary for reporting and understanding their service in a more intuitive way, project managers are content they are getting the same information and more data is readily available for service improvement. Reports are being built which give the clinicians easy access to data which allows them to engage better and feel ownership of the data.\r\nThe community mental health team wanted to collect some more information to improve their ability to understand their outcomes. They needed to be able to distinguish between the cohorts of patients that were being referred. This ended up having a simple solution that had not been known to the clinical team – adding in more specific referral reasons. The patient cohort was clear and defined and could be determined at referral. They wanted some more information on one of the cohorts to understand the group further and to see how specific patients within the group progressed. To gather this data, a short form on the electronic patient record was created which takes one minute to fill in but adds a wealth of information. The team also got to play with the form before it went live to gain familiarity and to help them feel ownership of it. They also wanted to be able to predict when referrals may come in. As we got to know the pathways that brought patients into the service, we learned that we had information about patientes in the previous stage of the pathway. So, we managed to collect some information to understand the time between the previous stage and the referral. This means we can see when there is an uptick in people passing through the previous stage and predict a spike in referrals for the team to prepare for.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-13-data-clinics-in-nottinghamshire-healthcare/img/grundzüge-der-mathematischen-geographie-und-der-landkartenprojection-106.jpg",
    "last_modified": "2023-09-01T10:10:39+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-14-classification-of-patient-feedback/",
    "title": "Classification of patient feedback",
    "description": "An NHSE funded project to devise an application to automatically tag the content of patient feedback",
    "author": [
      {
        "name": "Andreas Soteriades",
        "url": {}
      }
    ],
    "date": "2020-11-14",
    "categories": [
      "Patient Experience"
    ],
    "contents": "\r\nConsider the following problem. A NHS trust is devoted to improving their services to provide patients with the best possible experience. The trust sets up a Patient Feedback system in order to identify key areas of improvement. Specialized trust staff (the “coders”) read the feedback and decide what it is about. For example, if the feedback is “The doctor carefully listened to me and clearly explained me the possible solutions”, then the coders can safely conclude that the feedback is about communication.\r\nBut what happens when thousands and thousands of patient feedback records are populating the trust’s database every few days? Can the coders keep up with tagging such a high volume of records? After all, unless they read all of it, they cannot tag it!\r\nWe need to find a clever way to get some weight off the coders’ shoulders!\r\nHere in Nottinghamshire Healthcare NHS Foundation Trust, we (the Data Science team) have opted for a Machine Learning approach to help coders tag the incoming patient feedback. In particular, we are developing Text Classification algorithms that “read” the feedback and decide what it is about.\r\nFirst things first: what are Machine Learning and Text Classification?\r\nMachine Learning is a wider concept, but here we will talk about the so-called supervised Machine Learning. Say a child is playing with a hole cube:\r\nPhoto of a child’s sorting toy with the sorting shapes stacked to the side of the sorting boxBy trying to pass different shapes through different holes, the child follows a process of “training” or “learning”, through which they learn to identify the right shape for each hole. Once they have been “trained”, they can easily predict what shape is the right one for a specific hole, on this or any other hole cube.\r\nThe process that the child has just followed is very similar to Machine Learning: see the child as an algorithm, the shapes as a dataset, and the holes as tags and you have a supervised Machine Learning problem. In other words, in supervised Machine Learning the algorithm “learns from” or “is trained on” the dataset, and thus becomes able to predict what tag corresponds to each record.\r\nSo when we have patient feedback data that have already been tagged by our coders, we can train an algorithm to assign the most appropriate tag to each feedback record, based on the content of the feedback text. As fresh, untagged feedback populates the trust’s database, the algorithm is then able to predict the most appropriate tag for it. In other words, the algorithm learns to automatically classify the text according its content, which is what Text Classification is about: a form of supervised Machine Learning that is about predicting the appropriate tag for the given text.\r\nHow can Text Classification improve NHS services?\r\nIncrease tagging speed. As mentioned earlier, the idea is to have the algorithm automatically tag feedback that the coders simply do not have time to read and tag themselves. To begin with, it will make the process of tagging much more efficient.\r\nNarrow down searches for NHS staff. If a member of staff (e.g. manager, doctor, nurse) wishes to focus on improve patient experience that has to do with, e.g. communication, they will want to read some or all of the incoming feedback about it. The algorithm will crunch the incoming feedback, decide which records are about communication, and feed them back to the member of staff.\r\nAre there any cons?\r\nAlgorithms make errors. For example, an algorithm may incorrectly classify feedback about smoking as being about communication. This is to be expected as no algorithm can ever be 100% accurate. What is key then is to make the algorithm as accurate as possible for the task at hand. This is an area where we focus on on a daily basis.\r\nDespite some inaccuracies, Machine Learning will still offer the great advantage of narrowing down NHS staff searches almost exclusively to the feedback of interest. If a manager has 100 feedback records of which only 20 are potentially relevant, and the algorithm predicts that 30 are potentially relevant (because it will make a few mistakes), this would still be a 70% reduction in the number of feedback records to be read by the manager!\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-14-classification-of-patient-feedback/img/grundzüge-der-mathematischen-geographie-und-der-landkartenprojection-96.jpg",
    "last_modified": "2023-09-01T10:10:39+01:00",
    "input_file": {}
  }
]
