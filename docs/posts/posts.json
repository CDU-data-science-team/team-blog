[
  {
    "path": "posts/2021-08-06-nottshcverse/",
    "title": "Development of open tools for analysing healthcare data in R",
    "description": "***TLTR: (Too Long To Read)*** \nOur goal was to make it easier to work with healthcare data in a reproducible and collaborative way.\nWe wrote lots of R functions for recurring data manipulations and analytical tasks that magically translate into SQL code and communicate with large databases.\nAll our functions are grouped into R packages because this made it easier for us to: *(i)* write good documentation of our code and analytical tasks, *(ii)* easily distribute updates across all team members, *(iii)* formally test our code, and *(iv)* integrate common data manipulations (or analyses) into interactive dashboards in a modular way.",
    "author": [
      {
        "name": "Milan Wiedemann",
        "url": {}
      }
    ],
    "date": "2021-09-29",
    "categories": [
      "Open source",
      "Team working",
      "Packages"
    ],
    "contents": "\r\n\r\nContents\r\nChallenges and solutions\r\nOverview of the {nottshcverse}\r\nSimplified working example\r\nCreate example data\r\nCreate SQLite connection\r\nExample analysis\r\nWriting a function\r\nExample SQL code\r\nExample results\r\n\r\nWatch out now\r\nSummary\r\nRelated work and resources\r\n\r\n\r\n\r\n\r\nOur team works with routinely collected NHS patient data. Currently we focus on understanding better how patients are using the service, changes in clinical outcome measures, and analysis of patient experiences. The main questions that guide our work are ‚ÄòWhat works for whom and how does it work?‚Äô, ‚ÄòWhat doesn‚Äôt work?‚Äô, and ‚ÄòHow can we integrate patient experiences into our analyses?‚Äô. We developed a set of different tools, the {nottshcverse}, to help us look at these questions by automating recurring and time-consuming tasks so that we can spend more time thinking the clinical questions.\r\nChallenges and solutions\r\nReal data is messy but we should do our best to tidy the mess, where possible in an automated way. There are many challenges when working with healthcare data, here is only a small selection of those that I think underlies most analyses. Figure 1 shows our solutions to these challenges.\r\n\r\n\r\n\r\nFigure 1: Main goals that guided the development of the {nottshcverse} packages.\r\n\r\n\r\n\r\nü§î It‚Äôs really hard to get the clinical data that is needed in a reproducible way that is consistent across different people who work on the same (or related) analysis. This is particularly true because most of the data is stored on SQL servers and only starts to make sense after joining multiple different datasets. ü§ì We wrote functions that made it very easy and secure to (i) connect to and (ii) query data from different databases that we are working with. This way we could get the data we needed for our analyses using very few lines of code in R.\r\n\r\n\r\n# First, create connection to databases\r\nconn_s1 <- connect_sql(server = \"SystmOne\")\r\nconn_iapt <- connect_sql(server = \"IAPTus\")\r\n\r\n# Now we can use the connection to get contacts data from SystmOne ...\r\ndb_contacts_s1 <- get_s1_contacts(from = \"2020-01-01\", \r\n                                  to = \"2020-12-31\", \r\n                                  conn = conn_s1)\r\n\r\n# ... and IAPTus databases\r\ndb_contacts_iapt <- get_iapt_contacts(from = \"2020-01-01\", \r\n                                      to = \"2020-12-31\", \r\n                                      conn = conn_iapt)\r\n\r\n# Note that the objects 'contacts_s1' and 'contacts_iapt' are just pointing to \r\n# the databases (I like to use the prefix db) and not actually downloaded to the\r\n# environment on your computer.\r\n\r\n\r\n\r\nü§î Most of the time, the data is not in the format that is needed for further analyses. There may be specific data manipulations that are needed to make sense of the data and analyse it properly. Also, different databases might be set up in ways that it is hard to merge data. ü§ì We wrote function that tidy the raw data from the databases so that it is more consistent across databases and easier to analyse. All of this still happens within the server so that our computers don‚Äôt have to do all of this work.\r\n\r\n\r\n# Each get_*_data() function comes with a tidy_*_data(), here tidy_s1_contacts()\r\n# Here I use the connection to the raw (messy) contacts  data that I created above\r\n# and tidy it using the tidy_s1_contacts() function\r\ndb_contacts_s1 <- db_contacts_s1 %>% \r\n  tidy_s1_contacts()\r\n\r\ndb_contacts_iapt <- db_contacts_iapt %>% \r\n  tidy_iapt_contacts()\r\n\r\n\r\n\r\nü§î The methods of data analyses and visualisations should be understandable, reproducible, and available to other people. Unfortunately this is not always the case yet because the software tools that are used (e.g., Excel, PowerBI, and other silly solutions that are not worth their money) are not script based and usually shared in private emails. ü§ì We wrote R functions for common analytical tasks and visualisations.\r\nü§î Code should be really well documented so that it is easy to understand what‚Äôs going on. This includes the current version of the code as well as all previous versions and changes. This can be done using tools like Git and GitHub, but unfortunately most code is currently shared undocumented in private emails. ü§ì We created detailed documentations that are easily accessible to everyone who uses our R packages. Also, because develop our tools on GitHub, every change to our code is documented.\r\nü§î Mistakes happen! Sometimes things that you have no control over can change and break your code that previously worked fine (e.g., the format of the raw data or a functions that someone else wrote). Therefore, it‚Äôs important to continuously test whether the code is still working the way it‚Äôs supposed to work. ü§ì R packages (or similar solutions in other statistical programming languages) are relatively easy to test, for example using the {testthat} package. We started to implement tests into our work so that we can check if changes that we make to our code don‚Äôt break anything.\r\nOverview of the {nottshcverse}\r\n\r\n\r\n\r\nFigure 2: Overview of some R packages developed by the Clinical Development Unit Data Science Team‚≠êand the NHS-R Community ‚ù§Ô∏è\r\n\r\n\r\n\r\n{nottshcData}: Unified framework to query, transform, and aggregate data from different databases\r\n{nottshcMethods}: Tools for performing common analytical tasks (e.g., grouping continuous age into groups)\r\n{honos}, {LSOApop}: Packages designed in generic way to help use and others {nottshcData} work with specific questionnaires (e.g., Health of the Nation Outcome Scales, HoNOS) or open data sets (e.g.¬†LSOA population estimates)\r\n{outcomesdashboard}: Our dashboards use all the packages mentioned above + special packages developed specifically to support the dashboards with helper functions\r\nSimplified working example\r\nTo illustrate how R can be used to work with databases I‚Äôll use the following example. Imagine we‚Äôre working with a database called SystmTwo (S2) and need to use two different tables for our analysis:\r\n[S2].[contacts]: Information about contacts with clinical teams\r\n[S2].[demographics]: Some demographic information\r\n\r\n\r\n\r\nCreate example data\r\n\r\n\r\n# Set up example contacts table\r\ncontacts_s2 <- tibble(client_id = c(1, 1, 1, 2), \r\n                      contact_id = c(123, 124, 125, 156), \r\n                      referral_id = c(456, 459, 500, 501), \r\n                      referral_date = c(\"2018-04-19\", \"2019-05-23\", \r\n                                        \"2020-06-01\", \"2018-12-11\"),\r\n                      contact_date = c(\"2018-05-19\", \"2019-06-05\", \r\n                                       \"2020-07-08\", \"2019-01-15\"),\r\n                      team_id = c(\"tm1\", \"tm2\", \"tm1\", \"tm1\"), \r\n                      hcp_id = c(\"hcp1\", \"hcp2\", \"hcp1\", \"hcp1\"), \r\n                      contact_type = c(\"phone\", \"f2f\", \"video\", \"phone\"),\r\n                      assessment_id = c(321, 322, 344, NA))\r\n\r\n# Set up example demographics table with 2 patients\r\ndemographics_s2 <- tibble(client_id = c(1, 2), \r\n                          dob = c(\"1988-01-01\", \"1965-01-01\"),\r\n                          dod = c(NA, NA),\r\n                          sex = c(\"f\", \"m\"))\r\n\r\n\r\n\r\nCreate SQLite connection\r\n\r\n\r\n# Create connection (conn) to \"local\" database called SystmTwo (s2)\r\nconn_s2 <- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\r\n\r\n# Copy local data frame to conn_s2 database\r\ndb_s2_contacts <- copy_to(conn_s2, contacts_s2)\r\ndb_s2_demographics <- copy_to(conn_s2, demographics_s2)\r\n\r\n\r\n\r\nExample analysis\r\nHere we join the contacts with the demographics information to calculate the age at the time a patient has their contact (age_at_contact).\r\n\r\n\r\n# Calculate age at time of contact\r\ndb_age_at_contacts <- db_s2_contacts %>% \r\n  left_join(db_s2_demographics) %>% \r\n  mutate(age_at_contact = as.Date(contact_date) - as.Date(dob))\r\n\r\n\r\n\r\nWriting a function\r\nWe can also write our own functions and use them in a modular way whenever we need them. Here‚Äôs a simple example to demonstrate how we can do the same calculation as shown above using our own function. In this example the function arguments take the variable names for date of birth (dob) and the contact date (contact_date).\r\n\r\n\r\ncalc_age_at_contact <- function(data, var_dob, var_contact_date) {\r\n  # Add code here to check that arguments are specified correctly\r\n  data %>% \r\n    dplyr::mutate(age_at_contact = as.Date({{var_contact_date}}) - as.Date({{var_dob}}))\r\n  }\r\n\r\n\r\n\r\nExample SQL code\r\nAs mentioned above (Challenges and solutions, Point 1), the object that we work with most of the time are just SQL queries and not real data stored in your R environment. We can look at the underlying SQL code using the dplyr::show_query() function. I don‚Äôt really know SQL very well myself, but some people who do have created a great package that translates R code into SQL code (see the dbplyr package for more).\r\n\r\n\r\n# Use dplyr::show_query() function to see underlying SQL code\r\nshow_query(db_age_at_contacts)\r\n\r\n\r\n<SQL>\r\nSELECT `client_id`, `contact_id`, `referral_id`, `referral_date`, `contact_date`, `team_id`, `hcp_id`, `contact_type`, `assessment_id`, `dob`, `dod`, `sex`, CAST(`contact_date` AS DATE) - CAST(`dob` AS DATE) AS `age_at_contact`\r\nFROM (SELECT `LHS`.`client_id` AS `client_id`, `contact_id`, `referral_id`, `referral_date`, `contact_date`, `team_id`, `hcp_id`, `contact_type`, `assessment_id`, `dob`, `dod`, `sex`\r\nFROM `contacts_s2` AS `LHS`\r\nLEFT JOIN `demographics_s2` AS `RHS`\r\nON (`LHS`.`client_id` = `RHS`.`client_id`)\r\n)\r\n\r\nNote that we can also see the SQL code from our own functions.\r\n\r\n\r\ndb_age_at_contacts %>% \r\n  calc_age_at_contact(var_dob = dob, \r\n                      var_contact_date = contact_date) %>% \r\n  show_query()\r\n\r\n\r\n<SQL>\r\nSELECT `client_id`, `contact_id`, `referral_id`, `referral_date`, `contact_date`, `team_id`, `hcp_id`, `contact_type`, `assessment_id`, `dob`, `dod`, `sex`, CAST(`contact_date` AS DATE) - CAST(`dob` AS DATE) AS `age_at_contact`\r\nFROM (SELECT `client_id`, `contact_id`, `referral_id`, `referral_date`, `contact_date`, `team_id`, `hcp_id`, `contact_type`, `assessment_id`, `dob`, `dod`, `sex`, CAST(`contact_date` AS DATE) - CAST(`dob` AS DATE) AS `age_at_contact`\r\nFROM (SELECT `LHS`.`client_id` AS `client_id`, `contact_id`, `referral_id`, `referral_date`, `contact_date`, `team_id`, `hcp_id`, `contact_type`, `assessment_id`, `dob`, `dod`, `sex`\r\nFROM `contacts_s2` AS `LHS`\r\nLEFT JOIN `demographics_s2` AS `RHS`\r\nON (`LHS`.`client_id` = `RHS`.`client_id`)\r\n))\r\n\r\nExample results\r\n\r\n\r\n# Look at results from SQL query shown above\r\ndb_age_at_contacts %>% \r\n  select(client_id, contact_date, dob, age_at_contact)\r\n\r\n\r\n# Source:   lazy query [?? x 4]\r\n# Database: sqlite 3.36.0 [:memory:]\r\n  client_id contact_date dob        age_at_contact\r\n      <dbl> <chr>        <chr>               <int>\r\n1         1 2018-05-19   1988-01-01             30\r\n2         1 2019-06-05   1988-01-01             31\r\n3         1 2020-07-08   1988-01-01             32\r\n4         2 2019-01-15   1965-01-01             54\r\n\r\nOf course this is a VERY simple example. This can get way more complex, think BIG and solve BIG problems. There are many other examples out there showing how to work with databases in RStudio. I added some links that I found useful at the end of this post.\r\nWatch out now\r\nSo what‚Äôs coming next and where can we take this? Is this perfect? I don‚Äôt know exactly what‚Äôs coming next and this is definitely far from perfect. But it‚Äôs the best approach my colleagues and I could come up with in the time that we spent working on this. Maybe I‚Äôll improve this one day, maybe someone else will? Until then let‚Äôs share ideas and work together to improve healthcare analytics in the NHS. Ohhh, some people are already working like this üëÄ it‚Äôs time others join them. I hope those who make decisions about the direction of healthcare analytics in the NHS will start to understand the problems and opportunities and act soon. If not now, when then? We need to move towards a more open and modern way of healthcare analytics!\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nSummary\r\nHere‚Äôs a short summary in BOLD AND ALL CAPS:\r\nDOCUMENT everything, absolutely everything! Every function and every single change!\r\nAUTOMATE common analytical tasks! Write functions and packages!\r\nTEST everything! Expect mistakes, there will be üêõüêõüêõ\r\nSHARE as much as we can!\r\nOf course this doesn‚Äôt always work. There will always be some messy data, inconsistent variable names, undocumented code, and ‚Ä¶ blah blah blah.\r\nRelated work and resources\r\nüìñ Chris Mainey (2019). SQL Server Database connections in R.\r\nüìñ Emily Riederer (2021). Workflows for querying databases via R - Tricks for modularizing and refactoring your projects SQL/R interface.\r\nüìñ Hadley Wickham, Maximilian Girlich and Edgar Ruiz (2021). dbplyr: A ‚Äòdplyr‚Äô Back End for Databases.\r\nüìñ RStudio (2021). Databases using R from RStudio.\r\nüìñ RStudio (2021). Using an ODBC driver\r\nüìΩ Edgar Ruiz (2018). Best practices for working with databases.\r\nüßô Chris Beeley (2011 ‚ÄôTil Infinity). Random bits of related and unrelated statistics, programming, and healthcare wizardry.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-08-06-nottshcverse/img/Principes-de-Logique-p435.jpg",
    "last_modified": "2021-10-12T19:14:36+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-14-text-mining-pipeline/",
    "title": "A Text Mining Pipeline for NHS Patient Experience Feedback",
    "description": "This blog post is a more technical description of the pipeline that we have built to analyse patient feedback text data from the NHS.",
    "author": [
      {
        "name": "Andreas Soteriades",
        "url": {}
      }
    ],
    "date": "2021-09-14",
    "categories": [
      "Text Mining",
      "Patient Experience"
    ],
    "contents": "\r\nThe pipeline consists of two distinct modules:\r\nA text classification pipeline for classifying patient feedback text into themes like Communication, Environment/facilities, Staff, etc.\r\nA text mining dashboard reporting results from text classification, sentiment analysis, and analysis of word frequencies to surface information about what patients most talk about, what frustrates them, what they most like in the service etc.\r\nFor the scope and more high-level descriptions of the different analyses carried out see this blog post and project description.\r\nIn terms of the nitty gritty, we are particularly excited about having combined super-cool packages in R and Python to build the pipeline! Two highlights of our work are the use of what we consider to be game-changer R packages:\r\n{golem}- ‚Äú[‚Ä¶] an opinionated framework for building production-grade shiny applications‚Äù. Package {golem} automatically provides the structure for the {shiny} skeleton (app & ui) and makes it very easy to build Shiny apps that are modular, strict as to where the business logic goes, documented, tested, shareable, and agnostic to deployment.\r\n{reticulate}- an R interface to Python that opened up for us unique opportunities for using state-of-the-art Python packages for text classification and sentiment analysis directly in R.\r\nFor more details, refer to this presentation where I describe both packages in much enthusiasm!\r\nPipeline overview\r\nLet‚Äôs take a look at the whole pipeline:\r\n\r\nThe pipeline consists of an ecosystem of tailor-made packages in R ({experienceAnalysis}, {pxtextmineR}, {pxtextminingdashboard}) and Python (pxtextmining) that we designed to be both fit-for-purpose, but also as generic as possible for use by other NHS trusts or by anyone in general. Let‚Äôs break down the pipeline into smaller steps:\r\nThe pipeline reads the text data and sends it to the text classification pipeline, as well as to the dashboard.\r\nThe text classification pipeline uses Scikit-learn- fuelled pxtextmining to tune and train a Machine Learning model. It then writes the results (predictions, performance metrics, classifier performance bar plots, a SAV with the trained text classification pipeline etc.). These are then passed into dedicated modules in the {golem} dashboard that present predictions on unlabelled feedback, as well as tables and plots with performance metrics.\r\nMeanwhile, the text data is also passed into the dashboard for sentiment analysis and other text mining (e.g.¬†TF-IDFs). The dashboard has dedicated modules that use our external packages to perform these analyses. In particular, {experienceAnalysis} makes extensive use of {tidytext}, although it offers functions that conveniently perform automatically a few data preprocessing and manipulation steps that would otherwise need to be done manually before passing them to the {tidytext} functions. On the other hand, {pxtextmineR} has {reticulate}- fuelled functions for doing sentiment analysis with Python packages TextBlob and vaderSentiment.\r\nInternally, the {golem} dashboard also has a series of R scripts containing simple utility functions that are useful for running small tasks (e.g.¬† sort a character vector) that would otherwise be run inside the modules themselves.\r\nNote that using external packages and utility functions to prepare the data keeps the modules clean from any business logic that would make the dashboard too specific to the dataset used. This is a key advantage of {golem}: we can use the dashboard as a framework for reporting results on any dataset that we would like to use! A simple example is the following: say we want to report averages for a number of categories. This could be mean sepal length for each plant species in the iris data, mean flipper length for each penguin species in the penguin data, and mean miles per gallon for each car engine type in the mtcars data. We can build a {golem} that produces the mean of a variable according to different categories and then pass either iris, penguin or mtcars to get a dashboard for each of these datasets.\r\nAnd this is exactly where the YAML file in {golem} comes in handy. The YAML file acts as a control panel where the user specifies what dataset and which columns from the dataset to use in the business logic. In our case, this means that we can produce a dashboard for our own data or for the patient feedback data of any NHS trust! As deployment with {golem} is pretty straightforward, we are able to host several dashboards on the server, each of which uses a dataset from a different NHS trust.\r\nAmazingly, {golem} ships the whole dashboard as an R package! We call our packaged dashboard pxtextminingdashboard. This package contains open patient experience data in RDA format that can be used to run the app. All you need to do is install pxtextminingdashboard, load it in R and run run_app. The dashboard is also available here.\r\nConclusion\r\nWe have built a truly revolutionary text mining pipeline that can be used for free by any NHS trust and will hopefully help surface business-critical information to guide improvements in healthcare services. We believe that the pipeline is a great example of how one can make the best of both R and Python. Both {golem} and {reticulate} are game-changers- try them out!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-09-14-text-mining-pipeline/img/a-treatise-on-map-projections-p88.jpg",
    "last_modified": "2021-10-12T18:10:59+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-30-pair-programming-code-review-journal-club-and-team-time/",
    "title": "Pair programming, code review, journal club, and team time",
    "description": "What can the CDU data science team do to verify its outputs, disseminate learning, and support individual development in team sessions?",
    "author": [
      {
        "name": "Chris Beeley",
        "url": {}
      }
    ],
    "date": "2021-07-30",
    "categories": [
      "Team working"
    ],
    "contents": "\r\nWe‚Äôve been a proper data science team for a year now, there‚Äôs six of us at the moment which is GREAT and we‚Äôre starting to review some of the stuff that we‚Äôve been doing to make sure that it‚Äôs serving a purpose. I‚Äôm sort of writing this for our benefit just as a way of recording what the plan is (and to discuss the plan via pull request üòé) but we work in the open so why not talk about this in the open too.\r\nFairly near the beginning of become A Proper Data Science Team we kicked off ‚ÄúCode review‚Äù sessions which would take place fortnightly and team members would bring stuff, taking it in turns to do one each. I hardly think I can improve on Google‚Äôs guide to code review but I‚Äôll summarise the main points here for those who don‚Äôt click through. Code review is the process of one or more people who have not written the code in question to read it and check that it‚Äôs okay, before the code is merged into the codebase. Code reviewers are considering:\r\nDesign\r\nFunctionality\r\nComplexity\r\nTests\r\nGood naming practices (variables, functions)\r\nComments\r\nStyle (judged against a style guide like the tidyverse style guide)\r\nConsistency\r\nDocumentation\r\nIn the linked document Google emphasise that you should read every line- not scan over things and assume they‚Äôre okay. It‚Äôs worth remembering as well that we are doing data science- so we need to be reviewing statistical and ML methods as well. People (including me) fall into the trap of thinking that data science is just programming and they stop asking themselves hard questions about the methods they are using and that is very dangerous.\r\nI put code review in quotes advisedly because although we went into them thinking we would be doing code review over the weeks and months we ended up doing something totally different. Sometimes we would pretty much do code review. Sometimes we would spend a long time discussing code style. Other times we would start off doing code review and end up doing an ad hoc lesson in a particular method that the person who came to code review. Sometimes we would have such a good time doing one of these things that we couldn‚Äôt fit all the excitement into an hour and would call extraordinary code review meetings so we could carry on discussing it and not have to wait two weeks.\r\nIt was pretty anarchic but we were all learning and having fun in a supportive environment so I thought we may as well just see where we ended up. After a year I thought it was time to review what we were doing around assuring our code so I kicked off a discussion about it which this blog post is part of.\r\nWe have had a pretty wide ranging discussion about it and the first point of interest is that everybody wanted to keep the fortnightly sessions but everybody agreed that, however awesome they were, they weren‚Äôt really ‚Äúcode review‚Äù. We boiled down what we feel we need to four distinct activities. For the sake of brevity I will summarise them here- I could probably do a blog post on each and if anybody would like to hear more then find me on Twitter, have a look at the about page.\r\nPair programming\r\nPair programming is what it sounds like- programming in pairs. It‚Äôs a great way of teaching and helping each other, and solving problems together, but it‚Äôs also a way of reviewing code too. The maxim we came up with today is simple.\r\n\r\nEvery line of code should have been read by two people\r\n\r\nYou can do that in a pair, live, or you can do it by review (or you can do both). Interestingly enough the team just spontaneously started doing pair programming. Nobody mentioned the word, nobody asked them to. It just makes sense to do that so they just started doing it, which I absolutely love.\r\nCode review\r\nThis is the other review methodology that came out of the discussion. I already talked about what Google think this should be so there‚Äôs not much to add. One thing we said is that the other thing we need to change is doing it fortnightly as a group. Proper code review can only be done by someone who understands the code. We have a LOT of skills in the team (SQL, R, Python, Shiny, statistics, ML) and nobody understands it all (especially me). We decided that this would happen when people feel they‚Äôre ready and would take place with one or more designated people who understand the code, and not every fortnight with everybody.\r\nOne interesting thing that came out of this discussion was my realisation that we need to deepen and broaden the skills of the team. Several of us are writing code that nobody has the expertise to review. I‚Äôve long been obsessed with truck factor but I hadn‚Äôt considered it from the position of validating code before. It‚Äôs entirely my failing as a manager and it‚Äôs something else to think about when we‚Äôre recruiting next (it‚Äôs also relevant for training and development, but that‚Äôs another post).\r\nJournal club\r\nSomething else that we spontaneously did as part of the code reviews that weren‚Äôt really code reviews was what we‚Äôre loosely calling ‚Äújournal club‚Äù- basically one of ‚ÄúI know something that you need to know, I‚Äôm going to teach you‚Äù or ‚Äúwe all need to get better at x- let‚Äôs learn together‚Äù. We‚Äôre going to start with me: ‚ÄúEverything your data scientists wanted to know about managing servers and deploying in the cloud- but were afraid to ask‚Äù which will end up on GitHub somewhere.\r\nTeam time\r\nNot sure about the name for this one! The last thing that everybody seemed to want was something else we were already doing. It‚Äôs basically just the ability to just bring anything you like and talk about it. We‚Äôve had all sorts of things. ‚ÄúThis code is fine, it‚Äôs just horribly slow‚Äù, ‚ÄúI can‚Äôt figure out what is going to give the best experience for our users‚Äù, ‚ÄúI‚Äôve done this analysis but it‚Äôs kind of shallow- what else can I do?‚Äù. I don‚Äôt think this is anything particular, it doesn‚Äôt have a proper name, it isn‚Äôt particularly for one purpose, it‚Äôs more just the team‚Äôs way of saying ‚ÄúWe help and support each other and we carve out one hour every two weeks so team members can bring a problem and talk it through with us‚Äù.\r\nWrap up\r\nThat‚Äôs where we‚Äôre at now, once we‚Äôve agreed this as the way forward (or agreed something else, obviously) then we‚Äôll give it a go. As is probably clear, our Friday sessions and code review are just one part of trying to have a team that works well together and I‚Äôm totally new at this so if anybody out there has anything to add I‚Äôd be super grateful to hear it.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-07-30-pair-programming-code-review-journal-club-and-team-time/img/the-geographical-institutions-p84.jpg",
    "last_modified": "2021-08-02T12:57:31+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-22-age-bands-methodology/",
    "title": "Age bands methodology",
    "description": "A blog compiling all the age bands methodology that can be used for comparing analysis populations against.",
    "author": [
      {
        "name": "Zo√´ Turner",
        "url": {
          "https://twitter.com/Lextuga007": {}
        }
      }
    ],
    "date": "2021-06-22",
    "categories": [
      "Resources"
    ],
    "contents": "\r\nAge Bands\r\nOn the face of it applying age bands to data analysis is simple, you need to consider the start and end ages and group up the rest into reasonable groups. However, if you want to use the data to compare to other sources of aggregate data it needs to be grouped in a similar way.\r\nFriends and Family questionnaires\r\nTrusts can control some of the data they collect for the Friends and Family questionnaires and Nottinghamshire Healthcare NHS Foundation Trust have used a grouped age question to anonymise responses from patients. Our feedback can be analysed through the shiny app with code here.\r\nSUCE (Service User and Carers Experience) survey:\r\nUnder 12 12-17 18-25 26-39 40-64 65-79 80+ years Refused Missing\r\nSUCE (Service User and Carers Experience) survey - under 12 years specific\r\nUnder 6 6 to 8 9 to 11 2 to 17 18+ years\r\nNational Workforce Dataset\r\nThe National Workforce Dataset (ESR values) use ages grouped like from the Age profile projection model which uses:\r\n<20 20-24 25-29 30-34 35-39 40-44 45-49 50-54 55-59 60-64 65-69 70+\r\nOffice of National Statistics (ONS) - population projections\r\nAlso see: https://cdu-data-science-team.github.io/team-blog/posts/2021-06-22-population-projections-websites/\r\nAge bands from population projections are:\r\n0-4 5-9 10-14 15-19 20-24 25-29 30-34 35-39 40-44 45-49 50-54 55-59 60-64 65-69 70-74 75-79 80-84 85-89 90 and over\r\nOffice of National Statistics (ONS) - survey best practice\r\nThere are various suggested age bands for surveys which are listed on this Wikimedia page.\r\nNHS Staff Survey\r\nInformation on the NHS Staff Survey has changed in how it was published but the latest website (as of June 2021) has the dashboard with age bands as:\r\n16-20 21-30 31-40 41-50 51-65 66+\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-22-age-bands-methodology/voyageurs-anciens-et-modernes.jpg",
    "last_modified": "2021-07-01T18:49:28+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-22-population-projections-websites/",
    "title": "Population Projections",
    "description": "Links for population projection data.",
    "author": [
      {
        "name": "Zo√´ Turner",
        "url": {
          "https://twitter.com/Lextuga007": {}
        }
      }
    ],
    "date": "2021-06-22",
    "categories": [
      "Resources"
    ],
    "contents": "\r\nPopulation projections\r\nThe ONS run the census once every 10 years in England and are estimated between censuses. These are used for resource allocation and planning as well as providing denominator population groups for some analyses.\r\nOffice of National Statistics (ONS) Mid Year Estimates\r\nThe mid year estimates are available here.\r\nThe Analysis Tool ins an interactive tool that creates a population pyramid in excel.\r\nBy CCG\r\nMid-year (30 June) estimates of the usual resident population for clinical commissioning groups (CCGs) in England: link\r\nPopulation figures over a 25-year period, by five-year age groups and sex for clinical commissioning groups (CCGs) in England. 2018-based estimates are the latest principal projection: link\r\nSpecific Population Projections\r\nPOPPI and PANSI\r\nProjecting Older People Population Information and Projecting Adult Needs and Service Information\r\nThere are two specific websites designed to help explore the possible impact that demography and certain conditions on populations either aged 65 and over: POPPI or populations aged 18 to 64: PANSI.\r\n\r\nOriginally developed for the Department of Health, this system provides population data by age band, gender, ethnic group, and tenure, for English local authorities.\r\n\r\nThese require an account to access but only one account is required to access both site and is free to register.\r\nProjections are by Region and cover age groups but also gender, ethnic groups, health (including mental health) and learning disability.\r\nJournal articles\r\nLancet Public Health\r\nForecasting the care needs of the older population in England over the next 20 years estimates from the Population Ageing and Care Simulation (PACSim) modelling study. Aug 2018)\r\nModelling the growing need for social care in older people\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-22-population-projections-websites/img/allgemeine-erdbeschreibung.jpg",
    "last_modified": "2021-07-01T18:49:28+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-10-making-a-presentation-repo-github-template/",
    "title": "Making presentation slides into a GitHub repository template",
    "description": "One of a series of posts relating to creating presentation templates using {xaringan}, GitHub and R Studio.",
    "author": [
      {
        "name": "Zo√´ Turner",
        "url": {
          "https://twitter.com/Lextuga007": {}
        }
      }
    ],
    "date": "2021-06-10",
    "categories": [
      "Open source",
      "GitHub",
      "Presentations"
    ],
    "contents": "\r\nCreating a template GitHub repository\r\nAfter creating {xaringan} presentations slides for the CDU Data Science Team using the branding from Nottinghamshire Healthcare NHS Foundation Trust, I wanted to share the files used as a template. There are quite a few used in {xaringan} slides because it relies upon CSS and images to give the ‚Äòprofessional‚Äô look similar to PowerPoint.\r\nOne of the new features in GitHub is to make your repository a template:\r\n\r\nAnd the nice thing about this, is when someone selects the template button:\r\n\r\nIt means they can fork the repository (get a copy) but all of the commit history is removed giving the next person - if they want - a clean repository to work from. People still have the option to clone the repository with the history by following the usual cloning:\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-10-making-a-presentation-repo-github-template/img/grundz√ºge-der-mathematischen-geographie-und-der-landkartenprojection.jpg",
    "last_modified": "2021-07-01T18:49:28+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-10-making-an-rstudio-presentation-template/",
    "title": "Making an RStudio presentation template",
    "description": "One of a series of posts relating to creating presentation templates using {xaringan}, GitHub and R Studio.",
    "author": [
      {
        "name": "Zo√´ Turner",
        "url": {
          "https://twitter.com/Lextuga007": {}
        }
      }
    ],
    "date": "2021-06-10",
    "categories": [
      "Open source",
      "RStudio",
      "Presentations"
    ],
    "contents": "\r\nCreating a template through R Studio\r\nDid you know it‚Äôs possible to set up your own templates into RStudio? I didn‚Äôt until I started looking into the presentation templates for the CDU Data Science Team. The benefit of this is that I no longer have to set up clone repositories each time which is awkward too if you want to keep all presentations in one repository like we do for the team‚Äôs presentations.\r\nA few in the team suggested setting these up as a package but {xaringan} slides come with many supporting files (images and CSS), and it wasn‚Äôt too obvious how to do this. Luckily a number of people have done it and one such person tweeted about it so I added the details to the repository‚Äôs issues for reference.\r\nThis led me to @DrMowinckels‚Äôs package {uiothemes} which is particularly useful as she uses this package for various templates and themes and so, following the layout for adding the xaringan slides I added the following folders to our own package {nottshcMethods} following our process of:\r\ncreate a branch with the issue number like 1-slide-templates\r\nmake the changes\r\nset up a pull request to the development branch\r\nFile structure\r\nI added the file structure:\r\nnottshcMethods/inst/rmarkdown/templates/Nottshc/skeleton\r\nand in the Nottshc folder I added a file called template.yaml containing:\r\nname: Nottshc Presentation template\r\ndescription: >\r\n   Standard xaringan Nottshc template for presentations\r\ncreate_dir: TRUE\r\nThe create_dir is particularly important as this is asking if a new folder directory should be created or not. As this is set to TRUE all the files and folder structure in the folder skeleton will be copied. Note that in {uiothemes} all xaringan files are within one folder but I prefer my files to be in subfolders so {nottshcMethods} has the subfolders css and img.\r\nname is what will appear in the RStudio templates later so this needs to be clear and concise.\r\nIn the folder skeleton the important file is the skeleton.Rmd which is the template RMarkdown file for the slides.\r\nAs this is within a package, to run the package Ctrl+Shift+B will build the package on your computer. When that‚Äôs run the template will appear in File/New File/R Markdown‚Ä¶/From Template\r\n\r\nAddendum and a plea to blog things like this\r\nI wrote this out a number of weeks before publishing and I‚Äôm so glad I did as I immediately forgot everything I did. It was only when I needed to do a presentation and went to my templates in RStudio that I realised how great this is and how I couldn‚Äôt remember how I had set it all up. Luckily I had also started this blog in order to share with others and so the moral of the story is, when you write a blog you are sharing your current knowledge with others, but also your future self.\r\nBe kind to your future self, share your thoughts and your technical wins, even if they seem small to you today they may be huge tomorrow.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-10-making-an-rstudio-presentation-template/img/bibliothek-geographischer-handb√ºcher-107.jpg",
    "last_modified": "2021-07-01T18:49:28+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-14-index-of-multiple-deprivation/",
    "title": "Index of Multiple Deprivation",
    "description": "The measure of relative deprivation in small areas in England called lower-layer super output areas",
    "author": [
      {
        "name": "Zo√´ Turner",
        "url": {
          "https://twitter.com/Letxuga007": {}
        }
      }
    ],
    "date": "2021-05-14",
    "categories": [
      "Resources"
    ],
    "contents": "\r\nIndex of Multiple Deprivation (2019)\r\nIMD is very useful for categorising the area a person lives in for deprivation. This information/links and code only cover England and deprivation is scored in relation to all the areas within England using the IMD (2019). This means that they have been ordered by the deprivation score and then ranked (IMDRank).\r\nThe common decile used is between 1 and 10 with 10 being the least deprived.\r\nData warehouses\r\nThe data is often held in 3 tables:\r\nthe postcodes of the data held (for example patients when in the healthcare sector)\r\na lookup postcode table (like a directory of postcodes) from\r\nhttps://digital.nhs.uk/services/organisation-data-service/data-downloads/ods-postcode-files\r\nthe IMD data from\r\nOlder version: http://www.gov.uk/government/statistics/english-indices-of-deprivation-2015 https://www.gov.uk/government/statistics/english-indices-of-deprivation-2019\r\nSelect File 7 for the dataset.\r\nOther data is available from this dataset including IDAOPI which relates to only older people.\r\nNote that the column headers change, in 2015 it was LADistrictCode2013 and in 2019 it is LADistrictCode2019. Also LADistrictName2013 has become LADistrictName2019.\r\nWatch for‚Ä¶\r\n‚ÄòUnknown‚Äô in the data warehouse\r\nData warehouse tables may include a row for ‚Äòunknown‚Äô (https://www.sqlchick.com/entries/2011/5/16/usage-of-unknown-member-rows-in-a-data-warehouse.html) and this may create a valid join across tables: unknown is entered as a postcode that links to the postcode table that in turn returns data from the IMD table. This may result in a value being returned from the IMD table that isn‚Äôt valid like 0 which looks like it should be an IMD decile score, for example, but which is not.\r\nPostcode spaces\r\nAlso, postcode lengths vary according to how many spaces there are between the two parts. In the UK the postcode format can be 3 parts and then 3 or 4 then 3:\r\nNG1 1AA NG26 1AA\r\nJoining datasets is always better when the space between the postcode parts is removed. In SQL this can be:\r\nREPLACE(postcode, ‚Äô ‚Äò,‚Äô‚Äô)\r\nin R it can be\r\n\r\n\r\npostcode <- \"NG16 1AA\"\r\ngsub(\" \",\"\",postcode)\r\n\r\n\r\n\r\nPartial postcodes\r\nPartial postcodes will not give a sufficiently reliable IMD score.\r\nExample join code (SQL)\r\nTo get the IMD score the LSOA (Lower Super Output Area) code is required which is taken from the full postcode.\r\nThis code will not run and is dependent on the naming conventions of the SQL server. The column names of LSOA11 and LSOAcode2011 will have come from the data sources. PostCode_space will have been added to the table by the data warehouse administrator(s).\r\n\r\nSELECT Top 100 imd.*\r\nFROM DIM_AI.PatientData AS p\r\nLEFT JOIN DIM_AI.PostCodes AS pc ON p.PostCode = pc.PostCode_space                                              \r\nLEFT JOIN DIM_AI.IMD AS i ON pc.PC.LSOA11 = i.LSOAcode2011\r\nWHERE p.PostCode LIKE 'NG%'\r\n\r\nCreating quintiles\r\nSome publicly available data is in quintiles for IMD to remove small identifiable numbers.\r\nTo replicate that in local data the equation: floor((IMDDecile-1)/2) + 1 can be applied.\r\nQuintiles in SQL\r\n\r\nSELECT DISTINCT IMDDecile,\r\nFLOOR((IMDDecile-1)/2) + 1 AS IMDQuintile\r\nFROM DIM_AI.IMD\r\nORDER BY IMDDecile\r\n\r\nQuintiles in R\r\nThe following example Pubicly available data will not run download if this Rmarkdown script is run as the eval has been set to FALSE. Either change this to TRUE, remove eval=FALSE altogether or copy the code to another R script to run.\r\n\r\n\r\n# IMD by Ethnicity by Region 2007-2013\r\n# Latest: https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/livebirths/adhocs/006134birthsbyethnicitysexregionandimdquintilebyfinancialyear2007to2013\r\ndownload.file(\"https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/livebirths/adhocs/006134birthsbyethnicitysexregionandimdquintilebyfinancialyear2007to2013\",\r\n              destfile = \"regionbirthsbyethnicityIMD20072013.xls\",\r\n              method = \"wininet\", #use \"curl\" for OS X / Linux, \"wininet\" for Windows\r\n              mode = \"wb\") #wb means \"write binary\"\r\n\r\n\r\n\r\nApplying the formula:\r\n\r\n\r\nlibrary(tidyverse)\r\n# Generate a dataset\r\ndf <- structure(list(IMDDecile = c(0L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, \r\n8L, 9L, 10L, NA)), row.names = c(NA, -12L), class = c(\"tbl_df\", \r\n\"tbl\", \"data.frame\"))\r\n# Make the 0 generated into NA\r\ndf_quintile <- df %>% \r\n  replace_na(list(IMDDecile = 0)) %>% \r\n  mutate(IMDQuintile = floor((IMDDecile-1)/2) + 1,\r\n         IMDQuintile = as.character(IMDQuintile)\r\n         )\r\n\r\n\r\n\r\nCreating local IMDs\r\nIn Nottingham/Nottinghamshire the differences between the LSOA areas is diminished when ranked against England as a whole, but when ranked locally, the variation is much more pronounced. Consequently, for the majority of our analysis the Trust approach is to use Nottinghamshire quintiles of deprivation / IMD. There will be some instances where national quintiles (or deciles) will be required ‚Äì namely any external analysis ‚Äì however, these will be the exception.\r\nFor note, when using local quintiles anyone with a non-local postcode will come back unmatched as well as those people with proxy postcodes used often to denote homeless status (ZZ‚Ä¶).\r\nIMD in SQL\r\nTo create the local rankings in SQL the data needs to be restricted to the appropriate area, for example when joining to the Postcodes and restricting and then a windows partition applied to the data ROW_NUMBER() OVER(ORDER BY IMDRank) to create a new ranking score and NTILE(10) OVER (ORDER BY IMDRank) to create new deciles.\r\nTo replicate this in dplyr the code would be:\r\n\r\n\r\n # Data for Nottingham and Nottinghamshire taken from Postcodes and IMD. The original files are very large to download. \r\n# Note that there are columns in the Postcode data sample that don't exist in the source file. \r\n# These have been added locally but may be of use to others, such as CountyName and LocalAuthority_Name. # These are specifically added as Nottingham Local Authority is a Unitary Authority and appears in a different column to Nottinghamshire County's District Councils \r\nload(\"data/sampleDataNottingham.RData\")\r\nlibrary(dplyr)\r\n# Note on the join the two column names are different so are listed in the by = and they need to be in the correct order so the column from imdNottingham appears first.\r\nlocalRanking <- imdNottingham %>% \r\n  inner_join(postcodeData %>% \r\n               select(LSOA11) %>% \r\n               group_by(LSOA11) %>% \r\n               slice(1), by = c(\"LSOAcode2011\" = \"LSOA11\")) %>% \r\n  mutate(Notts_rank = row_number(IMDRank),\r\n         Notts_decile = ntile(IMDRank, 10)) \r\nlocalRanking %>% \r\n  select(LSOAcode2011:IMDDecile) %>% \r\n  head(5) %>% \r\n  knitr::kable(format = \"html\")\r\n\r\n\r\n\r\nLSOAcode2011\r\n\r\n\r\nLSOAname2011\r\n\r\n\r\nLADistrictCode2019\r\n\r\n\r\nLADistrictName2019\r\n\r\n\r\nIMDScore\r\n\r\n\r\nIMDRank\r\n\r\n\r\nIMDDecile\r\n\r\n\r\nE01013812\r\n\r\n\r\nNottingham 018C\r\n\r\n\r\nE06000018\r\n\r\n\r\nNottingham\r\n\r\n\r\n58.744\r\n\r\n\r\n1042\r\n\r\n\r\n1\r\n\r\n\r\nE01013814\r\n\r\n\r\nNottingham 022B\r\n\r\n\r\nE06000018\r\n\r\n\r\nNottingham\r\n\r\n\r\n42.893\r\n\r\n\r\n3499\r\n\r\n\r\n2\r\n\r\n\r\nE01013810\r\n\r\n\r\nNottingham 018A\r\n\r\n\r\nE06000018\r\n\r\n\r\nNottingham\r\n\r\n\r\n52.690\r\n\r\n\r\n1767\r\n\r\n\r\n1\r\n\r\n\r\nE01013811\r\n\r\n\r\nNottingham 018B\r\n\r\n\r\nE06000018\r\n\r\n\r\nNottingham\r\n\r\n\r\n53.234\r\n\r\n\r\n1688\r\n\r\n\r\n1\r\n\r\n\r\nE01013815\r\n\r\n\r\nNottingham 022C\r\n\r\n\r\nE06000018\r\n\r\n\r\nNottingham\r\n\r\n\r\n41.721\r\n\r\n\r\n3805\r\n\r\n\r\n2\r\n\r\n\r\nOther useful links\r\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/464430/English_Index_of_Multiple_Deprivation_2015_-_Guidance.pdf\r\nhttps://fingertips.phe.org.uk/search/imd\r\nhttp://dclgapps.communities.gov.uk/imd/idmap.html\r\nTechnical report for 2019: https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/833951/IoD2019_Technical_Report.pdf\r\nReferencing IMD in a paper or research\r\nFrom a journal check to see how IMD is referenced in published papers, this was from a 2016 BMJ article that cites the Index in the references as:\r\nDepartment for Communities and Local Government. English indices of deprivation 2015. 2015. https://www.gov.uk/government/statistics/ english-indices-of-deprivation-2015\r\n(Taken from https://bmjopen.bmj.com/content/bmjopen/6/11/e012750.full.pdf)\r\nAnother paper from 2016 cites as:\r\nDepartment for Communities and Local Government. English Indices of Deprivation 2015. Available online: http://www.gov.uk/government/statistics/english-indices-of-deprivation-2015 (accessed on 27 April 2016).\r\n(taken from https://www.mdpi.com/1660-4601/13/8/750)\r\nLooking at the Government page that lists the full text the library assistant said: ‚ÄúI would reference it from the Ministry of Housing, Communities and Local Government which would be more up to date for 2019 and with online references you should always put the date you accessed it. So I would suggest amending to the following:‚Äù\r\nMinistry of Housing, Communities and Local Government. English Indices of Deprivation 2015. 2015. https://www.gov.uk/government/statistics/english-indices-of-deprivation-2015 (Accessed 4 June 2019)\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-14-index-of-multiple-deprivation/img/grundz√ºge-der-mathematischen-geographie-und-der-landkartenprojection-93.jpg",
    "last_modified": "2021-07-01T18:49:28+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-14-mapping/",
    "title": "Mapping",
    "description": "Mapping using public health tools",
    "author": [
      {
        "name": "Zo√´ Turner",
        "url": {
          "https://twitter.com/Letxuga007": {}
        }
      }
    ],
    "date": "2021-05-14",
    "categories": [
      "Resources"
    ],
    "contents": "\r\nMapping\r\nPublic Health Tools\r\nAs Public Health is based within Local Authorities many of their boundaries are related to government rather than health boundaries. Smaller areas related to GPs will be included but data becomes patchy at Trust boundary level. For example, Nottinghamshire Healthcare NHS Foundation Trust covers Nottinghamshire, Nottingham and some areas outside of these boundaries.\r\nLocal Health\r\nPublic Health use the following to overlay data such as life expectancy over IMD scores. It is possible to upload data to this site but this has not been approved by IG.\r\nShape\r\nThis requires creating an account but is freely available to NHS staff.\r\nThis has Trust locations already in the account and data can be overlayed. Drive time and public transport within so many minutes is particularly useful.\r\nCentroid mapping\r\nSometimes you have a set of addresses but no way of mapping them. The Office for National Statistics‚Äô Open Geography Portal provides the centroids for all UK postcodes.\r\nUsing R to get centroid information: https://www.trafforddatalab.io/recipes/gis/postcodes.html#\r\nOr weighted by LSOA area: https://geoportal.statistics.gov.uk/datasets/lower-layer-super-output-areas-december-2011-population-weighted-centroids\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-14-mapping/img/half-hour-library-of-travel-nature-and-science-for-young-readers.jpg",
    "last_modified": "2021-07-01T18:49:28+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-27-working-in-the-open/",
    "title": "Working in the open",
    "description": "What does it mean to work in the open? What is open source? What problems can we solve if we share more openly?",
    "author": [
      {
        "name": "Zo√´ Turner",
        "url": {
          "https://twitter.com/Letxuga007": {}
        }
      }
    ],
    "date": "2021-02-27",
    "categories": [
      "Open source"
    ],
    "contents": "\r\nWhat is working out in the open?\r\n‚ÄúWorking in the open‚Äù isn‚Äôt a technical term; it‚Äôs just my simple way of explaining a concept without having to mention all the tools that are available to do this.\r\n\r\n\r\n\r\nAs you can see from @ChrisBeeley‚Äôs tweet, he refers to ‚Äúwriting stuff in the open and making it reusable‚Äù; we use slightly different language but it covers the same principles.\r\nBeing open in the Public Sector\r\nI‚Äôve always found that analysts working in the NHS and Local Authorities are always happy to share their methodologies, approaches to work and even code and how we shared this was often dictated by who we know and the tools we have to hand, like Excel or Word. If a change was made to the original I‚Äôd never know about it and, vice versa, if I improved the code I wouldn‚Äôt have an easy way to share back what I‚Äôd done.\r\nWhilst analysts were happy to share code I still built up a code repository for myself and for many years I recycled my own code. I often refer back to projects where I know I‚Äôve written a particular bit of code that is useful and I‚Äôd rarely wrote out complete chunks of code that ran independently of project data. Working openly changes how you approach code because sharing projects that don‚Äôt work too well without some changes isn‚Äôt all that useful to others.\r\nWorking with an intention to be open makes you aware of public scrutiny and so, inevitably, you may take a bit longer to make code tidy, write a few more explanatory comments and ensure that code does what you think it should. What‚Äôs nice about doing this is that although the openness of work is intended for someone else‚Äôs benefit, often that person is still you.\r\nRepeatedly solving the same problem\r\nMany of the tasks that analysts and data scientists in the public sector are tasked with are the same. National Returns and benchmarking submissions are common and are completed by many trusts using slightly different approaches but, ultimately, leading to the same data output. This results in a constant cycle of problem solving where the solution, if not shared publicly, means others have to do the very same discovery work.\r\nIn a completely different context, it would be like finding a chemical compound, not sharing that knowledge and other people working hard in other labs to repeat the discovery. By constantly working in this ‚Äòdiscovery‚Äô phase we never further our collective knowledge by refining the techniques, analysing the results and, hopefully, using the ‚Äúcompound‚Äù to make a difference.\r\nPublishing code means that anyone in my team, my trust, the NHS, even the world can see what I have already discovered. In sharing to the world audience, I know I have something I myself can use. Of course, it takes time to write these things out but once it‚Äôs written it never needs to be rediscovered again - but it can be improved upon.\r\nHow do the CDU data science work in the open?\r\nThe CDU data science team have a strong desire to work openly and we have created a GitHub account to share code and knowledge like the pages on IMD1 and mapping.\r\nWe are also involved with the NHS-R Community, facilitating training, presenting webinars and talks as well as hosting the annual Hacktoberfest which was virtual last year. We had originally set up the Hacktoberfest to just be our team, setting aside one day to contribute to projects and practice using GitHub. Pretty quickly after agreeing this would be a good idea we extended this to the NHS-R Community as we felt that there really wasn‚Äôt any need for it to be restricted to just our team. We had a few people come in and out of the MS Teams meeting through the day and, like many things in the NHS-R Community, it was very supportive and informal.\r\nBuiding up skills\r\nWorking on other people‚Äôs projects in a Hackathon may seem, on the face of it, ‚Äúnon-essential‚Äù work, but it‚Äôs invaluable as it not only opens up connections with others who can help with your projects, but you invariably see useful code you can then use. Reviewing code is one thing, but to really understand a piece of code, debug or solve a problem you often have to break it apart and build it back up. In doing so you learn how it is constructed programmatically and how the other person/people have approached a problem. Doing this with others‚Äô scripts has made me a better coder and none of the effort has been wasted.\r\nA recent example of valuable ‚Äúnon-essential‚Äù work for me was helping someone in the NHS-R Community Slack group who had an issue with their RMarkdown and getting a plotly chart to appear in the eventual html output, although it would appear when each chunk was run. I took the code and moved each chunk into a template RMarkdown to see if it ran, section by section. In doing so I located the problem but I also saw a new bit of code2\r\n\r\n\r\ncode_folding: \"hide\"\r\n\r\n\r\n\r\nwhich I‚Äôd never seen that before. Now I could have equally have learned about this from reading about RMarkdown but it will forever stick in my memory as it was in the context of solving a problem. We helped each other and now I‚Äôm sharing that learning in this blog - working out in the open.\r\n\r\n# The full YAML for reference\r\n\r\n---\r\ntitle: \"Test\"\r\ndate: \"25/02/2021\"\r\noutput: \r\n  html_document:\r\n    code_folding: \"hide\"\r\n    toc: true\r\n    toc_float: true\r\n    toc_collapsed: false\r\n---\r\n\r\nIf you want to read more about how our journey is going with working out in the open keep in touch by following us on Twitter and in these blogs.\r\n\r\nIndices of Multiple Deprivation‚Ü©Ô∏é\r\nThe problem was results=‚Äòhide‚Äô being in the knitr::opts_chunk$set() code which affects the output. I‚Äôm still learning how these codes work so didn‚Äôt spot that at first, so I learned more about RMarkdown by debugging.‚Ü©Ô∏é\r\n",
    "preview": "posts/2021-02-27-working-in-the-open/img/letters-from-high-latitudes.jpg",
    "last_modified": "2021-07-01T18:49:28+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-20-a-new-github-release-and-future-projects/",
    "title": "A new GitHub release and future projects",
    "description": "We have a new project out and would like to tell you about some more of our future work.",
    "author": [
      {
        "name": "Chris Beeley",
        "url": {}
      }
    ],
    "date": "2021-01-20",
    "categories": [
      "Open source"
    ],
    "contents": "\r\nIf you‚Äôve read the about section of this blog then you‚Äôll know that our team believes in (and practises!) open source data science. We strive to put as much code and (sometimes synthetic) data out as possible, with an open source licence (MIT, usually), and where we can we try to make our code reasonably easy to re-use (although this is not always simple). We have just pushed out a prototype version of an application and this seemed like a timely moment to talk about the application, and what else we have coming up on the open source side of things. It‚Äôs worth saying that some of our team members work really, really hard doing lots of stuff that is very difficult to share so although you might not see as much of them on the GitHub they‚Äôre doing sterling work for the Trust and the team is dependent on their expertise for all of our work, whether it‚Äôs open source or not.\r\nText mining application\r\nWe already have a blog post about this work and we have come to the point where we have produced a release version (0.1.0) for the dashboard which summarises the acccuracy of the models and helps to show the kinds of decisions that it‚Äôs making. Please read the blog post for details of this work but our ambition in brief is to produce a text mining algorithm for patient experience that can be used in any NHS organisation in the country. The actual algorithm work (which is in Python mainly) has not yet stabilised to a release version just yet but is available on GitHub.\r\nWe will be shipping another dashboard that helps trusts to visualise their feedback as part of the project. We‚Äôre currently working on that but we‚Äôre not quite ready to share it yet, keep an eye on our Twitter and blog for more details. We have a lot more to come in the way of working with staff and patient experience data, too, it‚Äôs not just this work, so please feel free to follow along with the code once it‚Äôs all open and maybe even send us a pull request üòâ.\r\nForecasting of patient numbers\r\nWe‚Äôre also involved in a Health Foundation funded project which looks at predicting numbers of certain types of patients in the hospital. It takes the form of a dashboard which can predict the numbers of patients likely to fall into particular categories in the next 1-10 days based on previous data of this kind. The model has complex seasonality (although currently it achieves better results if you constrain it to results since April because of COVID) and a TBATS model produced the best results. The code is MIT open source and could be easily adapted to predict lots of different univariate series. There are lots of other people involved in this project and written materials from them are forthcoming, I will add them to this blog post once they are available. Our role was just to help with the forecasting and write the dashboard, lots of other work has gone into it.\r\nForecasting pharmacy dispensing\r\nThis is another Health Foundation funded forecasting project which attempts to predict the amount of many different medications which will be dispensed from a pharmacy in order to better manage stock levels. Again the code for our bit is open source MIT, although it is very early days for this project so there will be much more to come. There is much more to this project than just the code, and I will update this blog post with more details once more of the outputs are ready.\r\nA Shiny interface to EndomineR\r\nThis work relates to another Health Foundation funded project but it was funded by NHS-R. EndomineR is a clinical text mining system which works with endoscopic reports and helps to collate and analyse data from free text reports automatically. This work replicates an existing Shiny interface but uses the {golem} package to rewrite the code within modules and to make the application run as an R package. This will make the code easier to maintain, update, and generalise to other clinical settings. This project is currently in active development but it should be ready for a first release in February some time, the code again available open source on GitHub.\r\n{golem}, gitflow, and production data science\r\nWe are all still learning but we are trying to use good methods to make sure that our code is robust and easy to deploy, and to help us collaborate with each other. To this end we:\r\nUse the {golem} package for a lot of our Shiny work, and modularise our Shiny code\r\nUse RStudio Connect (I have written some stuff about this on my own blog)\r\nUse gitflow\r\nHave regular (two weekly) code review sessions\r\nI‚Äôm really interested in understanding how to get better at working together in the open and tools to help code easy to deploy and generalise. If you‚Äôre interested in that too, especially if you work in the NHS (some of the hurdles are the same size and height everywhere in the NHS üòÜ) then please get in touch.\r\nWe have lots of stuff planned including better analysis of HoNOS data, more to come on staff and patient experience, methods for summarising clinical outcomes and health inequalities, and other stuff from team members where I‚Äôm not quite sure how close they are to launch. Please watch out for regular updates if you would like to see more stuff from us, on our Twitter and on this blog.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-20-a-new-github-release-and-future-projects/img/admiralty-manual-for-the-deviations-of-the-compass.jpg",
    "last_modified": "2021-07-01T18:49:28+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-13-data-clinics-in-nottinghamshire-healthcare/",
    "title": "Data clinics in Nottinghamshire Healthcare",
    "description": "We have been working with teams to help them with their data problems. This post describes some of the clinics and what has come about as a result of this work.",
    "author": [
      {
        "name": "Lori Edwards Su√°rez",
        "url": {
          "https://twitter.com/Lori_E_S_": {}
        }
      }
    ],
    "date": "2021-01-13",
    "categories": [
      "Data clinics"
    ],
    "contents": "\r\nOur team recently piloted data clinics within the Trust in order to:\r\nImprove data quality and completeness\r\nImprove the means by which staff collect and record the essential information, making it more efficient and freeing them up to spend more time with their patients.\r\nThis was achieved by going ‚Äúback to basics‚Äù with the people who collect and input data, the key principles are ensuring they know why they are collecting the data, making sure the data collection system works well for the teams and that the data can be used by both the data collectors and analysts. A variety of avenues were considered, such as reducing excess data collection and reducing duplication which make data gathering more laborious and tedious for clinicians.\r\nWhen data collection is difficult for clinicians it often results in the data not being filled properly, correcting this increases the accuracy and completeness of the dataset. Structuring clinical records and decreasing their reliaance on free text input is also beneficial for data analysis but is also often faster and easier for clinical staff. The clinics are a collaborative venture with the clinical team and others such as analysts and system admin, the type of staff varies depending on the needs of each teams. My role was to facilitate the conversations using skills learnt from working closely with the clinical teams to learn to ‚Äútranslate‚Äù between clinical language and data/IT language. Subtle differences in expression between the two groups often lead to misunderstandings which could stifle progress (more examples). The key element was that the problem was generated by the team themselves. This ensures that the clinic is focused on solving their difficulties which should help them to improve their own systems rather than forcing a change on them.\r\nAs a test run we had two teams go through the process:\r\nA forensic mental health team which wanted to move away from using Excel to collect their data\r\nA community mental health team which wanted to collect some extra information to better understand the impact of their team without adding too much to their workload\r\nThe forensic team was a new service which had a lot of data requested of them and they wished to improve their data collection and assess its quality. The Team Leader had used team-specific forms in RiO (the clinical database which they use) previously and was interested in seeing if it was possible here. However, they were having a tough time explaining to the managers who were not familiar with such a system how to approve it and get it built into the system. The spreadsheet was found to have a lot of duplication and data being requested that was not necessarily attainable by the team. We looked together at what the purpose was and changed some of the data from free text to a more structured pick list from the valid values for that piece of data. We also had to explain to the managers that this change was not going to affect their reporting adversely. The patient record system was able to provide the team with what they needed and to automate some aspects to reduce workload for clinicians. Some outcomes measures already existed but others were not yet available on the system, an Excel sheet was made to collect them (an improvement over a folder in the corner of the room) with a reduction in demand for clinicians with simple automation of score summations. The team are thrilled that they can collect the data necessary for reporting and understanding their service in a more intuitive way, project managers are content they are getting the same information and more data is readily available for service improvement. Reports are being built which give the clinicians easy access to data which allows them to engage better and feel ownership of the data.\r\nThe community mental health team wanted to collect some more information to improve their ability to understand their outcomes. They needed to be able to distinguish between the cohorts of patients that were being referred. This ended up having a simple solution that had not been known to the clinical team ‚Äì adding in more specific referral reasons. The patient cohort was clear and defined and could be determined at referral. They wanted some more information on one of the cohorts to understand the group further and to see how specific patients within the group progressed. To gather this data, a short form on the electronic patient record was created which takes one minute to fill in but adds a wealth of information. The team also got to play with the form before it went live to gain familiarity and to help them feel ownership of it. They also wanted to be able to predict when referrals may come in. As we got to know the pathways that brought patients into the service, we learned that we had information about patientes in the previous stage of the pathway. So, we managed to collect some information to understand the time between the previous stage and the referral. This means we can see when there is an uptick in people passing through the previous stage and predict a spike in referrals for the team to prepare for.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-13-data-clinics-in-nottinghamshire-healthcare/img/grundz√ºge-der-mathematischen-geographie-und-der-landkartenprojection-106.jpg",
    "last_modified": "2021-07-01T18:49:28+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-14-classification-of-patient-feedback/",
    "title": "Classification of patient feedback",
    "description": "An NHSE funded project to devise an application to automatically tag the content of patient feedback",
    "author": [
      {
        "name": "Andreas Soteriades",
        "url": {}
      }
    ],
    "date": "2020-11-14",
    "categories": [
      "Patient feedback"
    ],
    "contents": "\r\nConsider the following problem. A NHS trust is devoted to improving their services to provide patients with the best possible experience. The trust sets up a Patient Feedback system in order to identify key areas of improvement. Specialized trust staff (the ‚Äúcoders‚Äù) read the feedback and decide what it is about. For example, if the feedback is ‚ÄúThe doctor carefully listened to me and clearly explained me the possible solutions‚Äù, then the coders can safely conclude that the feedback is about communication.\r\nBut what happens when thousands and thousands of patient feedback records are populating the trust‚Äôs database every few days? Can the coders keep up with tagging such a high volume of records? After all, unless they read all of it, they cannot tag it!\r\nWe need to find a clever way to get some weight off the coders‚Äô shoulders!\r\nHere in Nottinghamshire Healthcare NHS Foundation Trust, we (the Data Science team) have opted for a Machine Learning approach to help coders tag the incoming patient feedback. In particular, we are developing Text Classification algorithms that ‚Äúread‚Äù the feedback and decide what it is about.\r\nFirst things first: what are Machine Learning and Text Classification?\r\nMachine Learning is a wider concept, but here we will talk about the so-called supervised Machine Learning. Say a child is playing with a hole cube:\r\nPhoto of a child‚Äôs sorting toy with the sorting shapes stacked to the side of the sorting boxBy trying to pass different shapes through different holes, the child follows a process of ‚Äútraining‚Äù or ‚Äúlearning‚Äù, through which they learn to identify the right shape for each hole. Once they have been ‚Äútrained‚Äù, they can easily predict what shape is the right one for a specific hole, on this or any other hole cube.\r\nThe process that the child has just followed is very similar to Machine Learning: see the child as an algorithm, the shapes as a dataset, and the holes as tags and you have a supervised Machine Learning problem. In other words, in supervised Machine Learning the algorithm ‚Äúlearns from‚Äù or ‚Äúis trained on‚Äù the dataset, and thus becomes able to predict what tag corresponds to each record.\r\nSo when we have patient feedback data that have already been tagged by our coders, we can train an algorithm to assign the most appropriate tag to each feedback record, based on the content of the feedback text. As fresh, untagged feedback populates the trust‚Äôs database, the algorithm is then able to predict the most appropriate tag for it. In other words, the algorithm learns to automatically classify the text according its content, which is what Text Classification is about: a form of supervised Machine Learning that is about predicting the appropriate tag for the given text.\r\nHow can Text Classification improve NHS services?\r\nIncrease tagging speed. As mentioned earlier, the idea is to have the algorithm automatically tag feedback that the coders simply do not have time to read and tag themselves. To begin with, it will make the process of tagging much more efficient.\r\nNarrow down searches for NHS staff. If a member of staff (e.g.¬†manager, doctor, nurse) wishes to focus on improve patient experience that has to do with, e.g.¬†communication, they will want to read some or all of the incoming feedback about it. The algorithm will crunch the incoming feedback, decide which records are about communication, and feed them back to the member of staff.\r\nAre there any cons?\r\nAlgorithms make errors. For example, an algorithm may incorrectly classify feedback about smoking as being about communication. This is to be expected as no algorithm can ever be 100% accurate. What is key then is to make the algorithm as accurate as possible for the task at hand. This is an area where we focus on on a daily basis.\r\nDespite some inaccuracies, Machine Learning will still offer the great advantage of narrowing down NHS staff searches almost exclusively to the feedback of interest. If a manager has 100 feedback records of which only 20 are potentially relevant, and the algorithm predicts that 30 are potentially relevant (because it will make a few mistakes), this would still be a 70% reduction in the number of feedback records to be read by the manager!\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-14-classification-of-patient-feedback/img/grundz√ºge-der-mathematischen-geographie-und-der-landkartenprojection-96.jpg",
    "last_modified": "2021-07-01T18:49:28+01:00",
    "input_file": {}
  }
]
